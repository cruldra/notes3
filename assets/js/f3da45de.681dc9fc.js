"use strict";(self.webpackChunknotes_3=self.webpackChunknotes_3||[]).push([[57319],{54213:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(36672);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},81078:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"AI/tokenizers/tokenizer_creation_demo","title":"Tokenizer Creation Demo","description":"\u8fd9\u4e24\u4e2a\u6587\u4ef6 (tokenizer.json \u548c tokenizer_config.json) \u901a\u5e38\u662f\u7531 Hugging Face \u7684 tokenizers \u5e93\u548c transformers \u5e93\u534f\u540c\u751f\u6210\u7684\u3002","source":"@site/docs/AI/tokenizers/tokenizer_creation_demo.md","sourceDirName":"AI/tokenizers","slug":"/AI/tokenizers/tokenizer_creation_demo","permalink":"/notes3/docs/AI/tokenizers/tokenizer_creation_demo","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI/tokenizers/tokenizer_creation_demo.md","tags":[],"version":"current","frontMatter":{"title":"Tokenizer Creation Demo","marimo-version":"0.19.2"},"sidebar":"ai","previous":{"title":"\u548ctiktoken\u7684\u533a\u522b","permalink":"/notes3/docs/AI/tokenizers/\u548ctiktoken\u7684\u533a\u522b"},"next":{"title":"\u7b80\u4ecb","permalink":"/notes3/docs/AI/tokenizers/\u7b80\u4ecb"}}');var r=o(23420),i=o(54213);const s={title:"Tokenizer Creation Demo","marimo-version":"0.19.2"},a="Tokenizer Creation Demo",c={},d=[];function l(e){const n={code:"code",h1:"h1",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:"import marimo as mo\n"})}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tokenizer-creation-demo",children:"Tokenizer Creation Demo"})}),"\n",(0,r.jsxs)(n.p,{children:["\u8fd9\u4e24\u4e2a\u6587\u4ef6 (",(0,r.jsx)(n.code,{children:"tokenizer.json"})," \u548c ",(0,r.jsx)(n.code,{children:"tokenizer_config.json"}),") \u901a\u5e38\u662f\u7531 Hugging Face \u7684 ",(0,r.jsx)(n.code,{children:"tokenizers"})," \u5e93\u548c ",(0,r.jsx)(n.code,{children:"transformers"})," \u5e93\u534f\u540c\u751f\u6210\u7684\u3002"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizer.json"})}),": \u5305\u542b\u4e86\u5206\u8bcd\u5668\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\uff08\u8bcd\u8868 Vocabulary\u3001\u5408\u5e76\u89c4\u5219 Merges\uff09\u3001\u9884\u5904\u7406\u903b\u8f91\uff08Normalizer, Pre-tokenizer\uff09\u548c\u89e3\u7801\u903b\u8f91\uff08Decoder\uff09\u3002\u8fd9\u662f\u7531 Rust \u7f16\u5199\u7684 ",(0,r.jsx)(n.code,{children:"tokenizers"})," \u5e93\u76f4\u63a5\u7ba1\u7406\u7684\u5e95\u5c42\u5e8f\u5217\u5316\u6587\u4ef6\u3002"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenizer_config.json"})}),": \u5305\u542b\u4e86 Hugging Face ",(0,r.jsx)(n.code,{children:"transformers"})," \u5e93\u5728\u52a0\u8f7d\u5206\u8bcd\u5668\u65f6\u9700\u8981\u7684\u9ad8\u7ea7\u914d\u7f6e\uff0c\u4f8b\u5982\u7279\u6b8a token \u7684\u6620\u5c04\uff08BOS, EOS, PAD\uff09\u3001\u804a\u5929\u6a21\u677f (",(0,r.jsx)(n.code,{children:"chat_template"}),")\u3001\u6700\u5927\u957f\u5ea6\u9650\u5236\u7b49\u3002"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\u4e0b\u9762\u6f14\u793a\u5982\u4f55\u4ece\u5934\u8bad\u7ec3\u4e00\u4e2a\u5206\u8bcd\u5668\u5e76\u751f\u6210\u8fd9\u4e24\u4e2a\u6587\u4ef6\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:"from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\nfrom transformers import PreTrainedTokenizerFast\nimport json\nimport os\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:"# 1. \u521d\u59cb\u5316\u4e00\u4e2a BPE Tokenizer\n# \u4f60\u7684 tokenizer.json \u663e\u793a\u4f7f\u7528\u4e86 ByteLevel \u7684 pre-tokenizer \u548c decoder\uff0c\u4ee5\u53ca BPE \u6a21\u578b\u3002\ntokenizer = Tokenizer(models.BPE())\n\n# \u914d\u7f6e Pre-tokenizer (\u8d1f\u8d23\u5c06\u6587\u672c\u5207\u5206\u4e3a\u5355\u8bcd/\u5b50\u8bcd\u7684\u521d\u6b65\u5355\u5143)\n# ByteLevel \u80fd\u591f\u5904\u7406 utf-8 \u5b57\u8282\uff0c\u9002\u5408\u591a\u8bed\u8a00\u548c\u4ee3\u7801\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n\n# \u914d\u7f6e Decoder (\u8d1f\u8d23\u5c06 token ID \u89e3\u7801\u56de\u6587\u672c)\ntokenizer.decoder = decoders.ByteLevel()\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:'# 2. \u8bad\u7ec3 Tokenizer\n# \u6211\u4eec\u9700\u8981\u4e00\u4e9b\u6587\u672c\u6570\u636e\u6765\u8bad\u7ec3\u3002\u8fd9\u91cc\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u8bed\u6599\u5e93\u5217\u8868\u3002\ncorpus = [\n    "Hello world!",\n    "This is a test sentence.",\n    "Tokenizers are awesome.",\n    "How to create tokenizer.json and tokenizer_config.json?",\n    "I love AI and Pytorch.",\n    "Deep learning involves neural networks.",\n    "The quick brown fox jumps over the lazy dog.",\n]\n\n# \u5b9a\u4e49\u7279\u6b8a Token\n# \u6ce8\u610f\uff1a\u8fd9\u4e9b token \u9700\u8981\u4e0e tokenizer_config.json \u4e2d\u7684 map \u5bf9\u5e94\nspecial_tokens = ["<|endoftext|>", "<|im_start|>", "<|im_end|>"]\n\n# \u521d\u59cb\u5316\u8bad\u7ec3\u5668\ntrainer = trainers.BpeTrainer(\n    vocab_size=1000,  # \u6f14\u793a\u7528\uff0c\u8bbe\u5c0f\u4e00\u70b9\n    special_tokens=special_tokens,\n    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n)\n\n# \u5728\u8bed\u6599\u4e0a\u8bad\u7ec3\ntokenizer.train_from_iterator(corpus, trainer=trainer)\n\nprint("Tokenizer \u8bad\u7ec3\u5b8c\u6210\uff01")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:'# 3. \u4fdd\u5b58\u539f\u59cb\u7684 tokenizer.json\n# \u8fd9\u6b65\u751f\u6210\u7684\u53ea\u662f\u5e95\u5c42\u7684 tokenizer \u6570\u636e\noutput_dir = "my_tokenizer_demo"\nos.makedirs(output_dir, exist_ok=True)\n\n# \u6b64\u65f6\u4e3b\u8981\u751f\u6210 tokenizer.json\n# \u6ce8\u610f\uff1a\u8fd9\u65f6\u5019\u8fd8\u6ca1\u6709 tokenizer_config.json\ntokenizer.save(os.path.join(output_dir, "tokenizer.json"))\n\nprint(f"\u5e95\u5c42 tokenizer.json \u5df2\u4fdd\u5b58\u5230 {output_dir}")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:"# 4. \u4f7f\u7528 Transformers \u5305\u88c5\u5e76\u6dfb\u52a0\u9ad8\u7ea7\u914d\u7f6e\n# \u8fd9\u4e00\u6b65\u662f\u751f\u6210 tokenizer_config.json \u7684\u5173\u952e\n\n# \u4ece\u521a\u624d\u4fdd\u5b58\u7684 tokenizer.json \u52a0\u8f7d\nfast_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=os.path.join(output_dir, \"tokenizer.json\"),\n    # \u4e0b\u9762\u8fd9\u4e9b\u53c2\u6570\u4f1a\u8fdb\u5165 tokenizer_config.json\n    bos_token=special_tokens[1],  # <|im_start|>\n    eos_token=special_tokens[2],  # <|im_end|>\n    unk_token=special_tokens[0],  # <|endoftext|>\n    pad_token=special_tokens[0],  # <|endoftext|>\n    clean_up_tokenization_spaces=False,\n)\n\n# \u6dfb\u52a0\u804a\u5929\u6a21\u677f (Chat Template)\n# \u8fd9\u662f\u4e00\u4e2a Jinja2 \u6a21\u677f\uff0c\u7528\u4e8e\u5c06\u5bf9\u8bdd\u5217\u8868\u8f6c\u6362\u4e3a prompt \u5b57\u7b26\u4e32\nfast_tokenizer.chat_template = \"{% if messages[0]['role'] == 'system' %}{{ '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}\"\n\nprint(\"Transformer Tokenizer \u5305\u88c5\u5b8c\u6210\uff0c\u5df2\u914d\u7f6e\u7279\u6b8a token \u548c\u804a\u5929\u6a21\u677f\u3002\")\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:'# 5. \u4fdd\u5b58\u5b8c\u6574\u7684\u5206\u8bcd\u5668\n# save_pretrained \u4f1a\u540c\u65f6\u751f\u6210/\u66f4\u65b0 tokenizer.json \u548c\u751f\u6210 tokenizer_config.json\nfast_tokenizer.save_pretrained(output_dir)\n\nprint(f"\u6700\u7ec8\u6587\u4ef6\u5df2\u4fdd\u5b58\u5230 {output_dir} \u76ee\u5f55\uff1a")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:"{.marimo}",children:'# \u67e5\u770b\u751f\u6210\u7684\u6587\u4ef6\nfiles = os.listdir(output_dir)\nprint("\u751f\u6210\u7684\u6587\u4ef6\u5217\u8868:", files)\n\n# \u6253\u5370 tokenizer_config.json \u7684\u5185\u5bb9\nconfig_path = os.path.join(output_dir, "tokenizer_config.json")\nif os.path.exists(config_path):\n    with open(config_path, "r", encoding="utf-8") as f:\n        config = json.load(f)\n        print("\\n\u751f\u6210\u7684 tokenizer_config.json \u5185\u5bb9\u6458\u8981:")\n        print(json.dumps(config, indent=2, ensure_ascii=False))\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}}}]);