## 目录

1. [项目依赖配置](#项目依赖配置)
2. [核心概念](#核心概念)
3. [Agent 定义](#agent-定义)
4. [Task 定义](#task-定义)
5. [Crew 组装](#crew-组装)
6. [LLM 配置](#llm-配置)
7. [知识库集成](#知识库集成)
8. [自定义工具](#自定义工具)
9. [完整使用示例](#完整使用示例)

---

## 项目依赖配置

### pyproject.toml

```toml
[project]
name = "thesis_crew"
version = "0.1.0"
requires-python = ">=3.10,<3.14"
dependencies = [
    "crewai[tools]==1.7.2",
    "litellm>=1.75.3",
]

[tool.crewai]
type = "crew"
```

---

## 核心概念

CrewAI 基于三个核心组件：

1. **Agent（智能体）**：执行任务的 AI 角色，具有特定的角色、目标和背景
2. **Task（任务）**：需要完成的具体工作，包含描述和预期输出
3. **Crew（团队）**：组织 Agent 和 Task，协调执行流程

---

## Agent 定义

### 方式一：使用装饰器和 YAML 配置


#### agents.yaml 配置文件

```yaml
outline_architect:
  role: >
    学术论文架构师
  goal: >
    根据研究主题 "{topic}" 和初步想法，构建逻辑严密、结构完整的论文提纲
  backstory: >
    你擅长构建学术论文的骨架结构。
    你深知一篇优秀论文必须具备清晰的"提出问题-分析问题-解决问题"的逻辑闭环。

thesis_writer:
  role: >
    学术论文写作专家
  goal: >
    帮助学生创建结构完整、格式规范、符合学术标准的论文文档
  backstory: >
    你是一位拥有多年大学论文指导经验的资深学术写作助手。
```

#### Python 代码中定义 Agent

```python
from crewai import Agent
from crewai.project import CrewBase, agent
from crewai.knowledge.knowledge_config import KnowledgeConfig

@CrewBase
class ThesisCrew:
    """ThesisCrew crew"""
    
    @agent
    def outline_architect(self) -> Agent:
        """论文架构师"""
        knowledge_config = KnowledgeConfig(
            results_limit=10,
            score_threshold=0.5
        )
        return Agent(
            config=self.agents_config['outline_architect'],
            verbose=True,
            allow_delegation=False,
            memory=True,
            llm=llm_factory.create_llm(),
            knowledge_config=knowledge_config
        )
```

### 方式二：直接创建 Agent

```python
from crewai import Agent

agent = Agent(
    role="论文研究助手",
    goal="帮助分析论文内容",
    backstory="你是一个专业的论文研究助手，擅长分析学术论文",
    llm=llm,
    verbose=True,
    allow_delegation=False,
    memory=True
)
```


### Agent 参数说明

- `role`: Agent 的角色定义
- `goal`: Agent 的目标，可使用 `{变量}` 占位符
- `backstory`: Agent 的背景故事，增强角色理解
- `verbose`: 是否输出详细日志
- `allow_delegation`: 是否允许委托任务给其他 Agent
- `memory`: 是否启用记忆功能
- `llm`: 使用的语言模型实例
- `knowledge_config`: 知识库配置（查询结果数量、相似度阈值）

---

## Task 定义

### 方式一：使用装饰器和 YAML 配置

#### tasks.yaml 配置文件

```yaml
generate_outline:
  description: >
    基于研究主题 "{topic}" 和学生的初步想法 "{thoughts}"，生成一份详细的论文提纲。
    
    要求：
    1. 提纲必须包含至少三级标题（例如 1.1.1）
    2. 结构应包含：绪论、文献综述、研究方法、研究结果、讨论、结论等标准学术章节
    3. 每个章节应有简要的内容说明，指导后续写作
  expected_output: >
    一份结构完整的论文提纲 Markdown 文档，包含：
    - 清晰的章节层次（至少三级）
    - 每个章节的内容概要
    - 预计的研究方法和数据来源
  agent: outline_architect

improve_content:
  description: >
    基于导师反馈意见，对现有文档内容进行改进和优化。
    
    **当前版本：** {version}
    **原始内容：** {content}
    **导师反馈：** {feedback}
    
    要求：
    1. 仔细分析导师的反馈意见
    2. 针对性地改进文档内容
    3. 保持原有的结构和逻辑
  expected_output: >
    改进后的文档 Markdown 内容
  agent: thesis_writer
```

#### Python 代码中定义 Task

```python
from crewai import Task
from crewai.project import task

@task
def generate_outline_task(self) -> Task:
    """生成论文提纲任务"""
    return Task(
        config=self.tasks_config['generate_outline'],
        agent=self.outline_architect()
    )

@task
def improve_content_task(self) -> Task:
    """改进文档内容任务"""
    return Task(
        config=self.tasks_config['improve_content'],
        agent=self.thesis_writer()
    )
```


### 方式二：直接创建 Task

```python
from crewai import Task

task = Task(
    description="总结知识库中的论文主题",
    expected_output="论文主题列表",
    agent=agent
)
```

### Task 参数说明

- `description`: 任务描述，支持 `{变量}` 占位符，会在 kickoff 时替换
- `expected_output`: 预期输出格式和内容
- `agent`: 执行该任务的 Agent 实例

---

## Crew 组装

### 方式一：使用装饰器定义完整 Crew

```python
from crewai import Crew, Process
from crewai.project import CrewBase, crew

@CrewBase
class ThesisCrew:
    """ThesisCrew crew"""
    
    agents: List[BaseAgent]
    tasks: List[Task]
    
    @crew
    def crew(self) -> Crew:
        """Creates the ThesisCrew crew"""
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True,
            knowledge=knowledge_manager.get_knowledge()
        )
```

### 方式二：动态组装 Crew

```python
from crewai import Crew

thesis_crew = ThesisCrew()

# 获取需要的 Agent 和 Task
architect = thesis_crew.outline_architect()
outline_task = thesis_crew.generate_outline_task()

# 动态组装 Crew
crew = Crew(
    agents=[architect],
    tasks=[outline_task],
    verbose=True,
    knowledge=knowledge_manager.get_knowledge(),
    tracing=True
)

# 执行 Crew
inputs = {
    'topic': '人工智能在教育中的应用',
    'thoughts': '探索 AI 如何改变传统教学模式'
}
result = crew.kickoff(inputs=inputs)
```

### 方式三：简单场景直接创建

```python
from crewai import Agent, Crew, Task

agent = Agent(
    role="论文研究助手",
    goal="帮助分析论文内容",
    backstory="你是一个专业的论文研究助手",
    llm=llm,
    verbose=True
)

task = Task(
    description="总结知识库中的论文主题",
    expected_output="论文主题列表",
    agent=agent
)

crew = Crew(
    agents=[agent],
    tasks=[task],
    verbose=True,
    memory=False,
    knowledge=knowledge
)

result = crew.kickoff()
```


### Crew 参数说明

- `agents`: Agent 列表
- `tasks`: Task 列表
- `process`: 执行流程，`Process.sequential`（顺序）或 `Process.hierarchical`（层级）
- `verbose`: 是否输出详细日志
- `memory`: 是否启用记忆功能
- `knowledge`: 知识库实例
- `tracing`: 是否启用追踪（用于调试）

### 执行 Crew

```python
# 不带参数执行
result = crew.kickoff()

# 带参数执行（替换 YAML 中的占位符）
inputs = {
    'topic': '研究主题',
    'thoughts': '初步想法',
    'version': 'v1.0'
}
result = crew.kickoff(inputs=inputs)

# 获取结果
output = result.raw if hasattr(result, 'raw') else str(result)
```

---

## LLM 配置

### 环境变量配置

```env
# OpenAI
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1

# DeepSeek
DEEPSEEK_API_KEY=sk-xxx
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# OpenRouter
OPENROUTER_API_KEY=sk-xxx
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# 默认配置
DEFAULT_LLM_PROVIDER=openrouter
DEFAULT_MODEL=openrouter/deepseek/deepseek-r1
AGENT_TEMPERATURE=0.7
MAX_TOKENS=4000
```

### LLM Factory 实现

```python
from crewai import LLM
from typing import Optional

class LLMFactory:
    """Factory for creating LLM instances"""
    
    def __init__(self):
        self._llm_cache = {}
    
    def create_llm(
        self,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> LLM:
        """创建 LLM 实例"""
        provider = provider or settings.default_llm_provider
        model = model or settings.default_model
        temperature = temperature if temperature is not None else settings.agent_temperature
        max_tokens = max_tokens or settings.max_tokens
        
        cache_key = f"{provider}:{model}:{temperature}:{max_tokens}"
        
        if cache_key in self._llm_cache:
            return self._llm_cache[cache_key]
        
        if provider == "openai":
            llm = self._create_openai_llm(model, temperature, max_tokens)
        elif provider == "deepseek":
            llm = self._create_deepseek_llm(model, temperature, max_tokens)
        elif provider == "openrouter":
            llm = self._create_openrouter_llm(model, temperature, max_tokens)
        
        self._llm_cache[cache_key] = llm
        return llm

    def _create_openai_llm(self, model: str, temperature: float, max_tokens: int) -> LLM:
        """创建 OpenAI LLM 实例"""
        config = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        if settings.openai_api_key:
            config["api_key"] = settings.openai_api_key
        if settings.openai_base_url:
            config["base_url"] = settings.openai_base_url
        return LLM(**config)
    
    def _create_deepseek_llm(self, model: str, temperature: float, max_tokens: int) -> LLM:
        """创建 DeepSeek LLM 实例"""
        return LLM(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url
        )
    
    def _create_openrouter_llm(self, model: str, temperature: float, max_tokens: int) -> LLM:
        """创建 OpenRouter LLM 实例"""
        return LLM(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.openrouter_api_key,
            base_url=settings.openrouter_base_url
        )

llm_factory = LLMFactory()
```

### 使用 LLM

```python
# 使用默认配置
llm = llm_factory.create_llm()

# 自定义配置
llm = llm_factory.create_llm(
    provider="openai",
    model="gpt-4",
    temperature=0.5,
    max_tokens=2000
)

# 在 Agent 中使用
agent = Agent(
    role="研究助手",
    goal="帮助研究",
    backstory="专业助手",
    llm=llm
)
```

---

## 知识库集成

### 环境变量配置

```env
# Embedder Configuration
EMBEDDER_PROVIDER=ollama
EMBEDDER_MODEL=qwen3-embedding
EMBEDDER_URL=http://localhost:11434/api/embeddings
EMBEDDER_CHUNK_SIZE=4000
EMBEDDER_CHUNK_OVERLAP=200
EMBEDDER_TIMEOUT=300

# Knowledge Configuration
KNOWLEDGE_RESULTS_LIMIT=10
KNOWLEDGE_SCORE_THRESHOLD=0.5
```

### 知识库管理器实现

```python
from crewai.knowledge.knowledge import Knowledge
from crewai.knowledge.source.pdf_knowledge_source import PDFKnowledgeSource
from pathlib import Path
import hashlib
import json

class KnowledgeManager:
    """知识库管理器（单例模式）"""
    
    _instance = None
    _knowledge = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
```


    
    def initialize_knowledge(
        self,
        knowledge_dir: str = "knowledge",
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        embedder_config: dict = None,
        collection_name: str = "thesis_knowledge",
        hash_file_path: str = "storage/knowledge_hash.txt"
    ) -> Knowledge:
        """初始化知识库（只初始化一次）"""
        if self._knowledge is not None:
            return self._knowledge
        
        knowledge_path = Path(knowledge_dir)
        pdf_files = list(knowledge_path.glob("*.pdf"))
        
        # 检查哪些文件需要重新嵌入
        hash_file = Path(hash_file_path)
        embedded_files = self._load_embedded_files(hash_file)
        
        files_to_embed = []
        for pdf_file in pdf_files:
            file_hash = self._calculate_file_hash(pdf_file)
            if str(pdf_file) not in embedded_files or embedded_files[str(pdf_file)]['file_hash'] != file_hash:
                files_to_embed.append(pdf_file)
        
        if not files_to_embed:
            # 所有文件已嵌入，直接创建 Knowledge 对象
            self._knowledge = Knowledge(
                sources=[],
                embedder=embedder_config,
                collection_name=collection_name
            )
            return self._knowledge
        
        # 创建 PDF 知识源
        pdf_source = PDFKnowledgeSource(
            file_paths=[f.name for f in files_to_embed],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # 创建 Knowledge 对象
        self._knowledge = Knowledge(
            sources=[pdf_source],
            embedder=embedder_config,
            collection_name=collection_name
        )
        
        # 执行嵌入
        self._knowledge.add_sources()
        
        # 保存嵌入记录
        for pdf_file in files_to_embed:
            file_hash = self._calculate_file_hash(pdf_file)
            self._save_embedded_file(hash_file, str(pdf_file), file_hash, chunk_size, chunk_overlap)
        
        return self._knowledge
    
    def get_knowledge(self):
        """获取已初始化的知识库"""
        return self._knowledge
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """计算文件 SHA256 哈希值"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

knowledge_manager = KnowledgeManager()
```


### Embedder 配置

```python
def create_embedder(
    provider: str = "ollama",
    model: str = "qwen3-embedding",
    url: str = "http://localhost:11434/api/embeddings",
    timeout: int = 300
) -> dict:
    """创建嵌入模型配置"""
    return {
        "provider": provider,
        "config": {
            "model_name": model,
            "url": url,
            "timeout": timeout
        }
    }

# 使用
embedder_config = create_embedder()
```

### 应用启动时初始化知识库

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    
    # 初始化知识库
    knowledge_manager.initialize_knowledge(
        knowledge_dir="knowledge",
        embedder_config=llm_factory.create_embedder(),
        collection_name="thesis_knowledge",
        hash_file_path="storage/knowledge_hash.txt"
    )
    
    yield

app = FastAPI(lifespan=lifespan)
```

### 在 Agent 中使用知识库

```python
from crewai.knowledge.knowledge_config import KnowledgeConfig

# 配置 Agent 的知识库查询参数
knowledge_config = KnowledgeConfig(
    results_limit=10,
    score_threshold=0.5
)

agent = Agent(
    role="论文写作专家",
    goal="撰写高质量论文",
    backstory="资深学术写作助手",
    llm=llm,
    knowledge_config=knowledge_config
)

# 在 Crew 中使用知识库
crew = Crew(
    agents=[agent],
    tasks=[task],
    knowledge=knowledge_manager.get_knowledge()
)
```

### 知识库优化技巧

1. **使用哈希文件避免重复嵌入**：记录已嵌入文件的哈希值，只处理新增或修改的文件
2. **单例模式**：确保知识库只初始化一次，避免重复加载
3. **应用启动时初始化**：在 FastAPI lifespan 中初始化，而不是每次请求时
4. **合理设置分块参数**：`chunk_size=4000`, `chunk_overlap=200` 适合大多数场景
5. **调整查询参数**：根据需求设置 `results_limit` 和 `score_threshold`

---

## 自定义工具

### 定义自定义工具

```python
from crewai.tools import BaseTool
from typing import Type
from pydantic import BaseModel, Field

class MyCustomToolInput(BaseModel):
    """Input schema for MyCustomTool"""
    argument: str = Field(..., description="Description of the argument.")

class MyCustomTool(BaseTool):
    name: str = "Name of my tool"
    description: str = "Clear description for what this tool is useful for"
    args_schema: Type[BaseModel] = MyCustomToolInput
    
    def _run(self, argument: str) -> str:
        # 实现工具逻辑
        return "tool output"
```


### 在 Agent 中使用工具

```python
from crewai import Agent

tool = MyCustomTool()

agent = Agent(
    role="研究助手",
    goal="完成研究任务",
    backstory="专业助手",
    tools=[tool],
    llm=llm
)
```

---

## 完整使用示例

### 示例 1：简单的论文摘要生成

```python
from crewai import Agent, Crew, Task, LLM

# 创建 LLM
llm = LLM(
    model="openrouter/deepseek/deepseek-r1",
    api_key="your-api-key",
    base_url="https://openrouter.ai/api/v1",
    temperature=0.7
)

# 创建 Agent
agent = Agent(
    role="论文研究助手",
    goal="帮助分析论文内容",
    backstory="你是一个专业的论文研究助手，擅长分析学术论文",
    llm=llm,
    verbose=True
)

# 创建 Task
task = Task(
    description="总结知识库中的论文主题",
    expected_output="论文主题列表",
    agent=agent
)

# 创建 Crew
crew = Crew(
    agents=[agent],
    tasks=[task],
    verbose=True
)

# 执行
result = crew.kickoff()
print(result.raw)
```

### 示例 2：带知识库的论文提纲生成

```python
from crewai import Agent, Crew, Task
from crewai.knowledge.knowledge import Knowledge
from crewai.knowledge.source.pdf_knowledge_source import PDFKnowledgeSource
from crewai.knowledge.knowledge_config import KnowledgeConfig

# 初始化知识库
pdf_source = PDFKnowledgeSource(
    file_paths=["paper1.pdf", "paper2.pdf"],
    chunk_size=4000,
    chunk_overlap=200
)

embedder_config = {
    "provider": "ollama",
    "config": {
        "model_name": "qwen3-embedding",
        "url": "http://localhost:11434/api/embeddings",
        "timeout": 300
    }
}

knowledge = Knowledge(
    sources=[pdf_source],
    embedder=embedder_config,
    collection_name="thesis_knowledge"
)

# 创建 Agent（配置知识库查询参数）
knowledge_config = KnowledgeConfig(
    results_limit=10,
    score_threshold=0.5
)

agent = Agent(
    role="学术论文架构师",
    goal="根据研究主题 {topic} 构建论文提纲",
    backstory="你擅长构建学术论文的骨架结构",
    llm=llm,
    knowledge_config=knowledge_config,
    verbose=True
)

# 创建 Task
task = Task(
    description="基于研究主题 {topic} 和初步想法 {thoughts}，生成详细的论文提纲",
    expected_output="结构完整的论文提纲 Markdown 文档",
    agent=agent
)

# 创建 Crew
crew = Crew(
    agents=[agent],
    tasks=[task],
    knowledge=knowledge,
    verbose=True
)

# 执行（传入参数）
inputs = {
    'topic': '人工智能在教育中的应用',
    'thoughts': '探索 AI 如何改变传统教学模式'
}
result = crew.kickoff(inputs=inputs)
print(result.raw)
```


### 示例 3：使用 YAML 配置的完整项目结构

```
project/
├── config/
│   ├── agents.yaml      # Agent 配置
│   └── tasks.yaml       # Task 配置
├── crew.py              # Crew 定义
├── main.py              # 应用入口
└── utils/
    ├── llm_factory.py   # LLM 工厂
    └── knowledge_manager.py  # 知识库管理
```

**crew.py**

```python
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai.knowledge.knowledge_config import KnowledgeConfig

@CrewBase
class ThesisCrew:
    """ThesisCrew crew"""
    
    @agent
    def outline_architect(self) -> Agent:
        knowledge_config = KnowledgeConfig(
            results_limit=10,
            score_threshold=0.5
        )
        return Agent(
            config=self.agents_config['outline_architect'],
            verbose=True,
            allow_delegation=False,
            memory=True,
            llm=llm_factory.create_llm(),
            knowledge_config=knowledge_config
        )
    
    @agent
    def thesis_writer(self) -> Agent:
        knowledge_config = KnowledgeConfig(
            results_limit=10,
            score_threshold=0.5
        )
        return Agent(
            config=self.agents_config['thesis_writer'],
            verbose=True,
            allow_delegation=False,
            memory=True,
            llm=llm_factory.create_llm(),
            knowledge_config=knowledge_config
        )
    
    @task
    def generate_outline_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_outline'],
            agent=self.outline_architect()
        )
    
    @task
    def generate_proposal_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_proposal'],
            agent=self.thesis_writer()
        )
    
    @crew
    def crew(self) -> Crew:
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True,
            knowledge=knowledge_manager.get_knowledge()
        )
```

**main.py**

```python
from crew import ThesisCrew

# 方式 1：使用完整 Crew
thesis_crew = ThesisCrew()
result = thesis_crew.crew().kickoff()

# 方式 2：动态组装 Crew
thesis_crew = ThesisCrew()
architect = thesis_crew.outline_architect()
outline_task = thesis_crew.generate_outline_task()

crew = Crew(
    agents=[architect],
    tasks=[outline_task],
    verbose=True,
    knowledge=knowledge_manager.get_knowledge()
)

inputs = {
    'topic': '研究主题',
    'thoughts': '初步想法'
}
result = crew.kickoff(inputs=inputs)
output = result.raw if hasattr(result, 'raw') else str(result)
```


### 示例 4：FastAPI 集成

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from pydantic import BaseModel, Field

class GenerateOutlineRequest(BaseModel):
    topic: str = Field(..., description="研究主题")
    thoughts: str = Field(..., description="初步想法")

class GenerateOutlineResponse(BaseModel):
    success: bool = Field(..., description="是否成功")
    outline: str = Field(..., description="生成的提纲")
    topic: str = Field(..., description="研究主题")

@asynccontextmanager
async def lifespan(app: FastAPI):
    # 应用启动时初始化知识库
    knowledge_manager.initialize_knowledge(
        knowledge_dir="knowledge",
        embedder_config=llm_factory.create_embedder(),
        collection_name="thesis_knowledge",
        hash_file_path="storage/knowledge_hash.txt"
    )
    yield

app = FastAPI(lifespan=lifespan)

@app.post("/api/generate-outline", response_model=GenerateOutlineResponse)
async def generate_outline(request: GenerateOutlineRequest):
    thesis_crew = ThesisCrew()
    
    # 动态组装 Crew
    architect = thesis_crew.outline_architect()
    outline_task = thesis_crew.generate_outline_task()
    
    crew = Crew(
        agents=[architect],
        tasks=[outline_task],
        verbose=True,
        knowledge=knowledge_manager.get_knowledge()
    )
    
    # 执行
    inputs = {
        'topic': request.topic,
        'thoughts': request.thoughts
    }
    result = crew.kickoff(inputs=inputs)
    outline = result.raw if hasattr(result, 'raw') else str(result)
    
    return GenerateOutlineResponse(
        success=True,
        outline=outline,
        topic=request.topic
    )
```

---

## 最佳实践

### 1. 知识库管理

- 使用单例模式管理知识库，避免重复初始化
- 记录文件哈希值，只处理新增或修改的文件
- 在应用启动时初始化，而不是每次请求时
- 合理设置 `chunk_size` 和 `chunk_overlap` 参数

### 2. LLM 配置

- 使用工厂模式创建 LLM 实例
- 缓存 LLM 实例，避免重复创建
- 支持多个 LLM 提供商（OpenAI、DeepSeek、OpenRouter）
- 通过环境变量配置 API Key 和 Base URL

### 3. Agent 设计

- 使用 YAML 配置文件定义 Agent 的 role、goal、backstory
- 在 goal 和 backstory 中使用 `{变量}` 占位符，提高复用性
- 合理设置 `allow_delegation` 和 `memory` 参数
- 配置 `knowledge_config` 控制知识库查询行为

### 4. Task 设计

- 在 description 中清晰描述任务要求
- 使用 `{变量}` 占位符，在 kickoff 时传入具体值
- 明确定义 `expected_output`，指导 Agent 输出格式
- 将复杂任务拆分为多个简单任务

### 5. Crew 组装

- 根据场景选择 `Process.sequential` 或 `Process.hierarchical`
- 动态组装 Crew，只包含需要的 Agent 和 Task
- 启用 `verbose` 便于调试
- 使用 `tracing` 追踪执行过程

### 6. 性能优化

- 限制知识库文件数量和大小（测试时）
- 使用缓存减少重复计算
- 合理设置 `max_tokens` 和 `temperature`
- 异步处理长时间运行的任务

---

## 常见问题

### Q1: 如何避免知识库重复嵌入？

使用哈希文件记录已嵌入的文件，每次初始化时检查文件是否变更。

### Q2: 如何在多个 Crew 之间共享知识库？

使用单例模式的 `KnowledgeManager`，确保知识库只初始化一次。

### Q3: 如何传递动态参数给 Agent？

在 YAML 配置中使用 `{变量}` 占位符，在 `crew.kickoff(inputs={...})` 时传入。

### Q4: 如何选择合适的 LLM？

- 快速响应：使用 DeepSeek
- 高质量输出：使用 GPT-4
- 成本优化：使用 OpenRouter

### Q5: 如何调试 Crew 执行过程？

- 设置 `verbose=True` 查看详细日志
- 启用 `tracing=True` 追踪执行流程
- 使用 MLflow 记录实验结果

---

## 参考资源

- [CrewAI 官方文档](https://docs.crewai.com/)
- [CrewAI GitHub](https://github.com/joaomdmoura/crewAI)
- [LiteLLM 文档](https://docs.litellm.ai/)

---

**文档版本**: 1.0  
**最后更新**: 2026-01-01  
**基于项目**: thesis_crew (CrewAI 1.7.2)
