---
sidebar_position: 2
---

在任何 DSPy 代码中，第一步都是设置你的语言模型。例如，你可以按照如下方式将 OpenAI 的 GPT-4o-mini 配置为默认 LM。

```python linenums="1"
# 通过 `OPENAI_API_KEY` 环境变量认证: import os; os.environ['OPENAI_API_KEY'] = 'here'
lm = dspy.LM('openai/gpt-4o-mini')
dspy.configure(lm=lm)
```

!!! info "一些不同的 LM"

    === "OpenAI"
        你可以通过设置 `OPENAI_API_KEY` 环境变量进行身份验证，或在下方传递 `api_key`。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')
        dspy.configure(lm=lm)
        ```

    === "Gemini (AI Studio)"
        你可以通过设置 `GEMINI_API_KEY` 环境变量进行身份验证，或在下方传递 `api_key`。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('gemini/gemini-2.5-pro-preview-03-25', api_key='GEMINI_API_KEY')
        dspy.configure(lm=lm)
        ```

    === "Anthropic"
        你可以通过设置 `ANTHROPIC_API_KEY` 环境变量进行身份验证，或在下方传递 `api_key`。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('anthropic/claude-sonnet-4-5-20250929', api_key='YOUR_ANTHROPIC_API_KEY')
        dspy.configure(lm=lm)
        ```

    === "Databricks"
        如果你在 Databricks 平台上，通过其 SDK 进行身份验证是自动的。如果不在，你可以设置环境变量 `DATABRICKS_API_KEY` 和 `DATABRICKS_API_BASE`，或在下方传递 `api_key` 和 `api_base`。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')
        dspy.configure(lm=lm)
        ```

    === "本地 LM (GPU 服务器)"
          首先，安装 [SGLang](https://sgl-project.github.io/start/install.html) 并使用你的 LM 启动其服务器。

          ```bash
          > pip install "sglang[all]"
          > pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/ 

          > CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --port 7501 --model-path meta-llama/Meta-Llama-3-8B-Instruct
          ```

          然后，从你的 DSPy 代码连接到它，作为一个兼容 OpenAI 的端点。

          ```python linenums="1"
          lm = dspy.LM("openai/meta-llama/Meta-Llama-3-8B-Instruct",
                           api_base="http://localhost:7501/v1",  # 确保这里指向你的端口
                           api_key="", model_type='chat')
          dspy.configure(lm=lm)
          ```

    === "本地 LM (笔记本电脑)"
          首先，安装 [Ollama](https://github.com/ollama/ollama) 并使用你的 LM 启动其服务器。

          ```bash
          > curl -fsSL https://ollama.ai/install.sh | sh
          > ollama run llama3.2:1b
          ```

          然后，从你的 DSPy 代码连接到它。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='')
        dspy.configure(lm=lm)
        ```

    === "其他提供商"
        在 DSPy 中，你可以使用 [LiteLLM 支持的数十家 LLM 提供商](https://docs.litellm.ai/docs/providers)中的任何一家。只需按照他们的说明设置 `{PROVIDER}_API_KEY`，并将 `{provider_name}/{model_name}` 传递给构造函数即可。

        一些例子：

        - `anyscale/mistralai/Mistral-7B-Instruct-v0.1`，使用 `ANYSCALE_API_KEY`
        - `together_ai/togethercomputer/llama-2-70b-chat`，使用 `TOGETHERAI_API_KEY`
        - `sagemaker/<your-endpoint-name>`，使用 `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, 和 `AWS_REGION_NAME`
        - `azure/<your_deployment_name>`，使用 `AZURE_API_KEY`, `AZURE_API_BASE`, `AZURE_API_VERSION`，以及可选的 `AZURE_AD_TOKEN` 和 `AZURE_API_TYPE` 作为环境变量。如果你在不设置环境变量的情况下启动外部模型，请使用以下方式：
        `lm = dspy.LM('azure/<your_deployment_name>', api_key = 'AZURE_API_KEY' , api_base = 'AZURE_API_BASE', api_version = 'AZURE_API_VERSION')`


        
        如果你的提供商提供兼容 OpenAI 的端点，只需在你的完整模型名称前加上 `openai/` 前缀。

        ```python linenums="1"
        import dspy
        lm = dspy.LM('openai/your-model-name', api_key='PROVIDER_API_KEY', api_base='YOUR_PROVIDER_URL')
        dspy.configure(lm=lm)
        ```
如果遇到错误，请参考 [LiteLLM 文档](https://docs.litellm.ai/docs/providers) 以验证你是否使用了相同的变量名/遵循了正确的步骤。

## 直接调用 LM

直接调用上面配置的 `lm` 很容易。这为你提供了一个统一的 API，并让你从自动缓存等实用程序中受益。

```python linenums="1"       
lm("Say this is a test!", temperature=0.7)  # => ['This is a test!']
lm(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
``` 

## 在 DSPy 模块中使用 LM

地道的 DSPy 用法涉及使用 *模块 (modules)*，我们将在下一指南中讨论。

```python linenums="1" 
# 定义一个模块 (ChainOfThought) 并为其分配一个签名（给定一个问题，返回一个答案）。
qa = dspy.ChainOfThought('question -> answer')

# 使用上面通过 `dspy.configure` 配置的默认 LM 运行。
response = qa(question="How many floors are in the castle David Gregory inherited?")
print(response.answer)
```
**可能的输出:**
```text
The castle David Gregory inherited has 7 floors.
```

## 使用多个 LM

你可以使用 `dspy.configure` 全局更改默认 LM，或者使用 `dspy.context` 在代码块内更改它。

!!! tip
    使用 `dspy.configure` 和 `dspy.context` 是线程安全的！


```python linenums="1" 
dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))
response = qa(question="How many floors are in the castle David Gregory inherited?")
print('GPT-4o-mini:', response.answer)

with dspy.context(lm=dspy.LM('openai/gpt-3.5-turbo')):
    response = qa(question="How many floors are in the castle David Gregory inherited?")
    print('GPT-3.5-turbo:', response.answer)
```
**可能的输出:**
```text
GPT-4o-mini: The number of floors in the castle David Gregory inherited cannot be determined with the information provided.
GPT-3.5-turbo: The castle David Gregory inherited has 7 floors.
```

## 配置 LM 生成

对于任何 LM，你都可以在初始化时或随后的每次调用中配置以下任何属性。

```python linenums="1" 
gpt_4o_mini = dspy.LM('openai/gpt-4o-mini', temperature=0.9, max_tokens=3000, stop=None, cache=False)
```

默认情况下，DSPy 中的 LM 是缓存的。如果你重复相同的调用，你将获得相同的输出。但是你可以通过设置 `cache=False` 来关闭缓存。

如果你想保持缓存开启但强制执行新请求（例如，为了获得多样化的输出），
请在调用中传递一个唯一的 `rollout_id` 并设置非零的 `temperature`。DSPy 在查找缓存条目时会同时哈希输入和 `rollout_id`，因此不同的值会强制执行新的 LM 请求，同时仍会缓存具有相同输入和 `rollout_id` 的未来调用。ID 也会记录在
`lm.history` 中，这使得在实验期间跟踪或比较不同的 rollout 变得容易。
仅更改 `rollout_id` 而保持 `temperature=0` 不会影响 LM 的输出。

```python linenums="1"
lm("Say this is a test!", rollout_id=1, temperature=1.0)
```

你也可以将这些 LM kwargs 直接传递给 DSPy 模块。在初始化时提供它们会设置每次调用的默认值：

```python linenums="1"
predict = dspy.Predict("question -> answer", rollout_id=1, temperature=1.0)
```

要在单次调用中覆盖它们，请在调用模块时提供 `config` 字典：

```python linenums="1"
predict = dspy.Predict("question -> answer")
predict(question="What is 1 + 52?", config={"rollout_id": 5, "temperature": 1.0})
```

在这两种情况下，`rollout_id` 都会转发给底层 LM，影响其缓存行为，并与每个响应一起存储，以便你以后可以重放或分析特定的 rollout。


## 检查输出和使用元数据

每个 LM 对象都维护其交互的历史记录，包括输入、输出、令牌使用（和 $$$ 成本）以及元数据。

```python linenums="1" 
len(lm.history)  # 例如，对 LM 的 3 次调用

lm.history[-1].keys()  # 访问对 LM 的最后一次调用，包含所有元数据
```

**输出:**
```text
dict_keys(['prompt', 'messages', 'kwargs', 'response', 'outputs', 'usage', 'cost', 'timestamp', 'uuid', 'model', 'response_model', 'model_type])
```

## 使用 Responses API

默认情况下，DSPy 使用 LiteLLM 的 [Chat Completions API](https://docs.litellm.ai/docs/completion) 调用语言模型 (LM)，这适用于大多数标准模型和任务。然而，一些高级模型，如 OpenAI 的推理模型（例如 `gpt-5` 或其他未来模型），在通过 [Responses API](https://docs.litellm.ai/docs/response_api) 访问时可能会提供更高的质量或额外的功能，DSPy 支持该 API。

**你应该什么时候使用 Responses API？**

- 如果你正在使用支持或需要 `responses` 端点的模型（例如 OpenAI 的推理模型）。
- 当你想利用某些模型提供的增强推理、多轮对话或更丰富的输出功能时。

**如何在 DSPy 中启用 Responses API:**

要启用 Responses API，只需在创建 `dspy.LM` 实例时设置 `model_type="responses"`。

```python
import dspy

# 配置 DSPy 以对你的语言模型使用 Responses API
dspy.configure(
    lm=dspy.LM(
        "openai/gpt-5-mini",
        model_type="responses",
        temperature=1.0,
        max_tokens=16000,
    ),
)
```

请注意，并非所有模型或提供商都支持 Responses API，请查看 [LiteLLM 的文档](https://docs.litellm.ai/docs/response_api)以获取更多详细信息。


## 高级：构建自定义 LM 和编写你自己的适配器

虽然很少需要，但你可以通过继承 `dspy.BaseLM` 来编写自定义 LM。DSPy 生态系统中的另一个高级层是 *适配器 (adapters)*，它位于 DSPy 签名和 LM 之间。本指南的未来版本将讨论这些高级功能，尽管你可能不需要它们。