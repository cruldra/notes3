`Transformer`是一种革命性的深度学习模型架构,由Google在2017年通过论文[Attention Is All You Need](https://arxiv.org/abs/1706.03762)首次提出.它彻底改变了自然语言处理(NLP)领域,并为现代大语言模型奠定了基础.

## Self-Attention

---

**Self-Attention（自注意力机制）** 是深度学习中的一种核心机制，尤其在 **Transformer 模型** 中被广泛应用。它的核心思想是让模型在处理序列数据（如一句话、一段文本）时，能够动态地关注到序列中不同位置之间的相关性，从而更好地捕捉上下文信息。

想象你正在读一句话：
> “那只猫追着自己的尾巴，结果它撞到了花瓶。”

要理解“它”指代的是“猫”还是“尾巴”，人类会自然地关注前文的“猫”和“尾巴”。**Self-Attention 的作用类似**：模型在处理“它”这个词时，会自动计算它与“猫”“尾巴”“追”等词的关联程度，从而确定“它”的指代对象。
