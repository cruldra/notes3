本文档记录了在 Linux 环境下使用 `uv` 安装 vLLM 时遇到的常见错误及解决方案。

## 环境信息
- 操作系统: Ubuntu 24.04 (WSL2)
- Python 版本: 3.12
- 包管理器: uv
- CUDA 版本: 12.0

---

## 错误 1: 找不到 CUDA_HOME

### 错误信息
```
RuntimeError: Cannot find CUDA_HOME. CUDA must be available to build the package.
```

### 问题原因
vLLM 需要 CUDA 来构建，但系统找不到 `CUDA_HOME` 环境变量。

### 解决方案

1. **检查 CUDA 是否已安装：**
```bash
nvcc --version
```

2. **设置 CUDA_HOME 环境变量：**
```bash
# 找到 CUDA 安装路径，通常在：
# Linux: /usr/local/cuda
# Windows: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.x

# 设置环境变量（Linux）
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

3. **永久设置（添加到 ~/.bashrc 或 ~/.zshrc）：**
```bash
echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

---

## 错误 2: 缺少 C++ 编译器

### 错误信息
```
/bin/sh: 1: c++: not found
gcc: No such file or directory
nvcc fatal: Failed to preprocess host compiler properties.
```

### 问题原因
系统缺少 C++ 编译器（`g++` 和 `gcc`）。vLLM 需要编译 C++ 和 CUDA 代码。

### 解决方案

安装 build-essential 包（包含 gcc、g++、make 等编译工具）：

```bash
sudo apt-get update
sudo apt-get install -y build-essential gcc g++ make
```

### 验证安装
```bash
gcc --version
g++ --version
make --version
```

---

## 错误 3: 缺少 Python 开发头文件

### 错误信息
```
fatal error: Python.h: No such file or directory
   12 | #include <Python.h>
      |          ^~~~~~~~~~
compilation terminated.
```

### 问题原因
缺少 Python 开发头文件。编译 C/C++ 扩展需要 `Python.h`，但系统没有安装 Python 开发包。

### 解决方案

安装 Python 开发包：

```bash
# 针对 Python 3.12
sudo apt-get install python3.12-dev

# 或者通用方式
sudo apt-get install python3-dev
```

### 验证安装
```bash
# 检查 Python.h 是否存在
find /usr/include -name Python.h
```

---

## 错误 4: xformers 构建依赖问题

### 错误信息
```
ModuleNotFoundError: No module named 'torch'

hint: This error likely indicates that `xformers@0.0.26.post1` depends on `torch`, 
but doesn't declare it as a build dependency.
```

### 问题原因
`xformers` 包在构建时需要 `torch`（PyTorch），但它没有在构建依赖中声明。

### 解决方案

**方案 1：在 pyproject.toml 中添加额外的构建依赖**

```toml
[tool.uv.extra-build-dependencies]
xformers = ["torch"]
```

**方案 2：先安装 torch，然后用 --no-build-isolation 标志安装**

```bash
uv pip install torch
uv pip install vllm --no-build-isolation
```

**方案 3：使用预编译的二进制包**

```bash
uv pip install vllm --only-binary :all:
```

---

## 错误 5: NumPy 模块未找到（警告）

### 错误信息
```
UserWarning: Failed to initialize NumPy: No module named 'numpy'
```

### 问题原因
在构建隔离环境中缺少 NumPy。

### 解决方案

这通常是一个警告，不会阻止安装。如果需要解决：

```bash
uv pip install numpy
```

或在 `pyproject.toml` 中添加：

```toml
[tool.uv.extra-build-dependencies]
xformers = ["torch", "numpy"]
```

---

## 完整的安装步骤

### 1. 安装系统依赖

```bash
# 更新包列表
sudo apt-get update

# 安装编译工具
sudo apt-get install -y build-essential gcc g++ make

# 安装 Python 开发包
sudo apt-get install -y python3.12-dev

# 验证 CUDA 安装
nvcc --version
```

### 2. 设置环境变量

```bash
# 设置 CUDA_HOME（如果需要）
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

### 3. 安装 Python 包

```bash
# 使用 uv 安装依赖
uv add datasets numpy pandas safetensors torch transformers trl unsloth

# 单独安装 vllm（如果上面失败）
uv add vllm
```

### 4. 验证安装

```bash
# 检查 PyTorch 版本
uv run python -c "import torch; print(torch.__version__)"

# 检查 CUDA 是否可用
uv run python -c "import torch; print(torch.cuda.is_available())"

# 检查 vLLM
uv run python -c "import vllm; print(vllm.__version__)"
```

---

## 常见问题

### Q1: CUDA 版本不匹配警告

```
The detected CUDA version (12.0) has a minor version mismatch with the version 
that was used to compile PyTorch (12.8).
```

**解答：** 这通常不是问题，小版本不匹配一般不会影响使用。如果遇到问题，考虑安装与 PyTorch 编译时相同的 CUDA 版本。

### Q2: 编译器 ABI 兼容性警告

```
Your compiler (x86_64-linux-gnu-g++) is not compatible with the compiler 
Pytorch was built with for this platform, which is g++ on linux.
```

**解答：** 这是一个警告，通常不会影响安装。如果遇到问题，确保使用的是标准的 `g++` 编译器。

### Q3: 安装速度慢

**解答：** 
- vLLM 需要从源码编译，这需要较长时间
- 考虑使用预编译的 wheel 包
- 或者使用 Docker 镜像

---

## 替代方案

### 使用 Docker

如果编译问题难以解决，推荐使用官方 Docker 镜像：

```bash
docker pull vllm/vllm-openai:latest
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest
```

### 使用 Conda

```bash
conda create -n vllm python=3.12
conda activate vllm
conda install -c conda-forge gcc gxx
pip install vllm
```

---

## 参考资源

- [vLLM 官方文档](https://docs.vllm.ai/)
- [PyTorch 安装指南](https://pytorch.org/get-started/locally/)
- [CUDA 安装指南](https://developer.nvidia.com/cuda-downloads)
- [uv 文档](https://github.com/astral-sh/uv)

---

## 更新日志

- 2025-10-09: 初始版本，记录 vLLM 0.2.5 安装过程中的常见错误

