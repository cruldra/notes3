<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AI/vllm/vLLM性能调优指南" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">vLLM 性能调优指南 | Cruldra</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vLLM性能调优指南"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="vLLM 性能调优指南 | Cruldra"><meta data-rh="true" name="description" content="性能指标"><meta data-rh="true" property="og:description" content="性能指标"><link data-rh="true" rel="icon" href="/notes3/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vLLM性能调优指南"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vLLM性能调优指南" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vLLM性能调优指南" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"vLLM 学习资料","item":"https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/"},{"@type":"ListItem","position":2,"name":"vLLM 性能调优指南","item":"https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vLLM性能调优指南"}]}</script><link rel="alternate" type="application/rss+xml" href="/notes3/blog/rss.xml" title="Cruldra RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes3/blog/atom.xml" title="Cruldra Atom Feed"><link rel="stylesheet" href="/notes3/assets/css/styles.f3dc2d00.css">
<script src="/notes3/assets/js/runtime~main.f728c01e.js" defer="defer"></script>
<script src="/notes3/assets/js/main.3e60493e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme",window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.documentElement.setAttribute("data-theme-choice","system"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/notes3/img/logo.svg"><style data-mantine-styles="classes">@media (max-width: 35.99375em) {.mantine-visible-from-xs {display: none !important;}}@media (min-width: 36em) {.mantine-hidden-from-xs {display: none !important;}}@media (max-width: 47.99375em) {.mantine-visible-from-sm {display: none !important;}}@media (min-width: 48em) {.mantine-hidden-from-sm {display: none !important;}}@media (max-width: 61.99375em) {.mantine-visible-from-md {display: none !important;}}@media (min-width: 62em) {.mantine-hidden-from-md {display: none !important;}}@media (max-width: 74.99375em) {.mantine-visible-from-lg {display: none !important;}}@media (min-width: 75em) {.mantine-hidden-from-lg {display: none !important;}}@media (max-width: 87.99375em) {.mantine-visible-from-xl {display: none !important;}}@media (min-width: 88em) {.mantine-hidden-from-xl {display: none !important;}}</style><div role="region" aria-label="Skip to main content"><a class="skipToContent_kJiJ" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/notes3/"><div class="navbar__logo"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_NrnZ themedComponent--light_kIco"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_NrnZ themedComponent--dark_HCTy"></div><b class="navbar__title text--truncate">Cruldra</b></a><a class="navbar__item navbar__link" href="/notes3/docs/category/设计模式">JVM</a><a class="navbar__item navbar__link" href="/notes3/docs/category/css">前端</a><a class="navbar__item navbar__link" href="/notes3/docs/category/astrbot">工具</a><a class="navbar__item navbar__link" href="/notes3/docs/category/ai电竞酒店">个人</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/notes3/docs/AI/Agno/介绍">AI</a><a class="navbar__item navbar__link" href="/notes3/docs/category/内置模块">Python</a><a class="navbar__item navbar__link" href="/notes3/docs/category/actix-web">Rust</a><a class="navbar__item navbar__link" href="/notes3/docs/category/数据库系统">软件工程</a><a class="navbar__item navbar__link" href="/notes3/docs/category/上古卷轴5">游戏</a><a class="navbar__item navbar__link" href="/notes3/docs/Go/Go-Python-Java三语言全方位对比">Go</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_V3q0"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_YMCn"><div class="docsWrapper_UDzO"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_OA9t" type="button"></button><div class="docRoot_zblS"><aside class="theme-doc-sidebar-container docSidebarContainer_utvl"><div class="sidebarViewport_uicj"><div class="sidebar_wSJP"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_xz03"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/Agno/介绍"><span title="Agno" class="categoryLinkLabel_5XjO">Agno</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/DALLE成本计算"><span title="DALL-E 画图成本计算" class="linkLabel_VeW_">DALL-E 画图成本计算</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/MCP/MCP-Python-SDK"><span title="MCP" class="categoryLinkLabel_5XjO">MCP</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/Transformer"><span title="Transformer" class="linkLabel_VeW_">Transformer</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--active" href="/notes3/docs/AI/vllm/"><span title="vLLM 学习资料" class="categoryLinkLabel_5XjO">vLLM 学习资料</span></a><button aria-label="Collapse sidebar category &#x27;vLLM 学习资料&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/PagedAttention技术详解"><span title="PagedAttention 技术详解" class="linkLabel_VeW_">PagedAttention 技术详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vLLM完整指南"><span title="vLLM 完整指南" class="linkLabel_VeW_">vLLM 完整指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vLLM实战部署指南"><span title="vLLM 实战部署指南" class="linkLabel_VeW_">vLLM 实战部署指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/notes3/docs/AI/vllm/vLLM性能调优指南"><span title="vLLM 性能调优指南" class="linkLabel_VeW_">vLLM 性能调优指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"><span title="vLLM 快速推理与 CUDA Graph 详解" class="linkLabel_VeW_">vLLM 快速推理与 CUDA Graph 详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/安装常见错误及解决方案"><span title="安装常见错误及解决方案" class="linkLabel_VeW_">安装常见错误及解决方案</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念1"><span title="一些概念1" class="linkLabel_VeW_">一些概念1</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念2"><span title="一些概念2" class="linkLabel_VeW_">一些概念2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念3"><span title="一些概念3" class="linkLabel_VeW_">一些概念3</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念4"><span title="一些概念4" class="linkLabel_VeW_">一些概念4</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念5"><span title="一些概念5" class="linkLabel_VeW_">一些概念5</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/候选模型"><span title="候选模型" class="linkLabel_VeW_">候选模型</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/变分自编码器详解"><span title="变分自编码器（VAE）详解" class="linkLabel_VeW_">变分自编码器（VAE）详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/大模型核心概念通俗解释"><span title="大模型核心概念通俗解释" class="linkLabel_VeW_">大模型核心概念通俗解释</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/大模型训练中的随机种子：为什么需要它？"><span title="大模型训练中的随机种子：为什么需要它？" class="linkLabel_VeW_">大模型训练中的随机种子：为什么需要它？</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/嵌入和向量/Chroma与PGVector对比"><span title="嵌入和向量" class="categoryLinkLabel_5XjO">嵌入和向量</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/感知机"><span title="感知机" class="linkLabel_VeW_">感知机</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/智能体"><span title="智能体(Agent)" class="linkLabel_VeW_">智能体(Agent)</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/深度学习/入门"><span title="深度学习" class="categoryLinkLabel_5XjO">深度学习</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/理解大模型：从函数视角看AI的本质"><span title="理解大模型：从函数视角看AI的本质" class="linkLabel_VeW_">理解大模型：从函数视角看AI的本质</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/知识蒸馏"><span title="知识蒸馏" class="linkLabel_VeW_">知识蒸馏</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/联邦学习概念"><span title="联邦学习概念" class="linkLabel_VeW_">联邦学习概念</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/通义千问/阿里云百炼"><span title="通义千问" class="categoryLinkLabel_5XjO">通义千问</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/金丝雀发布和AB测试"><span title="金丝雀发布和AB测试" class="linkLabel_VeW_">金丝雀发布和AB测试</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_e2u6"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_y0g3"><div class="docItemContainer_PZbB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_I1XV" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/notes3/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_ZIEp"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/notes3/docs/AI/vllm/"><span>vLLM 学习资料</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">vLLM 性能调优指南</span></li></ul></nav><div class="tocCollapsible_UUJj theme-doc-toc-mobile tocMobile_b9MH"><button type="button" class="clean-btn tocCollapsibleButton_zQap">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>vLLM 性能调优指南</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="性能指标">性能指标<a href="#性能指标" class="hash-link" aria-label="Direct link to 性能指标" title="Direct link to 性能指标" translate="no">​</a></h2>
<p>在优化 vLLM 性能之前，需要了解关键的性能指标：</p>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-吞吐量throughput">1. 吞吐量（Throughput）<a href="#1-吞吐量throughput" class="hash-link" aria-label="Direct link to 1. 吞吐量（Throughput）" title="Direct link to 1. 吞吐量（Throughput）" translate="no">​</a></h3>
<ul>
<li class=""><strong>定义</strong>：单位时间内处理的请求数或 token 数</li>
<li class=""><strong>单位</strong>：requests/s 或 tokens/s</li>
<li class=""><strong>重要性</strong>：衡量系统整体处理能力</li>
<li class=""><strong>优化目标</strong>：最大化吞吐量</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-延迟latency">2. 延迟（Latency）<a href="#2-延迟latency" class="hash-link" aria-label="Direct link to 2. 延迟（Latency）" title="Direct link to 2. 延迟（Latency）" translate="no">​</a></h3>
<ul>
<li class=""><strong>首 token 延迟（TTFT）</strong>：从请求到第一个 token 的时间</li>
<li class=""><strong>每 token 延迟（TPOT）</strong>：生成每个 token 的平均时间</li>
<li class=""><strong>总延迟</strong>：完成整个请求的时间</li>
<li class=""><strong>优化目标</strong>：最小化延迟</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-显存利用率">3. 显存利用率<a href="#3-显存利用率" class="hash-link" aria-label="Direct link to 3. 显存利用率" title="Direct link to 3. 显存利用率" translate="no">​</a></h3>
<ul>
<li class=""><strong>定义</strong>：GPU 显存的使用比例</li>
<li class=""><strong>范围</strong>：0-100%</li>
<li class=""><strong>最佳值</strong>：80-95%</li>
<li class=""><strong>优化目标</strong>：提高利用率，避免浪费</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="4-gpu-利用率">4. GPU 利用率<a href="#4-gpu-利用率" class="hash-link" aria-label="Direct link to 4. GPU 利用率" title="Direct link to 4. GPU 利用率" translate="no">​</a></h3>
<ul>
<li class=""><strong>定义</strong>：GPU 计算资源的使用比例</li>
<li class=""><strong>范围</strong>：0-100%</li>
<li class=""><strong>最佳值</strong>：大于90%</li>
<li class=""><strong>优化目标</strong>：充分利用 GPU 计算能力</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="核心参数调优">核心参数调优<a href="#核心参数调优" class="hash-link" aria-label="Direct link to 核心参数调优" title="Direct link to 核心参数调优" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-gpu-内存利用率---gpu-memory-utilization">1. GPU 内存利用率 (<code>--gpu-memory-utilization</code>)<a href="#1-gpu-内存利用率---gpu-memory-utilization" class="hash-link" aria-label="Direct link to 1-gpu-内存利用率---gpu-memory-utilization" title="Direct link to 1-gpu-内存利用率---gpu-memory-utilization" translate="no">​</a></h3>
<p><strong>作用</strong>：控制 vLLM 使用的 GPU 显存比例</p>
<p><strong>默认值</strong>：0.9 (90%)</p>
<p><strong>调优策略</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 保守配置（稳定性优先）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gpu-memory-utilization 0.8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 平衡配置（推荐）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gpu-memory-utilization 0.9</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 激进配置（性能优先）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--gpu-memory-utilization 0.95</span><br></span></code></pre></div></div>
<p><strong>调优步骤</strong>：</p>
<ol>
<li class="">从 0.95 开始测试</li>
<li class="">如果出现 OOM（Out of Memory），降低到 0.9</li>
<li class="">继续降低直到稳定运行</li>
<li class="">在稳定的基础上，逐步提高以获得最佳性能</li>
</ol>
<p><strong>注意事项</strong>：</p>
<ul>
<li class="">设置过高可能导致 OOM</li>
<li class="">设置过低会浪费显存，降低并发能力</li>
<li class="">不同模型和硬件需要不同的配置</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-最大模型长度---max-model-len">2. 最大模型长度 (<code>--max-model-len</code>)<a href="#2-最大模型长度---max-model-len" class="hash-link" aria-label="Direct link to 2-最大模型长度---max-model-len" title="Direct link to 2-最大模型长度---max-model-len" translate="no">​</a></h3>
<p><strong>作用</strong>：限制模型处理的最大上下文长度</p>
<p><strong>默认值</strong>：模型配置的最大长度</p>
<p><strong>调优策略</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 短文本场景</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-model-len 2048</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 中等文本场景（推荐）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-model-len 4096</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 长文本场景</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-model-len 8192</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 超长文本场景</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-model-len 32768</span><br></span></code></pre></div></div>
<p><strong>影响</strong>：</p>
<ul>
<li class="">越大：支持更长的上下文，但占用更多显存</li>
<li class="">越小：节省显存，可支持更多并发，但限制了输入长度</li>
</ul>
<p><strong>建议</strong>：</p>
<ul>
<li class="">根据实际业务需求设置</li>
<li class="">不要设置过大，避免浪费显存</li>
<li class="">可以通过监控实际请求长度来优化</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-最大并发序列数---max-num-seqs">3. 最大并发序列数 (<code>--max-num-seqs</code>)<a href="#3-最大并发序列数---max-num-seqs" class="hash-link" aria-label="Direct link to 3-最大并发序列数---max-num-seqs" title="Direct link to 3-最大并发序列数---max-num-seqs" translate="no">​</a></h3>
<p><strong>作用</strong>：限制同时处理的最大序列数</p>
<p><strong>默认值</strong>：256</p>
<p><strong>调优策略</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 低并发场景</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-seqs 64</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 中等并发场景（推荐）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-seqs 256</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 高并发场景</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-seqs 512</span><br></span></code></pre></div></div>
<p><strong>影响</strong>：</p>
<ul>
<li class="">越大：可处理更多并发请求，但每个请求可能等待更久</li>
<li class="">越小：每个请求响应更快，但总吞吐量降低</li>
</ul>
<p><strong>调优方法</strong>：</p>
<ol>
<li class="">监控实际并发请求数</li>
<li class="">设置为实际并发数的 1.5-2 倍</li>
<li class="">观察 GPU 利用率和延迟</li>
<li class="">逐步调整到最佳值</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="4-最大批处理-token-数---max-num-batched-tokens">4. 最大批处理 Token 数 (<code>--max-num-batched-tokens</code>)<a href="#4-最大批处理-token-数---max-num-batched-tokens" class="hash-link" aria-label="Direct link to 4-最大批处理-token-数---max-num-batched-tokens" title="Direct link to 4-最大批处理-token-数---max-num-batched-tokens" translate="no">​</a></h3>
<p><strong>作用</strong>：限制单次批处理的最大 token 数</p>
<p><strong>默认值</strong>：与 <code>--max-model-len</code> 相同</p>
<p><strong>调优策略</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 小批量（低延迟优先）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-batched-tokens 2048</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 中批量（平衡）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-batched-tokens 4096</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 大批量（吞吐量优先）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--max-num-batched-tokens 8192</span><br></span></code></pre></div></div>
<p><strong>影响</strong>：</p>
<ul>
<li class="">越大：吞吐量越高，但首 token 延迟可能增加</li>
<li class="">越小：延迟更低，但吞吐量降低</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="5-张量并行大小---tensor-parallel-size">5. 张量并行大小 (<code>--tensor-parallel-size</code>)<a href="#5-张量并行大小---tensor-parallel-size" class="hash-link" aria-label="Direct link to 5-张量并行大小---tensor-parallel-size" title="Direct link to 5-张量并行大小---tensor-parallel-size" translate="no">​</a></h3>
<p><strong>作用</strong>：将模型分割到多个 GPU 上并行计算</p>
<p><strong>默认值</strong>：1</p>
<p><strong>调优策略</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 单卡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tensor-parallel-size 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 双卡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tensor-parallel-size 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 四卡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tensor-parallel-size 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 八卡</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--tensor-parallel-size 8</span><br></span></code></pre></div></div>
<p><strong>建议</strong>：</p>
<ul>
<li class="">设置为 GPU 数量</li>
<li class="">确保模型大小需要多卡才能运行</li>
<li class="">注意通信开销，不是越多越好</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="6-数据类型---dtype">6. 数据类型 (<code>--dtype</code>)<a href="#6-数据类型---dtype" class="hash-link" aria-label="Direct link to 6-数据类型---dtype" title="Direct link to 6-数据类型---dtype" translate="no">​</a></h3>
<p><strong>作用</strong>：指定模型推理的数据类型</p>
<p><strong>选项</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 自动选择（推荐）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dtype auto</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 半精度（推荐，平衡性能和精度）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dtype bfloat16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 半精度（兼容性好）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dtype float16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 全精度（精度最高，但慢）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--dtype float32</span><br></span></code></pre></div></div>
<p><strong>对比</strong>：</p>
<table><thead><tr><th>数据类型</th><th>显存占用</th><th>速度</th><th>精度</th><th>推荐场景</th></tr></thead><tbody><tr><td>float32</td><td>100%</td><td>慢</td><td>最高</td><td>研究、对精度要求极高</td></tr><tr><td>float16</td><td>50%</td><td>快</td><td>高</td><td>通用场景</td></tr><tr><td>bfloat16</td><td>50%</td><td>快</td><td>高</td><td>推荐，兼容性好</td></tr></tbody></table>
<p><strong>建议</strong>：</p>
<ul>
<li class="">优先使用 <code>bfloat16</code></li>
<li class="">如果 GPU 不支持，使用 <code>float16</code></li>
<li class="">避免使用 <code>float32</code>，除非有特殊需求</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="高级优化技巧">高级优化技巧<a href="#高级优化技巧" class="hash-link" aria-label="Direct link to 高级优化技巧" title="Direct link to 高级优化技巧" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-启用分块预填充---enable-chunked-prefill">1. 启用分块预填充 (<code>--enable-chunked-prefill</code>)<a href="#1-启用分块预填充---enable-chunked-prefill" class="hash-link" aria-label="Direct link to 1-启用分块预填充---enable-chunked-prefill" title="Direct link to 1-启用分块预填充---enable-chunked-prefill" translate="no">​</a></h3>
<p><strong>作用</strong>：将大型预填充操作拆分成更小的块，与解码请求一起批处理</p>
<p><strong>启用方式</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --enable-chunked-prefill \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-num-batched-tokens 8192</span><br></span></code></pre></div></div>
<p><strong>优势</strong>：</p>
<ul>
<li class="">减少首 token 延迟</li>
<li class="">提高 GPU 利用率</li>
<li class="">更好的批处理效率</li>
</ul>
<p><strong>适用场景</strong>：</p>
<ul>
<li class="">长文本输入</li>
<li class="">混合长短请求</li>
<li class="">需要低延迟的场景</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-禁用日志请求---disable-log-requests">2. 禁用日志请求 (<code>--disable-log-requests</code>)<a href="#2-禁用日志请求---disable-log-requests" class="hash-link" aria-label="Direct link to 2-禁用日志请求---disable-log-requests" title="Direct link to 2-禁用日志请求---disable-log-requests" translate="no">​</a></h3>
<p><strong>作用</strong>：禁用请求日志，减少 I/O 开销</p>
<p><strong>启用方式</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --disable-log-requests</span><br></span></code></pre></div></div>
<p><strong>优势</strong>：</p>
<ul>
<li class="">减少 I/O 开销</li>
<li class="">提升性能（约 5-10%）</li>
</ul>
<p><strong>注意</strong>：</p>
<ul>
<li class="">生产环境建议启用</li>
<li class="">调试时建议禁用</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-使用量化模型">3. 使用量化模型<a href="#3-使用量化模型" class="hash-link" aria-label="Direct link to 3. 使用量化模型" title="Direct link to 3. 使用量化模型" translate="no">​</a></h3>
<p><strong>作用</strong>：通过量化减少显存占用，提升推理速度</p>
<p><strong>支持的量化方法</strong>：</p>
<ul>
<li class=""><strong>AWQ</strong>：4-bit 量化，精度损失小</li>
<li class=""><strong>GPTQ</strong>：4-bit 量化，兼容性好</li>
<li class=""><strong>SqueezeLLM</strong>：3-bit 量化，压缩率高</li>
</ul>
<p><strong>使用方式</strong>：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># AWQ 量化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct-AWQ \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --quantization awq</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># GPTQ 量化</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct-GPTQ \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --quantization gptq</span><br></span></code></pre></div></div>
<p><strong>性能对比</strong>：</p>
<table><thead><tr><th>量化方法</th><th>显存占用</th><th>速度</th><th>精度损失</th></tr></thead><tbody><tr><td>无量化</td><td>100%</td><td>基准</td><td>0%</td></tr><tr><td>AWQ</td><td>25-30%</td><td>1.5-2x</td><td>小于1%</td></tr><tr><td>GPTQ</td><td>25-30%</td><td>1.5-2x</td><td>小于2%</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="4-调整采样参数">4. 调整采样参数<a href="#4-调整采样参数" class="hash-link" aria-label="Direct link to 4. 调整采样参数" title="Direct link to 4. 调整采样参数" translate="no">​</a></h3>
<p><strong>作用</strong>：优化生成质量和速度</p>
<p><strong>关键参数</strong>：</p>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">sampling_params </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> SamplingParams</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    temperature</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.7</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">          </span><span class="token comment" style="color:#999988;font-style:italic"># 控制随机性，0-1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    top_p</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.9</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">               </span><span class="token comment" style="color:#999988;font-style:italic"># 核采样，优先调节</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    top_k</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">                </span><span class="token comment" style="color:#999988;font-style:italic"># Top-K 采样</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    repetition_penalty</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1.1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 重复惩罚，大于1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_tokens</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1000</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">         </span><span class="token comment" style="color:#999988;font-style:italic"># 最大生成 token 数</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    presence_penalty</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 存在惩罚</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    frequency_penalty</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.0</span><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 频率惩罚</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>调优建议</strong>：</p>
<ul>
<li class="">
<p><strong>temperature</strong>：</p>
<ul>
<li class="">0.1-0.3：确定性强，适合事实性任务</li>
<li class="">0.7-0.9：平衡，适合通用对话</li>
<li class="">1.0+：创造性强，适合创作任务</li>
</ul>
</li>
<li class="">
<p><strong>top_p</strong>：</p>
<ul>
<li class="">优先调节此参数，比 top_k 更有效</li>
<li class="">0.9-0.95：推荐值</li>
<li class="">越小越确定，越大越多样</li>
</ul>
</li>
<li class="">
<p><strong>repetition_penalty</strong>：</p>
<ul>
<li class="">1.0：无惩罚</li>
<li class="">1.1-1.2：轻度惩罚（推荐）</li>
<li class="">1.5+：强惩罚，可能影响流畅性</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="性能监控">性能监控<a href="#性能监控" class="hash-link" aria-label="Direct link to 性能监控" title="Direct link to 性能监控" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-使用内置监控">1. 使用内置监控<a href="#1-使用内置监控" class="hash-link" aria-label="Direct link to 1. 使用内置监控" title="Direct link to 1. 使用内置监控" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 启用监控</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --enable-metrics \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --metrics-port 9090</span><br></span></code></pre></div></div>
<p>访问 <code>http://localhost:9090/metrics</code> 查看指标。</p>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-关键监控指标">2. 关键监控指标<a href="#2-关键监控指标" class="hash-link" aria-label="Direct link to 2. 关键监控指标" title="Direct link to 2. 关键监控指标" translate="no">​</a></h3>
<ul>
<li class=""><code>vllm:num_requests_running</code>：正在运行的请求数</li>
<li class=""><code>vllm:num_requests_waiting</code>：等待中的请求数</li>
<li class=""><code>vllm:gpu_cache_usage_perc</code>：GPU 缓存使用率</li>
<li class=""><code>vllm:time_to_first_token_seconds</code>：首 token 延迟</li>
<li class=""><code>vllm:time_per_output_token_seconds</code>：每 token 延迟</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-使用-prometheus--grafana">3. 使用 Prometheus + Grafana<a href="#3-使用-prometheus--grafana" class="hash-link" aria-label="Direct link to 3. 使用 Prometheus + Grafana" title="Direct link to 3. 使用 Prometheus + Grafana" translate="no">​</a></h3>
<p><strong>Prometheus 配置</strong> (<code>prometheus.yml</code>)：</p>
<div class="language-yaml codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-yaml codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">scrape_configs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">job_name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;vllm&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">static_configs</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">targets</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&#x27;localhost:9090&#x27;</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
<p><strong>Grafana 仪表板</strong>：</p>
<ul>
<li class="">导入 vLLM 官方仪表板</li>
<li class="">监控吞吐量、延迟、GPU 使用率等</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="性能测试">性能测试<a href="#性能测试" class="hash-link" aria-label="Direct link to 性能测试" title="Direct link to 性能测试" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-使用-vllm-内置基准测试">1. 使用 vLLM 内置基准测试<a href="#1-使用-vllm-内置基准测试" class="hash-link" aria-label="Direct link to 1. 使用 vLLM 内置基准测试" title="Direct link to 1. 使用 vLLM 内置基准测试" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 吞吐量测试</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python -m vllm.entrypoints.openai.api_server \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --benchmark</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 自定义测试</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python benchmarks/benchmark_throughput.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --model /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --num-prompts 1000 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --input-len 128 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --output-len 128</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-使用-wrk-压测">2. 使用 wrk 压测<a href="#2-使用-wrk-压测" class="hash-link" aria-label="Direct link to 2. 使用 wrk 压测" title="Direct link to 2. 使用 wrk 压测" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 安装 wrk</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sudo apt-get install wrk</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 压测</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wrk -t 4 -c 100 -d 60s \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H &quot;Content-Type: application/json&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  -H &quot;Authorization: Bearer your-api-key&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --script post.lua \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  http://localhost:8000/v1/completions</span><br></span></code></pre></div></div>
<p><strong>post.lua</strong>：</p>
<div class="language-lua codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-lua codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">wrk.method = &quot;POST&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wrk.body = &#x27;{&quot;model&quot;: &quot;/data/models/Qwen2.5-7B-Instruct&quot;, &quot;prompt&quot;: &quot;你好&quot;, &quot;max_tokens&quot;: 100}&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wrk.headers[&quot;Content-Type&quot;] = &quot;application/json&quot;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="调优流程">调优流程<a href="#调优流程" class="hash-link" aria-label="Direct link to 调优流程" title="Direct link to 调优流程" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-基准测试">1. 基准测试<a href="#1-基准测试" class="hash-link" aria-label="Direct link to 1. 基准测试" title="Direct link to 1. 基准测试" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 记录默认配置的性能</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 运行压测，记录吞吐量和延迟</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-逐步调优">2. 逐步调优<a href="#2-逐步调优" class="hash-link" aria-label="Direct link to 2. 逐步调优" title="Direct link to 2. 逐步调优" translate="no">​</a></h3>
<p><strong>步骤 1：优化显存利用率</strong></p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 测试不同的 gpu-memory-utilization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for util in 0.8 0.85 0.9 0.95; do</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --gpu-memory-utilization $util</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # 运行压测，记录结果</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">done</span><br></span></code></pre></div></div>
<p><strong>步骤 2：优化并发数</strong></p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 测试不同的 max-num-seqs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for seqs in 64 128 256 512; do</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --max-num-seqs $seqs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # 运行压测，记录结果</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">done</span><br></span></code></pre></div></div>
<p><strong>步骤 3：优化批处理</strong></p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 测试不同的 max-num-batched-tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for tokens in 2048 4096 8192; do</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --max-num-batched-tokens $tokens</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # 运行压测，记录结果</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">done</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-综合优化">3. 综合优化<a href="#3-综合优化" class="hash-link" aria-label="Direct link to 3. 综合优化" title="Direct link to 3. 综合优化" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># 应用最佳配置</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --dtype bfloat16 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --gpu-memory-utilization 0.9 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-model-len 4096 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-num-seqs 256 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-num-batched-tokens 8192 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --enable-chunked-prefill \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --disable-log-requests</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="常见性能问题">常见性能问题<a href="#常见性能问题" class="hash-link" aria-label="Direct link to 常见性能问题" title="Direct link to 常见性能问题" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="1-吞吐量低">1. 吞吐量低<a href="#1-吞吐量低" class="hash-link" aria-label="Direct link to 1. 吞吐量低" title="Direct link to 1. 吞吐量低" translate="no">​</a></h3>
<p><strong>可能原因</strong>：</p>
<ul>
<li class="">GPU 利用率低</li>
<li class="">并发数不足</li>
<li class="">批处理大小过小</li>
</ul>
<p><strong>解决方案</strong>：</p>
<ul>
<li class="">增加 <code>--max-num-seqs</code></li>
<li class="">增加 <code>--max-num-batched-tokens</code></li>
<li class="">启用 <code>--enable-chunked-prefill</code></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="2-延迟高">2. 延迟高<a href="#2-延迟高" class="hash-link" aria-label="Direct link to 2. 延迟高" title="Direct link to 2. 延迟高" translate="no">​</a></h3>
<p><strong>可能原因</strong>：</p>
<ul>
<li class="">批处理大小过大</li>
<li class="">并发数过高</li>
<li class="">模型过大</li>
</ul>
<p><strong>解决方案</strong>：</p>
<ul>
<li class="">减小 <code>--max-num-batched-tokens</code></li>
<li class="">减小 <code>--max-num-seqs</code></li>
<li class="">使用量化模型</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="3-oom显存不足">3. OOM（显存不足）<a href="#3-oom显存不足" class="hash-link" aria-label="Direct link to 3. OOM（显存不足）" title="Direct link to 3. OOM（显存不足）" translate="no">​</a></h3>
<p><strong>可能原因</strong>：</p>
<ul>
<li class=""><code>--gpu-memory-utilization</code> 过高</li>
<li class=""><code>--max-model-len</code> 过大</li>
<li class="">并发数过高</li>
</ul>
<p><strong>解决方案</strong>：</p>
<ul>
<li class="">降低 <code>--gpu-memory-utilization</code></li>
<li class="">减小 <code>--max-model-len</code></li>
<li class="">减小 <code>--max-num-seqs</code></li>
<li class="">使用量化模型</li>
<li class="">使用多卡部署</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="总结">总结<a href="#总结" class="hash-link" aria-label="Direct link to 总结" title="Direct link to 总结" translate="no">​</a></h2>
<p>vLLM 性能调优是一个系统工程，需要：</p>
<ol>
<li class=""><strong>了解业务需求</strong>：吞吐量优先还是延迟优先</li>
<li class=""><strong>监控关键指标</strong>：GPU 利用率、显存使用、吞吐量、延迟</li>
<li class=""><strong>逐步调优</strong>：从默认配置开始，逐步优化各个参数</li>
<li class=""><strong>持续监控</strong>：部署后持续监控，根据实际情况调整</li>
</ol>
<p><strong>推荐配置</strong>（通用场景）：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve /data/models/Qwen2.5-7B-Instruct \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --dtype bfloat16 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --gpu-memory-utilization 0.9 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-model-len 4096 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-num-seqs 256 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --max-num-batched-tokens 8192 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --enable-chunked-prefill \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --disable-log-requests \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --enable-metrics</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_RM2i"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI/vllm/vLLM性能调优指南.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NBA7" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_Ng_4"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/notes3/docs/AI/vllm/vLLM实战部署指南"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">vLLM 实战部署指南</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">vLLM 快速推理与 CUDA Graph 详解</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_qSfz thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#性能指标" class="table-of-contents__link toc-highlight">性能指标</a><ul><li><a href="#1-吞吐量throughput" class="table-of-contents__link toc-highlight">1. 吞吐量（Throughput）</a></li><li><a href="#2-延迟latency" class="table-of-contents__link toc-highlight">2. 延迟（Latency）</a></li><li><a href="#3-显存利用率" class="table-of-contents__link toc-highlight">3. 显存利用率</a></li><li><a href="#4-gpu-利用率" class="table-of-contents__link toc-highlight">4. GPU 利用率</a></li></ul></li><li><a href="#核心参数调优" class="table-of-contents__link toc-highlight">核心参数调优</a><ul><li><a href="#1-gpu-内存利用率---gpu-memory-utilization" class="table-of-contents__link toc-highlight">1. GPU 内存利用率 (<code>--gpu-memory-utilization</code>)</a></li><li><a href="#2-最大模型长度---max-model-len" class="table-of-contents__link toc-highlight">2. 最大模型长度 (<code>--max-model-len</code>)</a></li><li><a href="#3-最大并发序列数---max-num-seqs" class="table-of-contents__link toc-highlight">3. 最大并发序列数 (<code>--max-num-seqs</code>)</a></li><li><a href="#4-最大批处理-token-数---max-num-batched-tokens" class="table-of-contents__link toc-highlight">4. 最大批处理 Token 数 (<code>--max-num-batched-tokens</code>)</a></li><li><a href="#5-张量并行大小---tensor-parallel-size" class="table-of-contents__link toc-highlight">5. 张量并行大小 (<code>--tensor-parallel-size</code>)</a></li><li><a href="#6-数据类型---dtype" class="table-of-contents__link toc-highlight">6. 数据类型 (<code>--dtype</code>)</a></li></ul></li><li><a href="#高级优化技巧" class="table-of-contents__link toc-highlight">高级优化技巧</a><ul><li><a href="#1-启用分块预填充---enable-chunked-prefill" class="table-of-contents__link toc-highlight">1. 启用分块预填充 (<code>--enable-chunked-prefill</code>)</a></li><li><a href="#2-禁用日志请求---disable-log-requests" class="table-of-contents__link toc-highlight">2. 禁用日志请求 (<code>--disable-log-requests</code>)</a></li><li><a href="#3-使用量化模型" class="table-of-contents__link toc-highlight">3. 使用量化模型</a></li><li><a href="#4-调整采样参数" class="table-of-contents__link toc-highlight">4. 调整采样参数</a></li></ul></li><li><a href="#性能监控" class="table-of-contents__link toc-highlight">性能监控</a><ul><li><a href="#1-使用内置监控" class="table-of-contents__link toc-highlight">1. 使用内置监控</a></li><li><a href="#2-关键监控指标" class="table-of-contents__link toc-highlight">2. 关键监控指标</a></li><li><a href="#3-使用-prometheus--grafana" class="table-of-contents__link toc-highlight">3. 使用 Prometheus + Grafana</a></li></ul></li><li><a href="#性能测试" class="table-of-contents__link toc-highlight">性能测试</a><ul><li><a href="#1-使用-vllm-内置基准测试" class="table-of-contents__link toc-highlight">1. 使用 vLLM 内置基准测试</a></li><li><a href="#2-使用-wrk-压测" class="table-of-contents__link toc-highlight">2. 使用 wrk 压测</a></li></ul></li><li><a href="#调优流程" class="table-of-contents__link toc-highlight">调优流程</a><ul><li><a href="#1-基准测试" class="table-of-contents__link toc-highlight">1. 基准测试</a></li><li><a href="#2-逐步调优" class="table-of-contents__link toc-highlight">2. 逐步调优</a></li><li><a href="#3-综合优化" class="table-of-contents__link toc-highlight">3. 综合优化</a></li></ul></li><li><a href="#常见性能问题" class="table-of-contents__link toc-highlight">常见性能问题</a><ul><li><a href="#1-吞吐量低" class="table-of-contents__link toc-highlight">1. 吞吐量低</a></li><li><a href="#2-延迟高" class="table-of-contents__link toc-highlight">2. 延迟高</a></li><li><a href="#3-oom显存不足" class="table-of-contents__link toc-highlight">3. OOM（显存不足）</a></li></ul></li><li><a href="#总结" class="table-of-contents__link toc-highlight">总结</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/notes3/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>