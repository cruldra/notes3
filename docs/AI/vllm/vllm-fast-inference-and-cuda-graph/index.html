<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AI/vllm/vllm-fast-inference-and-cuda-graph" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">vLLM 快速推理与 CUDA Graph 详解 | Cruldra</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="vLLM 快速推理与 CUDA Graph 详解 | Cruldra"><meta data-rh="true" name="description" content="本文档详细介绍 vLLM 的快速推理机制、CUDA Graph 模式以及相关配置。"><meta data-rh="true" property="og:description" content="本文档详细介绍 vLLM 的快速推理机制、CUDA Graph 模式以及相关配置。"><link data-rh="true" rel="icon" href="/notes3/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"vLLM 学习资料","item":"https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/"},{"@type":"ListItem","position":2,"name":"vLLM 快速推理与 CUDA Graph 详解","item":"https://your-docusaurus-site.example.com/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"}]}</script><link rel="alternate" type="application/rss+xml" href="/notes3/blog/rss.xml" title="Cruldra RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes3/blog/atom.xml" title="Cruldra Atom Feed"><link rel="stylesheet" href="/notes3/assets/css/styles.f3dc2d00.css">
<script src="/notes3/assets/js/runtime~main.29b975ef.js" defer="defer"></script>
<script src="/notes3/assets/js/main.03073097.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme",window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.documentElement.setAttribute("data-theme-choice","system"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/notes3/img/logo.svg"><style data-mantine-styles="classes">@media (max-width: 35.99375em) {.mantine-visible-from-xs {display: none !important;}}@media (min-width: 36em) {.mantine-hidden-from-xs {display: none !important;}}@media (max-width: 47.99375em) {.mantine-visible-from-sm {display: none !important;}}@media (min-width: 48em) {.mantine-hidden-from-sm {display: none !important;}}@media (max-width: 61.99375em) {.mantine-visible-from-md {display: none !important;}}@media (min-width: 62em) {.mantine-hidden-from-md {display: none !important;}}@media (max-width: 74.99375em) {.mantine-visible-from-lg {display: none !important;}}@media (min-width: 75em) {.mantine-hidden-from-lg {display: none !important;}}@media (max-width: 87.99375em) {.mantine-visible-from-xl {display: none !important;}}@media (min-width: 88em) {.mantine-hidden-from-xl {display: none !important;}}</style><div role="region" aria-label="Skip to main content"><a class="skipToContent_kJiJ" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/notes3/"><div class="navbar__logo"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_NrnZ themedComponent--light_kIco"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_NrnZ themedComponent--dark_HCTy"></div><b class="navbar__title text--truncate">Cruldra</b></a><a class="navbar__item navbar__link" href="/notes3/docs/category/设计模式">JVM</a><a class="navbar__item navbar__link" href="/notes3/docs/category/css">前端</a><a class="navbar__item navbar__link" href="/notes3/docs/category/astrbot">工具</a><a class="navbar__item navbar__link" href="/notes3/docs/category/ai电竞酒店">个人</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/notes3/docs/AI/Agno/介绍">AI</a><a class="navbar__item navbar__link" href="/notes3/docs/category/内置模块">Python</a><a class="navbar__item navbar__link" href="/notes3/docs/category/actix-web">Rust</a><a class="navbar__item navbar__link" href="/notes3/docs/category/数据库系统">软件工程</a><a class="navbar__item navbar__link" href="/notes3/docs/category/上古卷轴5">游戏</a><a class="navbar__item navbar__link" href="/notes3/docs/Go/Go-Python-Java三语言全方位对比">Go</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_V3q0"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_YMCn"><div class="docsWrapper_UDzO"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_OA9t" type="button"></button><div class="docRoot_zblS"><aside class="theme-doc-sidebar-container docSidebarContainer_utvl"><div class="sidebarViewport_uicj"><div class="sidebar_wSJP"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_xz03"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/Agno/介绍"><span title="Agno" class="categoryLinkLabel_5XjO">Agno</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/DALLE成本计算"><span title="DALL-E 画图成本计算" class="linkLabel_VeW_">DALL-E 画图成本计算</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/MCP/MCP-Python-SDK"><span title="MCP" class="categoryLinkLabel_5XjO">MCP</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/Transformer"><span title="Transformer" class="linkLabel_VeW_">Transformer</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--active" href="/notes3/docs/AI/vllm/"><span title="vLLM 学习资料" class="categoryLinkLabel_5XjO">vLLM 学习资料</span></a><button aria-label="Collapse sidebar category &#x27;vLLM 学习资料&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/PagedAttention技术详解"><span title="PagedAttention 技术详解" class="linkLabel_VeW_">PagedAttention 技术详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vLLM完整指南"><span title="vLLM 完整指南" class="linkLabel_VeW_">vLLM 完整指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vLLM实战部署指南"><span title="vLLM 实战部署指南" class="linkLabel_VeW_">vLLM 实战部署指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/vLLM性能调优指南"><span title="vLLM 性能调优指南" class="linkLabel_VeW_">vLLM 性能调优指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/notes3/docs/AI/vllm/vllm-fast-inference-and-cuda-graph"><span title="vLLM 快速推理与 CUDA Graph 详解" class="linkLabel_VeW_">vLLM 快速推理与 CUDA Graph 详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes3/docs/AI/vllm/安装常见错误及解决方案"><span title="安装常见错误及解决方案" class="linkLabel_VeW_">安装常见错误及解决方案</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念1"><span title="一些概念1" class="linkLabel_VeW_">一些概念1</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念2"><span title="一些概念2" class="linkLabel_VeW_">一些概念2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念3"><span title="一些概念3" class="linkLabel_VeW_">一些概念3</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念4"><span title="一些概念4" class="linkLabel_VeW_">一些概念4</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念5"><span title="一些概念5" class="linkLabel_VeW_">一些概念5</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/候选模型"><span title="候选模型" class="linkLabel_VeW_">候选模型</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/变分自编码器详解"><span title="变分自编码器（VAE）详解" class="linkLabel_VeW_">变分自编码器（VAE）详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/大模型核心概念通俗解释"><span title="大模型核心概念通俗解释" class="linkLabel_VeW_">大模型核心概念通俗解释</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/大模型训练中的随机种子：为什么需要它？"><span title="大模型训练中的随机种子：为什么需要它？" class="linkLabel_VeW_">大模型训练中的随机种子：为什么需要它？</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/嵌入和向量/Chroma与PGVector对比"><span title="嵌入和向量" class="categoryLinkLabel_5XjO">嵌入和向量</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/感知机"><span title="感知机" class="linkLabel_VeW_">感知机</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/智能体"><span title="智能体(Agent)" class="linkLabel_VeW_">智能体(Agent)</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/深度学习/入门"><span title="深度学习" class="categoryLinkLabel_5XjO">深度学习</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/理解大模型：从函数视角看AI的本质"><span title="理解大模型：从函数视角看AI的本质" class="linkLabel_VeW_">理解大模型：从函数视角看AI的本质</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/知识蒸馏"><span title="知识蒸馏" class="linkLabel_VeW_">知识蒸馏</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/联邦学习概念"><span title="联邦学习概念" class="linkLabel_VeW_">联邦学习概念</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OIbK menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/通义千问/阿里云百炼"><span title="通义千问" class="categoryLinkLabel_5XjO">通义千问</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/金丝雀发布和AB测试"><span title="金丝雀发布和AB测试" class="linkLabel_VeW_">金丝雀发布和AB测试</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_e2u6"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_y0g3"><div class="docItemContainer_PZbB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_I1XV" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/notes3/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_ZIEp"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/notes3/docs/AI/vllm/"><span>vLLM 学习资料</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">vLLM 快速推理与 CUDA Graph 详解</span></li></ul></nav><div class="tocCollapsible_UUJj theme-doc-toc-mobile tocMobile_b9MH"><button type="button" class="clean-btn tocCollapsibleButton_zQap">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>vLLM 快速推理与 CUDA Graph 详解</h1></header>
<p>本文档详细介绍 vLLM 的快速推理机制、CUDA Graph 模式以及相关配置。</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="1-什么是快速推理-fast-inference">1. 什么是快速推理 (Fast Inference)<a href="#1-什么是快速推理-fast-inference" class="hash-link" aria-label="Direct link to 1. 什么是快速推理 (Fast Inference)" title="Direct link to 1. 什么是快速推理 (Fast Inference)" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="11-定义">1.1 定义<a href="#11-定义" class="hash-link" aria-label="Direct link to 1.1 定义" title="Direct link to 1.1 定义" translate="no">​</a></h3>
<p><strong>快速推理 (Fast Inference)</strong> 是指使用优化技术来加速大语言模型 (LLM) 的推理过程。在 Unsloth 和 vLLM 的上下文中，快速推理主要指：</p>
<ul>
<li class=""><strong>vLLM 集成</strong>：使用 vLLM 库来优化推理性能</li>
<li class=""><strong>内存优化</strong>：通过 PagedAttention 等技术提高内存利用率</li>
<li class=""><strong>批处理优化</strong>：高效处理多个请求</li>
<li class=""><strong>CUDA Graph 加速</strong>：减少 CPU-GPU 通信开销</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="12-vllm-的核心优化技术">1.2 vLLM 的核心优化技术<a href="#12-vllm-的核心优化技术" class="hash-link" aria-label="Direct link to 1.2 vLLM 的核心优化技术" title="Direct link to 1.2 vLLM 的核心优化技术" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="pagedattention">PagedAttention<a href="#pagedattention" class="hash-link" aria-label="Direct link to PagedAttention" title="Direct link to PagedAttention" translate="no">​</a></h4>
<ul>
<li class="">将注意力机制的 KV Cache 分页管理</li>
<li class="">类似操作系统的虚拟内存管理</li>
<li class="">减少内存碎片，提高利用率</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="连续批处理-continuous-batching">连续批处理 (Continuous Batching)<a href="#连续批处理-continuous-batching" class="hash-link" aria-label="Direct link to 连续批处理 (Continuous Batching)" title="Direct link to 连续批处理 (Continuous Batching)" translate="no">​</a></h4>
<ul>
<li class="">动态调整批次大小</li>
<li class="">不等待整个批次完成</li>
<li class="">提高 GPU 利用率</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="cuda-graph">CUDA Graph<a href="#cuda-graph" class="hash-link" aria-label="Direct link to CUDA Graph" title="Direct link to CUDA Graph" translate="no">​</a></h4>
<ul>
<li class="">预先捕获 CUDA 操作序列</li>
<li class="">减少 CPU 开销</li>
<li class="">加速重复执行的计算图</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="13-在-unsloth-中的应用">1.3 在 Unsloth 中的应用<a href="#13-在-unsloth-中的应用" class="hash-link" aria-label="Direct link to 1.3 在 Unsloth 中的应用" title="Direct link to 1.3 在 Unsloth 中的应用" translate="no">​</a></h3>
<p>在 Unsloth 中，<code>fast_inference=True</code> 参数会：</p>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_seq_length</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2048</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    load_in_4bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 启用 vLLM 快速推理</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_lora_rank</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>作用：</strong></p>
<ul>
<li class="">启用 vLLM 推理引擎</li>
<li class="">自动优化推理性能</li>
<li class="">适用于推理和生成任务</li>
</ul>
<p><strong>注意：</strong></p>
<ul>
<li class="">主要用于<strong>推理阶段</strong>，不是训练阶段</li>
<li class="">训练时通常设置为 <code>False</code></li>
<li class="">GRPO 等训练任务不需要此优化</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="2-cuda-graph-详解">2. CUDA Graph 详解<a href="#2-cuda-graph-详解" class="hash-link" aria-label="Direct link to 2. CUDA Graph 详解" title="Direct link to 2. CUDA Graph 详解" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="21-什么是-cuda-graph">2.1 什么是 CUDA Graph<a href="#21-什么是-cuda-graph" class="hash-link" aria-label="Direct link to 2.1 什么是 CUDA Graph" title="Direct link to 2.1 什么是 CUDA Graph" translate="no">​</a></h3>
<p><strong>CUDA Graph</strong> 是 NVIDIA CUDA 提供的一种优化技术，用于减少 CPU 开销和提高 GPU 利用率。</p>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="传统-cuda-执行流程">传统 CUDA 执行流程<a href="#传统-cuda-执行流程" class="hash-link" aria-label="Direct link to 传统 CUDA 执行流程" title="Direct link to 传统 CUDA 执行流程" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">CPU: 准备参数 → 启动 Kernel 1 → 等待 → 启动 Kernel 2 → 等待 → ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GPU:              执行 Kernel 1         执行 Kernel 2</span><br></span></code></pre></div></div>
<p><strong>问题：</strong></p>
<ul>
<li class="">每次 Kernel 启动都需要 CPU-GPU 通信</li>
<li class="">CPU 开销大</li>
<li class="">延迟高</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="cuda-graph-执行流程">CUDA Graph 执行流程<a href="#cuda-graph-执行流程" class="hash-link" aria-label="Direct link to CUDA Graph 执行流程" title="Direct link to CUDA Graph 执行流程" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">CPU: 捕获图 (一次) → 重放图 → 重放图 → 重放图 → ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GPU:                 执行整个图  执行整个图  执行整个图</span><br></span></code></pre></div></div>
<p><strong>优势：</strong></p>
<ul>
<li class="">一次捕获，多次重放</li>
<li class="">减少 CPU 开销（可降低 50% 以上）</li>
<li class="">降低延迟</li>
<li class="">提高吞吐量</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="22-cuda-graph-的工作原理">2.2 CUDA Graph 的工作原理<a href="#22-cuda-graph-的工作原理" class="hash-link" aria-label="Direct link to 2.2 CUDA Graph 的工作原理" title="Direct link to 2.2 CUDA Graph 的工作原理" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="捕获阶段-capture">捕获阶段 (Capture)<a href="#捕获阶段-capture" class="hash-link" aria-label="Direct link to 捕获阶段 (Capture)" title="Direct link to 捕获阶段 (Capture)" translate="no">​</a></h4>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># PyTorch 示例</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">with</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">graph</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cuda_graph</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 执行一次前向传播</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">input</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<ul>
<li class="">记录所有 CUDA 操作</li>
<li class="">构建计算图</li>
<li class="">保存参数和依赖关系</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="重放阶段-replay">重放阶段 (Replay)<a href="#重放阶段-replay" class="hash-link" aria-label="Direct link to 重放阶段 (Replay)" title="Direct link to 重放阶段 (Replay)" translate="no">​</a></h4>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 重放图</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda_graph</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">replay</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<ul>
<li class="">直接执行预先捕获的操作序列</li>
<li class="">无需 CPU 参与</li>
<li class="">极低延迟</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="23-cuda-graph-的限制">2.3 CUDA Graph 的限制<a href="#23-cuda-graph-的限制" class="hash-link" aria-label="Direct link to 2.3 CUDA Graph 的限制" title="Direct link to 2.3 CUDA Graph 的限制" translate="no">​</a></h3>
<p><strong>不支持的操作：</strong></p>
<ul>
<li class="">动态形状（shape 必须固定）</li>
<li class="">动态控制流（if/while 等）</li>
<li class="">CPU-GPU 同步操作</li>
<li class="">某些特殊的 Attention 实现</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li class="">批次大小固定</li>
<li class="">模型结构固定</li>
<li class="">重复执行相同操作</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="3-vllm-的-cuda-graph-模式">3. vLLM 的 CUDA Graph 模式<a href="#3-vllm-的-cuda-graph-模式" class="hash-link" aria-label="Direct link to 3. vLLM 的 CUDA Graph 模式" title="Direct link to 3. vLLM 的 CUDA Graph 模式" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="31-三种模式详解">3.1 三种模式详解<a href="#31-三种模式详解" class="hash-link" aria-label="Direct link to 3.1 三种模式详解" title="Direct link to 3.1 三种模式详解" translate="no">​</a></h3>
<p>vLLM 通过 <code>VLLM_CUDAGRAPH_MODE</code> 环境变量控制 CUDA Graph 的使用方式。</p>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="模式-1-never-从不使用">模式 1: NEVER (从不使用)<a href="#模式-1-never-从不使用" class="hash-link" aria-label="Direct link to 模式 1: NEVER (从不使用)" title="Direct link to 模式 1: NEVER (从不使用)" translate="no">​</a></h4>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=NEVER</span><br></span></code></pre></div></div>
<p><strong>特点：</strong></p>
<ul>
<li class="">完全禁用 CUDA Graph</li>
<li class="">所有操作在 Eager 模式下执行</li>
<li class="">最灵活，支持动态形状</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li class="">调试和开发</li>
<li class="">使用不兼容的 Attention 后端</li>
<li class="">需要动态批次大小</li>
</ul>
<p><strong>性能：</strong></p>
<ul>
<li class="">最慢</li>
<li class="">CPU 开销最大</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="模式-2-piecewise-分段捕获">模式 2: PIECEWISE (分段捕获)<a href="#模式-2-piecewise-分段捕获" class="hash-link" aria-label="Direct link to 模式 2: PIECEWISE (分段捕获)" title="Direct link to 模式 2: PIECEWISE (分段捕获)" translate="no">​</a></h4>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=PIECEWISE</span><br></span></code></pre></div></div>
<p><strong>特点：</strong></p>
<ul>
<li class="">将计算图分段捕获</li>
<li class="">只对兼容的部分使用 CUDA Graph</li>
<li class="">Attention 操作在 Eager 模式下执行</li>
<li class="">其他操作（FFN、LayerNorm 等）使用 CUDA Graph</li>
</ul>
<p><strong>工作原理：</strong></p>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">[CUDA Graph] → [Eager Attention] → [CUDA Graph] → [Eager Attention] → ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   FFN/Norm        Attention           FFN/Norm        Attention</span><br></span></code></pre></div></div>
<p><strong>适用场景：</strong></p>
<ul>
<li class="">使用 FlexAttention 等特殊 Attention 实现</li>
<li class="">需要灵活性和性能的平衡</li>
<li class=""><strong>vLLM V1 架构的默认模式</strong></li>
</ul>
<p><strong>性能：</strong></p>
<ul>
<li class="">中等</li>
<li class="">平衡灵活性和速度</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="模式-3-full-完全捕获">模式 3: FULL (完全捕获)<a href="#模式-3-full-完全捕获" class="hash-link" aria-label="Direct link to 模式 3: FULL (完全捕获)" title="Direct link to 模式 3: FULL (完全捕获)" translate="no">​</a></h4>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=FULL</span><br></span></code></pre></div></div>
<p><strong>特点：</strong></p>
<ul>
<li class="">捕获整个计算图</li>
<li class="">包括 Attention 操作</li>
<li class="">最大化性能</li>
</ul>
<p><strong>要求：</strong></p>
<ul>
<li class="">Attention 后端必须支持 CUDA Graph</li>
<li class="">批次大小固定</li>
<li class="">所有操作必须兼容</li>
</ul>
<p><strong>适用场景：</strong></p>
<ul>
<li class="">使用兼容的 Attention 后端（如 FlashAttention）</li>
<li class="">批次大小固定</li>
<li class="">追求极致性能</li>
</ul>
<p><strong>性能：</strong></p>
<ul>
<li class="">最快</li>
<li class="">CPU 开销最小</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="32-模式对比表">3.2 模式对比表<a href="#32-模式对比表" class="hash-link" aria-label="Direct link to 3.2 模式对比表" title="Direct link to 3.2 模式对比表" translate="no">​</a></h3>
<table><thead><tr><th>特性</th><th>NEVER</th><th>PIECEWISE</th><th>FULL</th></tr></thead><tbody><tr><td><strong>性能</strong></td><td>慢</td><td>中等</td><td>快</td></tr><tr><td><strong>灵活性</strong></td><td>高</td><td>中等</td><td>低</td></tr><tr><td><strong>Attention 支持</strong></td><td>所有</td><td>所有</td><td>仅兼容的</td></tr><tr><td><strong>动态批次</strong></td><td>支持</td><td>部分支持</td><td>不支持</td></tr><tr><td><strong>CPU 开销</strong></td><td>高</td><td>中等</td><td>低</td></tr><tr><td><strong>推荐场景</strong></td><td>调试/开发</td><td>生产环境（默认）</td><td>高性能推理</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="33-vllm-v1-架构的-piecewise-实现">3.3 vLLM V1 架构的 Piecewise 实现<a href="#33-vllm-v1-架构的-piecewise-实现" class="hash-link" aria-label="Direct link to 3.3 vLLM V1 架构的 Piecewise 实现" title="Direct link to 3.3 vLLM V1 架构的 Piecewise 实现" translate="no">​</a></h3>
<p>vLLM V1 使用 <strong>Piecewise CUDA Graph</strong> 作为默认模式：</p>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">计算图分割：</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">┌─────────────────────────────────────────────────┐</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│ Layer 1                                         │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─ [CUDA Graph] Input Processing               │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─ [Eager] Attention Operation                 │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  └─ [CUDA Graph] FFN + LayerNorm                │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─────────────────────────────────────────────────┤</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│ Layer 2                                         │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─ [CUDA Graph] (复用 Layer 1 的图)            │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─ [Eager] Attention Operation                 │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  └─ [CUDA Graph] (复用 Layer 1 的图)            │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─────────────────────────────────────────────────┤</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│ ...                                             │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└─────────────────────────────────────────────────┘</span><br></span></code></pre></div></div>
<p><strong>优势：</strong></p>
<ul>
<li class="">保持 Attention 的灵活性</li>
<li class="">优化其他计算密集型操作</li>
<li class="">自动管理中间缓冲区</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="4-常见错误及解决方案">4. 常见错误及解决方案<a href="#4-常见错误及解决方案" class="hash-link" aria-label="Direct link to 4. 常见错误及解决方案" title="Direct link to 4. 常见错误及解决方案" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="41-flexattentionmetadatabuilder-错误">4.1 FlexAttentionMetadataBuilder 错误<a href="#41-flexattentionmetadatabuilder-错误" class="hash-link" aria-label="Direct link to 4.1 FlexAttentionMetadataBuilder 错误" title="Direct link to 4.1 FlexAttentionMetadataBuilder 错误" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="错误信息">错误信息<a href="#错误信息" class="hash-link" aria-label="Direct link to 错误信息" title="Direct link to 错误信息" translate="no">​</a></h4>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">ValueError: CUDAGraphMode.FULL is not supported with FlexAttentionMetadataBuilder </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">backend (support: AttentionCGSupport.NEVER); please try cudagraph_mode=PIECEWISE</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="原因">原因<a href="#原因" class="hash-link" aria-label="Direct link to 原因" title="Direct link to 原因" translate="no">​</a></h4>
<ul>
<li class="">使用了 FlexAttention 后端</li>
<li class="">FlexAttention 不支持 FULL 模式的 CUDA Graph</li>
<li class="">vLLM 默认尝试使用 FULL 模式</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="解决方案">解决方案<a href="#解决方案" class="hash-link" aria-label="Direct link to 解决方案" title="Direct link to 解决方案" translate="no">​</a></h4>
<p><strong>方案 1：设置环境变量（推荐）</strong></p>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;PIECEWISE&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># ... 其他参数</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>方案 2：禁用快速推理（训练时）</strong></p>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 训练时不需要 vLLM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># ... 其他参数</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>方案 3：使用兼容的 Attention 后端</strong></p>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 在 vLLM 配置中指定 Attention 后端</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 例如使用 FlashAttention</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="42-训练-vs-推理的配置">4.2 训练 vs 推理的配置<a href="#42-训练-vs-推理的配置" class="hash-link" aria-label="Direct link to 4.2 训练 vs 推理的配置" title="Direct link to 4.2 训练 vs 推理的配置" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="训练阶段grpolora-等">训练阶段（GRPO、LoRA 等）<a href="#训练阶段grpolora-等" class="hash-link" aria-label="Direct link to 训练阶段（GRPO、LoRA 等）" title="Direct link to 训练阶段（GRPO、LoRA 等）" translate="no">​</a></h4>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_seq_length</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2048</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    load_in_4bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># ❌ 训练时禁用</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_lora_rank</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>原因：</strong></p>
<ul>
<li class="">训练不需要 vLLM 的推理优化</li>
<li class="">避免 CUDA Graph 相关问题</li>
<li class="">简化训练流程</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_FzkC" id="推理阶段">推理阶段<a href="#推理阶段" class="hash-link" aria-label="Direct link to 推理阶段" title="Direct link to 推理阶段" translate="no">​</a></h4>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;PIECEWISE&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_seq_length</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2048</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    load_in_4bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># ✅ 推理时启用</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    max_lora_rank</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="5-性能优化建议">5. 性能优化建议<a href="#5-性能优化建议" class="hash-link" aria-label="Direct link to 5. 性能优化建议" title="Direct link to 5. 性能优化建议" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="51-选择合适的-cuda-graph-模式">5.1 选择合适的 CUDA Graph 模式<a href="#51-选择合适的-cuda-graph-模式" class="hash-link" aria-label="Direct link to 5.1 选择合适的 CUDA Graph 模式" title="Direct link to 5.1 选择合适的 CUDA Graph 模式" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 开发/调试</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;NEVER&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 生产环境（推荐）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;PIECEWISE&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 极致性能（需要兼容的后端）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;FULL&quot;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="52-编译特定形状">5.2 编译特定形状<a href="#52-编译特定形状" class="hash-link" aria-label="Direct link to 5.2 编译特定形状" title="Direct link to 5.2 编译特定形状" translate="no">​</a></h3>
<p>vLLM 支持为特定批次大小编译优化的 Kernel：</p>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">vllm serve meta-llama/Llama-3.2-1B \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --compilation-config &#x27;{&quot;compile_sizes&quot;: [1, 2, 4, 8]}&#x27;</span><br></span></code></pre></div></div>
<p><strong>效果：</strong></p>
<ul>
<li class="">为常用批次大小生成优化代码</li>
<li class="">自动调优 Kernel 参数</li>
<li class="">显著提升性能</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="53-缓存编译结果">5.3 缓存编译结果<a href="#53-缓存编译结果" class="hash-link" aria-label="Direct link to 5.3 缓存编译结果" title="Direct link to 5.3 缓存编译结果" translate="no">​</a></h3>
<p>vLLM 会缓存编译结果到：</p>
<div class="language-text codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-text codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain">~/.cache/vllm/torch_compile_cache/</span><br></span></code></pre></div></div>
<p><strong>建议：</strong></p>
<ul>
<li class="">在部署时复制缓存目录</li>
<li class="">避免重复编译</li>
<li class="">加快启动速度</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="6-参考资源">6. 参考资源<a href="#6-参考资源" class="hash-link" aria-label="Direct link to 6. 参考资源" title="Direct link to 6. 参考资源" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="官方文档">官方文档<a href="#官方文档" class="hash-link" aria-label="Direct link to 官方文档" title="Direct link to 官方文档" translate="no">​</a></h3>
<ul>
<li class=""><a href="https://docs.vllm.ai/" target="_blank" rel="noopener noreferrer" class="">vLLM 官方文档</a></li>
<li class=""><a href="https://docs.vllm.ai/en/latest/design/torch_compile.html" target="_blank" rel="noopener noreferrer" class="">vLLM torch.compile 集成</a></li>
<li class=""><a href="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/" target="_blank" rel="noopener noreferrer" class="">PyTorch CUDA Graphs</a></li>
<li class=""><a href="https://docs.unsloth.ai/" target="_blank" rel="noopener noreferrer" class="">Unsloth 文档</a></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="相关-issues">相关 Issues<a href="#相关-issues" class="hash-link" aria-label="Direct link to 相关 Issues" title="Direct link to 相关 Issues" translate="no">​</a></h3>
<ul>
<li class=""><a href="https://github.com/vllm-project/vllm/issues/24943" target="_blank" rel="noopener noreferrer" class="">vLLM #24943 - CUDAGraphMode PIECEWISE</a></li>
<li class=""><a href="https://github.com/vllm-project/vllm/issues/23261" target="_blank" rel="noopener noreferrer" class="">vLLM #23261 - Piecewise Graph Splitting</a></li>
<li class=""><a href="https://github.com/unslothai/unsloth/issues/2846" target="_blank" rel="noopener noreferrer" class="">Unsloth #2846 - vLLM Fast Inference</a></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="博客文章">博客文章<a href="#博客文章" class="hash-link" aria-label="Direct link to 博客文章" title="Direct link to 博客文章" translate="no">​</a></h3>
<ul>
<li class=""><a href="https://developers.redhat.com/articles/2025/01/28/vllm-v1-a-major-upgrade-vllms-core-architecture" target="_blank" rel="noopener noreferrer" class="">vLLM V1 Alpha Release</a></li>
<li class=""><a href="https://unsloth.ai/blog/r1-reasoning" target="_blank" rel="noopener noreferrer" class="">Unsloth GRPO Guide</a></li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="7-快速参考">7. 快速参考<a href="#7-快速参考" class="hash-link" aria-label="Direct link to 7. 快速参考" title="Direct link to 7. 快速参考" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="环境变量速查">环境变量速查<a href="#环境变量速查" class="hash-link" aria-label="Direct link to 环境变量速查" title="Direct link to 环境变量速查" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-bash codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token plain"># CUDA Graph 模式</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=NEVER      # 禁用</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=PIECEWISE  # 分段（默认）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_CUDAGRAPH_MODE=FULL       # 完全</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 禁用编译缓存（调试用）</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_DISABLE_COMPILE_CACHE=1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># 日志级别</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">export VLLM_LOGGING_LEVEL=DEBUG</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_FzkC" id="代码模板">代码模板<a href="#代码模板" class="hash-link" aria-label="Direct link to 代码模板" title="Direct link to 代码模板" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_iV14 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_jh8H"><pre tabindex="0" class="prism-code language-python codeBlock_E9uv thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_Kv4h"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 训练配置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">setup_for_training</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_seq_length</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2048</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        load_in_4bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 训练时禁用</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_lora_rank</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 推理配置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">setup_for_inference</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    os</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">&quot;VLLM_CUDAGRAPH_MODE&quot;</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;PIECEWISE&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FastLanguageModel</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">from_pretrained</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model_name</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;unsloth/Qwen3-4B-Base&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_seq_length</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2048</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        load_in_4bit</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fast_inference</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 推理时启用</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_lora_rank</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tokenizer</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_FzkC" id="更新日志">更新日志<a href="#更新日志" class="hash-link" aria-label="Direct link to 更新日志" title="Direct link to 更新日志" translate="no">​</a></h2>
<ul>
<li class="">2025-10-10: 初始版本，整理 vLLM 快速推理和 CUDA Graph 相关知识</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_RM2i"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI/vllm/vllm-fast-inference-and-cuda-graph.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NBA7" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_Ng_4"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/notes3/docs/AI/vllm/vLLM性能调优指南"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">vLLM 性能调优指南</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/notes3/docs/AI/vllm/安装常见错误及解决方案"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">安装常见错误及解决方案</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_qSfz thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-什么是快速推理-fast-inference" class="table-of-contents__link toc-highlight">1. 什么是快速推理 (Fast Inference)</a><ul><li><a href="#11-定义" class="table-of-contents__link toc-highlight">1.1 定义</a></li><li><a href="#12-vllm-的核心优化技术" class="table-of-contents__link toc-highlight">1.2 vLLM 的核心优化技术</a></li><li><a href="#13-在-unsloth-中的应用" class="table-of-contents__link toc-highlight">1.3 在 Unsloth 中的应用</a></li></ul></li><li><a href="#2-cuda-graph-详解" class="table-of-contents__link toc-highlight">2. CUDA Graph 详解</a><ul><li><a href="#21-什么是-cuda-graph" class="table-of-contents__link toc-highlight">2.1 什么是 CUDA Graph</a></li><li><a href="#22-cuda-graph-的工作原理" class="table-of-contents__link toc-highlight">2.2 CUDA Graph 的工作原理</a></li><li><a href="#23-cuda-graph-的限制" class="table-of-contents__link toc-highlight">2.3 CUDA Graph 的限制</a></li></ul></li><li><a href="#3-vllm-的-cuda-graph-模式" class="table-of-contents__link toc-highlight">3. vLLM 的 CUDA Graph 模式</a><ul><li><a href="#31-三种模式详解" class="table-of-contents__link toc-highlight">3.1 三种模式详解</a></li><li><a href="#32-模式对比表" class="table-of-contents__link toc-highlight">3.2 模式对比表</a></li><li><a href="#33-vllm-v1-架构的-piecewise-实现" class="table-of-contents__link toc-highlight">3.3 vLLM V1 架构的 Piecewise 实现</a></li></ul></li><li><a href="#4-常见错误及解决方案" class="table-of-contents__link toc-highlight">4. 常见错误及解决方案</a><ul><li><a href="#41-flexattentionmetadatabuilder-错误" class="table-of-contents__link toc-highlight">4.1 FlexAttentionMetadataBuilder 错误</a></li><li><a href="#42-训练-vs-推理的配置" class="table-of-contents__link toc-highlight">4.2 训练 vs 推理的配置</a></li></ul></li><li><a href="#5-性能优化建议" class="table-of-contents__link toc-highlight">5. 性能优化建议</a><ul><li><a href="#51-选择合适的-cuda-graph-模式" class="table-of-contents__link toc-highlight">5.1 选择合适的 CUDA Graph 模式</a></li><li><a href="#52-编译特定形状" class="table-of-contents__link toc-highlight">5.2 编译特定形状</a></li><li><a href="#53-缓存编译结果" class="table-of-contents__link toc-highlight">5.3 缓存编译结果</a></li></ul></li><li><a href="#6-参考资源" class="table-of-contents__link toc-highlight">6. 参考资源</a><ul><li><a href="#官方文档" class="table-of-contents__link toc-highlight">官方文档</a></li><li><a href="#相关-issues" class="table-of-contents__link toc-highlight">相关 Issues</a></li><li><a href="#博客文章" class="table-of-contents__link toc-highlight">博客文章</a></li></ul></li><li><a href="#7-快速参考" class="table-of-contents__link toc-highlight">7. 快速参考</a><ul><li><a href="#环境变量速查" class="table-of-contents__link toc-highlight">环境变量速查</a></li><li><a href="#代码模板" class="table-of-contents__link toc-highlight">代码模板</a></li></ul></li><li><a href="#更新日志" class="table-of-contents__link toc-highlight">更新日志</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/notes3/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_EO6u"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>