---
sidebar_position: 2
---
在深度学习中，**Tokenization（标记化）**和**Embedding（嵌入）** 是处理文本数据的关键步骤，分别解决不同的问题，共同将人类可读的文本转化为模型可理解的数值形式。以下是它们的核心作用及区别：

---

### 一、**Tokenization（标记化）：解决文本的结构化分解问题**
#### **1. 是什么？**
将原始文本（如句子、段落）分割成更小的单元（称为 **token**），例如：
- **单词**：`"I love machine learning"` → `["I", "love", "machine", "learning"]`
- **子词**（Subword）：`"unhappy"` → `["un", "happy"]`（解决未登录词问题）
- **字符**：`"cat"` → `["c", "a", "t"]`

#### **2. 为什么要做？**
- **计算机无法直接处理文本**：模型需要数值输入，而文本是符号化的。
- **统一文本表示**：处理大小写、标点、缩写（如`"don't"`→`["do", "n't"]`）。
- **应对语言复杂性**：
  - 中文无空格：`"我爱机器学习"`→`["我", "爱", "机器学习"]`。
  - 处理罕见词：通过子词（如`BPE`、`WordPiece`）拆分，避免模型遇到未见过词汇（OOV）。

#### **3. 常见方法**
- **空格分词**：简单但无法处理复合词（如德语`"Lebensversicherungsgesellschaft"`）。
- **子词分词**：
  - **BPE（Byte-Pair Encoding）**：合并高频字符对（如将`"low"`和`"lower"`拆分为`["low", "er"]`）。
  - **WordPiece**（被BERT采用）：基于概率合并子词。
- **SentencePiece**：支持多语言，直接处理原始文本（包括空格）。

---

### 二、**Embedding（嵌入）：解决语义的数值化表示问题**
#### **1. 是什么？**
将每个 **token** 映射为一个低维稠密向量（例如300维），捕捉其语义信息。  
例如：`"cat"` → `[0.2, -0.5, 1.3, ...]`，`"dog"` → `[0.3, -0.4, 1.2, ...]`（两者向量接近）。

#### **2. 为什么要做？**
- **符号→数值转换**：模型只能处理数值，但简单的One-Hot编码（如`[0,0,1,0]`）维度高且无法表达语义。
- **捕捉语义关系**：
  - 相似词距离近：`"king"`和`"queen"`的向量接近。
  - 类比关系：`"king" - "man" + "woman" ≈ "queen"`。
- **降维与泛化**：低维向量（如300维）比One-Hot（数万维）更高效，且能泛化到未见的用法。

#### **3. 常见方法**
- **静态嵌入**（预训练）：
  - **Word2Vec**：基于上下文预测（Skip-Gram/CBOW）。
  - **GloVe**：基于全局词共现矩阵。
- **动态嵌入**（上下文相关）：
  - **Transformer模型**（如BERT、GPT）：同一词在不同上下文的向量不同。  
    例如，`"bank"`在`"river bank"`和`"bank account"`中的向量不同。

---

### 三、两者的关系与协作
#### **1. 处理流程**
```text
原始文本 → Tokenization → Tokens → Embedding → 向量序列 → 输入模型
```

#### **2. 协作示例**
假设输入句子：`"The cat sat on the mat."`
1. **Tokenization**：分割为 `["The", "cat", "sat", "on", "the", "mat", "."]`。
2. **Embedding**：每个token转换为向量，形成矩阵（形状：`7 tokens × 300 dim`）。

#### **3. 关键区别**
|                | Tokenization               | Embedding                  |
|----------------|----------------------------|----------------------------|
| **目标**       | 结构化分解文本             | 语义的数值化表示           |
| **输入/输出**  | 文本 → Tokens              | Tokens → 向量              |
| **核心问题**   | 如何处理复杂语言结构？     | 如何让向量保留语义信息？   |

---

### 四、实际应用中的挑战
#### **1. Tokenization的挑战**
- **语言差异**：中文分词 vs. 英文空格分词。
- **未登录词（OOV）**：子词分词（如`BPE`）通过拆分解决。
- **歧义**：例如`"苹果"`（水果 vs. 公司）需要上下文区分（动态Embedding解决）。

#### **2. Embedding的挑战**
- **一词多义**：静态嵌入（如Word2Vec）无法处理，需动态嵌入（如BERT）。
- **计算效率**：大词表的嵌入层可能占用大量内存（可通过参数共享或压缩优化）。

---

### 五、总结
- **Tokenization**：将文本分解为模型可处理的基本单元（token），解决**结构化分解**问题。
- **Embedding**：将token映射为富含语义的向量，解决**语义表示**问题。

二者共同构建了文本到数值的桥梁，是深度学习模型（如BERT、GPT）处理语言任务的基础。