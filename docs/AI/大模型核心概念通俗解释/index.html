<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-AI/大模型核心概念通俗解释" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">大模型核心概念通俗解释 | Cruldra</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/notes3/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/notes3/docs/AI/大模型核心概念通俗解释"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="大模型核心概念通俗解释 | Cruldra"><meta data-rh="true" name="description" content="前言"><meta data-rh="true" property="og:description" content="前言"><link data-rh="true" rel="icon" href="/notes3/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/notes3/docs/AI/大模型核心概念通俗解释"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/大模型核心概念通俗解释" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/notes3/docs/AI/大模型核心概念通俗解释" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"大模型核心概念通俗解释","item":"https://your-docusaurus-site.example.com/notes3/docs/AI/大模型核心概念通俗解释"}]}</script><link rel="alternate" type="application/rss+xml" href="/notes3/blog/rss.xml" title="Cruldra RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/notes3/blog/atom.xml" title="Cruldra Atom Feed"><link rel="stylesheet" href="/notes3/assets/css/styles.da241bce.css">
<script src="/notes3/assets/js/runtime~main.983dbf1a.js" defer="defer"></script>
<script src="/notes3/assets/js/main.50610574.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>document.documentElement.setAttribute("data-theme",window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.documentElement.setAttribute("data-theme-choice","system"),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/notes3/img/logo.svg"><style data-mantine-styles="classes">@media (max-width: 35.99375em) {.mantine-visible-from-xs {display: none !important;}}@media (min-width: 36em) {.mantine-hidden-from-xs {display: none !important;}}@media (max-width: 47.99375em) {.mantine-visible-from-sm {display: none !important;}}@media (min-width: 48em) {.mantine-hidden-from-sm {display: none !important;}}@media (max-width: 61.99375em) {.mantine-visible-from-md {display: none !important;}}@media (min-width: 62em) {.mantine-hidden-from-md {display: none !important;}}@media (max-width: 74.99375em) {.mantine-visible-from-lg {display: none !important;}}@media (min-width: 75em) {.mantine-hidden-from-lg {display: none !important;}}@media (max-width: 87.99375em) {.mantine-visible-from-xl {display: none !important;}}@media (min-width: 88em) {.mantine-hidden-from-xl {display: none !important;}}</style><div role="region" aria-label="Skip to main content"><a class="skipToContent_HuOE" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/notes3/"><div class="navbar__logo"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_E0TY themedComponent--light_oQTo"><img src="/notes3/img/logo.svg" alt="My Site Logo" class="themedComponent_E0TY themedComponent--dark_xnBK"></div><b class="navbar__title text--truncate">Cruldra</b></a><a class="navbar__item navbar__link" href="/notes3/docs/category/设计模式">JVM</a><a class="navbar__item navbar__link" href="/notes3/docs/category/css">前端</a><a class="navbar__item navbar__link" href="/notes3/docs/category/astrbot">工具</a><a class="navbar__item navbar__link" href="/notes3/docs/category/ai电竞酒店">个人</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/notes3/docs/AI/Agno/介绍">AI</a><a class="navbar__item navbar__link" href="/notes3/docs/category/内置模块">Python</a><a class="navbar__item navbar__link" href="/notes3/docs/category/actix-web">Rust</a><a class="navbar__item navbar__link" href="/notes3/docs/category/数据库系统">软件工程</a><a class="navbar__item navbar__link" href="/notes3/docs/category/上古卷轴5">游戏</a><a class="navbar__item navbar__link" href="/notes3/docs/Go/Go语言常用语法">Go</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbarSearchContainer_lmNG"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_ec9K"><div class="docsWrapper_T_Kj"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_U0vv" type="button"></button><div class="docRoot_n2jZ"><aside class="theme-doc-sidebar-container docSidebarContainer_BcoR"><div class="sidebarViewport_Cc21"><div class="sidebar_OHx_"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_n4En"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/Agno/介绍"><span title="Agno" class="categoryLinkLabel_X2zX">Agno</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/DALLE成本计算"><span title="DALL-E 画图成本计算" class="linkLabel_j3lx">DALL-E 画图成本计算</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/MCP/MCP-Python-SDK"><span title="MCP" class="categoryLinkLabel_X2zX">MCP</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/Transformer"><span title="Transformer" class="linkLabel_j3lx">Transformer</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist" href="/notes3/docs/AI/vllm/"><span title="vLLM 学习资料" class="categoryLinkLabel_X2zX">vLLM 学习资料</span></a><button aria-label="Expand sidebar category &#x27;vLLM 学习资料&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念1"><span title="一些概念1" class="linkLabel_j3lx">一些概念1</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念2"><span title="一些概念2" class="linkLabel_j3lx">一些概念2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念3"><span title="一些概念3" class="linkLabel_j3lx">一些概念3</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念4"><span title="一些概念4" class="linkLabel_j3lx">一些概念4</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/一些概念5"><span title="一些概念5" class="linkLabel_j3lx">一些概念5</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/候选模型"><span title="候选模型" class="linkLabel_j3lx">候选模型</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/变分自编码器详解"><span title="变分自编码器（VAE）详解" class="linkLabel_j3lx">变分自编码器（VAE）详解</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/notes3/docs/AI/大模型核心概念通俗解释"><span title="大模型核心概念通俗解释" class="linkLabel_j3lx">大模型核心概念通俗解释</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/大模型训练中的随机种子：为什么需要它？"><span title="大模型训练中的随机种子：为什么需要它？" class="linkLabel_j3lx">大模型训练中的随机种子：为什么需要它？</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/嵌入和向量/Chroma与PGVector对比"><span title="嵌入和向量" class="categoryLinkLabel_X2zX">嵌入和向量</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/感知机"><span title="感知机" class="linkLabel_j3lx">感知机</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/智能体"><span title="智能体(Agent)" class="linkLabel_j3lx">智能体(Agent)</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/深度学习/入门"><span title="深度学习" class="categoryLinkLabel_X2zX">深度学习</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/理解大模型：从函数视角看AI的本质"><span title="理解大模型：从函数视角看AI的本质" class="linkLabel_j3lx">理解大模型：从函数视角看AI的本质</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/知识蒸馏"><span title="知识蒸馏" class="linkLabel_j3lx">知识蒸馏</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/联邦学习概念"><span title="联邦学习概念" class="linkLabel_j3lx">联邦学习概念</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_smok menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes3/docs/AI/通义千问/阿里云百炼"><span title="通义千问" class="categoryLinkLabel_X2zX">通义千问</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes3/docs/AI/金丝雀发布和AB测试"><span title="金丝雀发布和AB测试" class="linkLabel_j3lx">金丝雀发布和AB测试</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_vrpX"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_WIWH"><div class="docItemContainer_zhTd"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_kL0w" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/notes3/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_y6sH"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">大模型核心概念通俗解释</span></li></ul></nav><div class="tocCollapsible_sNig theme-doc-toc-mobile tocMobile_AD9K"><button type="button" class="clean-btn tocCollapsibleButton_dkdr">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>大模型核心概念通俗解释</h1></header>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="前言">前言<a href="#前言" class="hash-link" aria-label="Direct link to 前言" title="Direct link to 前言" translate="no">​</a></h2>
<p>大模型领域有很多专业术语，初学者往往被这些概念搞得云里雾里。本文用最通俗易懂的语言，帮你理解这些核心概念，就像和朋友聊天一样轻松。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="基础概念篇">基础概念篇<a href="#基础概念篇" class="hash-link" aria-label="Direct link to 基础概念篇" title="Direct link to 基础概念篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="分词-tokenization">分词 (Tokenization)<a href="#分词-tokenization" class="hash-link" aria-label="Direct link to 分词 (Tokenization)" title="Direct link to 分词 (Tokenization)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：把一句话切成小块，让计算机能理解。</p>
<p>想象你要教一个外国朋友中文，你会把&quot;我爱吃苹果&quot;拆成&quot;我&quot;、&quot;爱&quot;、&quot;吃&quot;、&quot;苹果&quot;这样的词汇。分词就是这个过程，把文本切成计算机能处理的最小单位。</p>
<p><strong>举例</strong>：</p>
<ul>
<li>中文：&quot;今天天气很好&quot; → [&quot;今天&quot;, &quot;天气&quot;, &quot;很&quot;, &quot;好&quot;]</li>
<li>英文：&quot;Hello world&quot; → [&quot;Hello&quot;, &quot;world&quot;] 或 [&quot;Hel&quot;, &quot;lo&quot;, &quot;wor&quot;, &quot;ld&quot;]</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="嵌入-embedding-和向量-vector">嵌入 (Embedding) 和向量 (Vector)<a href="#嵌入-embedding-和向量-vector" class="hash-link" aria-label="Direct link to 嵌入 (Embedding) 和向量 (Vector)" title="Direct link to 嵌入 (Embedding) 和向量 (Vector)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：把文字变成数字，让计算机能&quot;理解&quot;意思。</p>
<p>就像给每个词汇一个身份证号码，但这个号码不是随机的，而是根据词汇的意思来设计的。意思相近的词，号码也相近。</p>
<p><strong>举例</strong>：</p>
<ul>
<li>&quot;国王&quot; 可能变成 [0.8, 0.2, 0.9, ...]</li>
<li>&quot;皇帝&quot; 可能变成 [0.7, 0.3, 0.8, ...]</li>
<li>&quot;苹果&quot; 可能变成 [0.1, 0.9, 0.2, ...]</li>
</ul>
<p>你会发现&quot;国王&quot;和&quot;皇帝&quot;的数字比较接近，因为意思相似。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="潜在空间-latent-space">潜在空间 (Latent Space)<a href="#潜在空间-latent-space" class="hash-link" aria-label="Direct link to 潜在空间 (Latent Space)" title="Direct link to 潜在空间 (Latent Space)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：AI的&quot;想象世界&quot;，一个存储所有可能性的神秘空间。</p>
<p>想象一个巨大的图书馆，但这个图书馆很特殊：</p>
<p><strong>这个图书馆的特点</strong>：</p>
<ul>
<li><strong>不存储具体的书</strong>：而是存储&quot;书的精髓&quot;</li>
<li><strong>有神奇的地图</strong>：相似的书的精髓会放在相近的位置</li>
<li><strong>可以创造新书</strong>：在任意位置取一个&quot;精髓&quot;，就能生成一本全新的书</li>
</ul>
<p><strong>潜在空间就是这样的&quot;精髓图书馆&quot;</strong>：</p>
<p><strong>1. 压缩的智慧</strong>：</p>
<ul>
<li>把复杂的数据（图片、文字、声音）压缩成简单的&quot;密码&quot;</li>
<li>这个密码包含了原始数据的核心特征</li>
<li>就像把一整本书的内容浓缩成几个关键词</li>
</ul>
<p><strong>2. 有序的排列</strong>：</p>
<ul>
<li>相似的东西在潜在空间中距离很近</li>
<li>比如&quot;猫&quot;和&quot;狗&quot;的密码会比较接近</li>
<li>&quot;猫&quot;和&quot;飞机&quot;的密码就相距很远</li>
</ul>
<p><strong>3. 神奇的插值</strong>：</p>
<ul>
<li>在&quot;猫&quot;和&quot;狗&quot;之间取一个中间点</li>
<li>能生成一个&quot;既像猫又像狗&quot;的奇妙生物</li>
<li>这就是AI的&quot;想象力&quot;来源</li>
</ul>
<p><strong>生活中的比喻</strong>：</p>
<ul>
<li><strong>调色盘</strong>：红色和蓝色之间可以调出紫色</li>
<li><strong>音乐</strong>：古典音乐和流行音乐之间可以创造出新风格</li>
<li><strong>烹饪</strong>：中餐和西餐结合能创造出融合菜</li>
</ul>
<p><strong>潜在空间的应用</strong>：</p>
<ul>
<li><strong>图像生成</strong>：在潜在空间中&quot;漫步&quot;，生成各种图片</li>
<li><strong>风格转换</strong>：把一张照片的&quot;风格密码&quot;应用到另一张照片</li>
<li><strong>数据探索</strong>：理解数据的内在结构和关系</li>
<li><strong>创意设计</strong>：探索从未见过的设计可能性</li>
</ul>
<p><strong>为什么重要</strong>：
潜在空间是AI&quot;创造力&quot;的核心。它让AI不只是复制已有的东西，而是能够理解、组合、创新。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="注意力机制-attention">注意力机制 (Attention)<a href="#注意力机制-attention" class="hash-link" aria-label="Direct link to 注意力机制 (Attention)" title="Direct link to 注意力机制 (Attention)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：让模型知道应该重点关注哪些词。</p>
<p>就像你在读一篇文章时，会自动把注意力放在重要的词汇上。比如在句子&quot;小明的妈妈给他买了一个苹果&quot;中，如果问题是&quot;谁买了苹果？&quot;，注意力就会集中在&quot;妈妈&quot;这个词上。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="transformer">Transformer<a href="#transformer" class="hash-link" aria-label="Direct link to Transformer" title="Direct link to Transformer" translate="no">​</a></h3>
<p><strong>简单理解</strong>：目前最流行的AI模型架构，就像是一个超级聪明的翻译官。</p>
<p>Transformer就像一个有很多层的翻译系统，每一层都在理解和处理信息，最终给出最好的答案。GPT、BERT这些著名模型都是基于Transformer的。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="训练过程篇">训练过程篇<a href="#训练过程篇" class="hash-link" aria-label="Direct link to 训练过程篇" title="Direct link to 训练过程篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="前向传播-forward-propagation">前向传播 (Forward Propagation)<a href="#前向传播-forward-propagation" class="hash-link" aria-label="Direct link to 前向传播 (Forward Propagation)" title="Direct link to 前向传播 (Forward Propagation)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：信息从输入到输出的流动过程。</p>
<p>就像工厂的流水线，原材料（输入）经过一道道工序（神经网络层），最终变成产品（输出）。</p>
<p><strong>过程</strong>：输入文本 → 分词 → 嵌入 → 多层处理 → 输出结果</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="反向传播-backpropagation">反向传播 (Backpropagation)<a href="#反向传播-backpropagation" class="hash-link" aria-label="Direct link to 反向传播 (Backpropagation)" title="Direct link to 反向传播 (Backpropagation)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：发现错误后，倒推回去找出问题在哪里并修正。</p>
<p>就像考试后老师改卷子，发现学生答错了，就要找出是哪个知识点没掌握好，然后重点讲解。模型也是这样，通过错误来学习和改进。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="梯度-gradient">梯度 (Gradient)<a href="#梯度-gradient" class="hash-link" aria-label="Direct link to 梯度 (Gradient)" title="Direct link to 梯度 (Gradient)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：指出改进方向的&quot;指南针&quot;。</p>
<p>想象你在爬山找最高点，梯度就像指南针，告诉你应该往哪个方向走才能爬得更高（或者在机器学习中，是为了找到最低的错误率）。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="损失函数-loss-function">损失函数 (Loss Function)<a href="#损失函数-loss-function" class="hash-link" aria-label="Direct link to 损失函数 (Loss Function)" title="Direct link to 损失函数 (Loss Function)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：衡量模型表现好坏的&quot;评分标准&quot;。</p>
<p>就像考试的评分标准，告诉你答案离正确答案有多远。损失越小，说明模型表现越好。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="过拟合-overfitting">过拟合 (Overfitting)<a href="#过拟合-overfitting" class="hash-link" aria-label="Direct link to 过拟合 (Overfitting)" title="Direct link to 过拟合 (Overfitting)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：AI&quot;死记硬背&quot;了训练数据，但不会举一反三。</p>
<p>想象一个学生准备考试，他把所有练习题的答案都背得滚瓜烂熟，但考试时遇到新题型就不会做了。过拟合就是这种情况：</p>
<p><strong>过拟合的表现</strong>：</p>
<ul>
<li><strong>训练数据表现很好</strong>：在练习题上得满分</li>
<li><strong>新数据表现很差</strong>：遇到新题就不会做</li>
<li><strong>缺乏泛化能力</strong>：只会死记硬背，不会灵活应用</li>
</ul>
<p><strong>生活中的比喻</strong>：</p>
<ul>
<li><strong>背书式学习</strong>：只会背课文，不理解意思</li>
<li><strong>应试教育</strong>：只会做练习册上的题，不会解决实际问题</li>
<li><strong>照搬经验</strong>：只会按照固定套路做事，遇到新情况就慌了</li>
</ul>
<p><strong>过拟合的原因</strong>：</p>
<ul>
<li><strong>模型太复杂</strong>：就像用大炮打蚊子，能力过强</li>
<li><strong>训练数据太少</strong>：练习题太少，没见过足够多的题型</li>
<li><strong>训练时间太长</strong>：过度训练，把噪声也当成规律学了</li>
</ul>
<p><strong>如何识别过拟合</strong>：</p>
<ul>
<li>训练准确率很高，但验证准确率很低</li>
<li>训练损失持续下降，但验证损失开始上升</li>
<li>模型在训练集上表现完美，在测试集上表现糟糕</li>
</ul>
<p><strong>解决过拟合的方法</strong>：</p>
<ul>
<li><strong>早停 (Early Stopping)</strong>：发现过拟合苗头就停止训练</li>
<li><strong>正则化</strong>：给模型加一些&quot;约束&quot;，防止过度复杂</li>
<li><strong>数据增强</strong>：增加更多训练数据，见识更多题型</li>
<li><strong>Dropout</strong>：训练时随机&quot;关闭&quot;一些神经元，防止依赖特定路径</li>
<li><strong>交叉验证</strong>：用不同的数据集验证模型性能</li>
</ul>
<p><strong>与欠拟合的对比</strong>：</p>
<ul>
<li><strong>欠拟合</strong>：学习能力不足，连练习题都做不好</li>
<li><strong>过拟合</strong>：学习过度，只会做练习题，不会做新题</li>
<li><strong>刚好合适</strong>：既能做好练习题，也能应对新题型</li>
</ul>
<p><strong>实际意义</strong>：过拟合是机器学习中最常见的问题之一，理解并预防过拟合是训练好模型的关键。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="学习率-learning-rate">学习率 (Learning Rate)<a href="#学习率-learning-rate" class="hash-link" aria-label="Direct link to 学习率 (Learning Rate)" title="Direct link to 学习率 (Learning Rate)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：控制AI学习&quot;步伐大小&quot;的参数。</p>
<p>想象你在爬山找最高点，学习率就是你每次迈步的大小：</p>
<p><strong>学习率太大</strong>：</p>
<ul>
<li>就像迈步太大，可能一下子跨过山顶，永远找不到最高点</li>
<li>模型训练不稳定，可能错过最优解</li>
</ul>
<p><strong>学习率太小</strong>：</p>
<ul>
<li>就像迈步太小，爬山速度很慢，可能永远到不了山顶</li>
<li>模型训练很慢，可能陷在局部最优解</li>
</ul>
<p><strong>学习率合适</strong>：</p>
<ul>
<li>步伐刚好，既能稳步前进，又不会错过目标</li>
<li>模型训练稳定且高效</li>
</ul>
<p><strong>常见取值</strong>：</p>
<ul>
<li>通常在 0.001 到 0.1 之间</li>
<li>需要根据具体任务调整</li>
<li>训练过程中通常会逐渐减小（这就是调度器的作用）</li>
</ul>
<p><strong>实际意义</strong>：学习率决定了每次训练后权重调整的幅度，是影响训练效果的关键参数之一。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="超参数-hyperparameters">超参数 (Hyperparameters)<a href="#超参数-hyperparameters" class="hash-link" aria-label="Direct link to 超参数 (Hyperparameters)" title="Direct link to 超参数 (Hyperparameters)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：训练AI模型时需要人工设定的&quot;配置选项&quot;。</p>
<p>想象你在烹饪一道菜，超参数就像是你需要决定的各种设置：</p>
<p><strong>烹饪中的&quot;超参数&quot;</strong>：</p>
<ul>
<li><strong>火候大小</strong>：大火、中火、小火（对应学习率）</li>
<li><strong>烹饪时间</strong>：炒几分钟（对应训练轮数）</li>
<li><strong>调料分量</strong>：盐放多少、糖放多少（对应各种权重衰减系数）</li>
<li><strong>锅的大小</strong>：用大锅还是小锅（对应批次大小）</li>
</ul>
<p><strong>AI训练中的超参数</strong>：</p>
<ul>
<li><strong>学习率</strong>：模型学习的快慢</li>
<li><strong>批次大小 (Batch Size)</strong>：每次训练用多少数据</li>
<li><strong>训练轮数 (Epochs)</strong>：整个数据集训练多少遍</li>
<li><strong>网络层数</strong>：模型有多少层</li>
<li><strong>隐藏单元数</strong>：每层有多少神经元</li>
<li><strong>正则化系数</strong>：防止过拟合的强度</li>
</ul>
<p><strong>超参数 vs 参数的区别</strong>：</p>
<ul>
<li><strong>参数</strong>：模型自己学会的（如权重），训练过程中自动调整</li>
<li><strong>超参数</strong>：人工设定的（如学习率），训练开始前就要确定</li>
</ul>
<p><strong>为什么重要</strong>：</p>
<ul>
<li><strong>决定训练效果</strong>：超参数设置不当，模型可能学不好</li>
<li><strong>影响训练效率</strong>：好的超参数能让训练更快更稳定</li>
<li><strong>需要经验和实验</strong>：通常需要多次尝试才能找到最佳组合</li>
</ul>
<p><strong>如何选择超参数</strong>：</p>
<ul>
<li><strong>网格搜索</strong>：尝试所有可能的组合</li>
<li><strong>随机搜索</strong>：随机尝试一些组合</li>
<li><strong>贝叶斯优化</strong>：用智能方法寻找最优组合</li>
<li><strong>经验法则</strong>：参考别人的成功经验</li>
</ul>
<p><strong>常见的超参数调优策略</strong>：</p>
<ul>
<li><strong>先粗调后细调</strong>：先大范围搜索，再精细调整</li>
<li><strong>一次调一个</strong>：避免同时改变太多参数</li>
<li><strong>记录实验结果</strong>：方便对比和复现</li>
</ul>
<p><strong>实际意义</strong>：超参数调优是机器学习工程师的核心技能，好的超参数设置往往决定了项目的成败。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="张量-tensor">张量 (Tensor)<a href="#张量-tensor" class="hash-link" aria-label="Direct link to 张量 (Tensor)" title="Direct link to 张量 (Tensor)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：多维数组，是AI计算的基本数据结构。</p>
<p>想象一个魔方，它有长、宽、高三个维度。张量就像这样的多维数据容器：</p>
<ul>
<li><strong>1维张量</strong>：一排数字 [1, 2, 3, 4]（像一条线）</li>
<li><strong>2维张量</strong>：一个表格（像一张纸）</li>
<li><strong>3维张量</strong>：多张表格叠在一起（像一本书）</li>
<li><strong>更高维</strong>：更复杂的数据结构</li>
</ul>
<p><strong>在AI中的作用</strong>：所有的文字、图片、声音最终都会变成张量来处理。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="权重-weights-和参数-parameters">权重 (Weights) 和参数 (Parameters)<a href="#权重-weights-和参数-parameters" class="hash-link" aria-label="Direct link to 权重 (Weights) 和参数 (Parameters)" title="Direct link to 权重 (Weights) 和参数 (Parameters)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：模型的&quot;知识&quot;和&quot;技能&quot;都存储在这些数字里。</p>
<p>想象大脑中的神经连接，每个连接都有不同的强度。权重就像这些连接的强度值，决定了信息如何在网络中流动。</p>
<p><strong>举例</strong>：</p>
<ul>
<li>一个简单的模型可能有几千个权重</li>
<li>GPT-3有1750亿个参数</li>
<li>GPT-4可能有上万亿个参数</li>
</ul>
<p><strong>训练过程</strong>：就是不断调整这些权重，让模型给出更好的答案。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="检查点-checkpoint">检查点 (Checkpoint)<a href="#检查点-checkpoint" class="hash-link" aria-label="Direct link to 检查点 (Checkpoint)" title="Direct link to 检查点 (Checkpoint)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：训练过程中的&quot;存档点&quot;。</p>
<p>就像玩游戏时的存档，训练AI模型需要很长时间，检查点让你可以：</p>
<ul>
<li><strong>保存进度</strong>：避免训练中断后从头开始</li>
<li><strong>回滚</strong>：如果训练出现问题，可以回到之前的状态</li>
<li><strong>对比</strong>：比较不同阶段的模型性能</li>
<li><strong>部署</strong>：选择最好的版本用于实际应用</li>
</ul>
<p><strong>包含内容</strong>：</p>
<ul>
<li>模型的所有权重</li>
<li>训练状态信息</li>
<li>优化器状态</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="调度器-scheduler">调度器 (Scheduler)<a href="#调度器-scheduler" class="hash-link" aria-label="Direct link to 调度器 (Scheduler)" title="Direct link to 调度器 (Scheduler)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：控制训练节奏的&quot;教练&quot;。</p>
<p>想象你在健身，教练会根据你的状态调整训练强度。调度器就是这样的教练，主要控制：</p>
<p><strong>学习率调度器</strong>：</p>
<ul>
<li><strong>开始</strong>：学习率较高，快速学习</li>
<li><strong>中期</strong>：逐渐降低学习率，精细调整</li>
<li><strong>后期</strong>：很小的学习率，微调细节</li>
</ul>
<p><strong>常见类型</strong>：</p>
<ul>
<li><strong>线性衰减</strong>：学习率匀速下降</li>
<li><strong>余弦衰减</strong>：学习率像余弦曲线一样变化</li>
<li><strong>阶梯衰减</strong>：每隔一段时间降低学习率</li>
<li><strong>预热调度</strong>：开始时学习率很小，逐渐增加到目标值</li>
</ul>
<p><strong>为什么重要</strong>：合适的调度策略能让模型训练更稳定，效果更好。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="均值-mean">均值 (Mean)<a href="#均值-mean" class="hash-link" aria-label="Direct link to 均值 (Mean)" title="Direct link to 均值 (Mean)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：一组数据的&quot;平均水平&quot;。</p>
<p>想象你们班有5个同学，数学成绩分别是：80分、85分、90分、75分、95分。均值就是把所有成绩加起来除以人数：</p>
<p><strong>计算过程</strong>：</p>
<ul>
<li>总分：80 + 85 + 90 + 75 + 95 = 425分</li>
<li>人数：5人</li>
<li>均值：425 ÷ 5 = 85分</li>
</ul>
<p><strong>生活中的均值</strong>：</p>
<ul>
<li><strong>平均身高</strong>：全班同学身高的平均值</li>
<li><strong>平均工资</strong>：公司所有员工工资的平均值</li>
<li><strong>平均温度</strong>：一个月每天温度的平均值</li>
<li><strong>平均分数</strong>：考试成绩的平均值</li>
</ul>
<p><strong>在AI中的作用</strong>：</p>
<ul>
<li><strong>数据中心化</strong>：通过减去均值，让数据围绕0分布</li>
<li><strong>特征标准化</strong>：配合标准差进行数据归一化</li>
<li><strong>性能评估</strong>：计算模型在多次实验中的平均表现</li>
<li><strong>批量处理</strong>：计算一批数据的平均特征</li>
</ul>
<p><strong>均值的特点</strong>：</p>
<ul>
<li><strong>代表性</strong>：反映数据的整体水平</li>
<li><strong>敏感性</strong>：容易受极端值影响</li>
<li><strong>稳定性</strong>：数据量越大，均值越稳定</li>
</ul>
<p><strong>实际意义</strong>：均值是最基础的统计概念，帮助我们理解数据的整体趋势。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="方差-variance">方差 (Variance)<a href="#方差-variance" class="hash-link" aria-label="Direct link to 方差 (Variance)" title="Direct link to 方差 (Variance)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：衡量数据&quot;散乱程度&quot;的指标。</p>
<p>还是用班级成绩的例子，虽然两个班的平均分都是85分，但：</p>
<p><strong>A班成绩</strong>：84分、85分、86分、84分、86分
<strong>B班成绩</strong>：70分、80分、85分、90分、100分</p>
<p>虽然平均分相同，但B班的成绩更&quot;散乱&quot;，方差更大。</p>
<p><strong>方差的直观理解</strong>：</p>
<ul>
<li><strong>方差小</strong>：数据比较集中，大家水平差不多</li>
<li><strong>方差大</strong>：数据比较分散，差距明显</li>
</ul>
<p><strong>生活中的方差</strong>：</p>
<ul>
<li><strong>考试成绩</strong>：方差小说明全班水平相当，方差大说明有学霸也有学渣</li>
<li><strong>收入水平</strong>：方差小说明贫富差距小，方差大说明贫富差距大</li>
<li><strong>身高体重</strong>：方差小说明大家体型相似，方差大说明体型差异明显</li>
<li><strong>股票价格</strong>：方差小说明价格稳定，方差大说明波动剧烈</li>
</ul>
<p><strong>在AI中的作用</strong>：</p>
<ul>
<li><strong>数据标准化</strong>：配合均值进行Z-score标准化</li>
<li><strong>特征选择</strong>：方差太小的特征可能没有区分度</li>
<li><strong>模型稳定性</strong>：评估模型预测结果的稳定性</li>
<li><strong>批量归一化</strong>：控制每层数据的分布</li>
</ul>
<p><strong>方差与标准差</strong>：</p>
<ul>
<li><strong>标准差</strong>：方差的平方根，单位与原数据相同</li>
<li><strong>方差</strong>：标准差的平方，单位是原数据单位的平方</li>
<li><strong>实际使用</strong>：标准差更直观，方差更便于计算</li>
</ul>
<p><strong>计算示例</strong>：
假设数据是 [2, 4, 6]</p>
<ul>
<li>均值：(2+4+6)/3 = 4</li>
<li>方差：[(2-4)² + (4-4)² + (6-4)²]/3 = [4+0+4]/3 = 8/3 ≈ 2.67</li>
</ul>
<p><strong>实际意义</strong>：方差帮助我们理解数据的分布特征，是数据预处理和模型训练中的重要概念。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="归一化-normalization">归一化 (Normalization)<a href="#归一化-normalization" class="hash-link" aria-label="Direct link to 归一化 (Normalization)" title="Direct link to 归一化 (Normalization)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：让数据变得&quot;整齐划一&quot;，方便AI处理。</p>
<p>想象你是一个老师，要比较学生的成绩，但有些科目满分是100分，有些是150分，还有些是5分制。为了公平比较，你需要把所有成绩都转换成同一个标准。</p>
<p><strong>为什么需要归一化</strong>：</p>
<ul>
<li><strong>数据范围不同</strong>：图片像素值0-255，文本长度可能几千</li>
<li><strong>训练不稳定</strong>：数值差异太大会导致训练困难</li>
<li><strong>收敛速度慢</strong>：模型需要更长时间才能学会</li>
</ul>
<p><strong>常见的归一化方法</strong>：</p>
<p><strong>1. 数据归一化</strong>：</p>
<ul>
<li><strong>最小-最大归一化</strong>：把数据缩放到0-1之间</li>
<li><strong>标准化（Z-score）</strong>：利用均值和方差，让数据均值为0，标准差为1<!-- -->
<ul>
<li>公式：(数据 - 均值) / 标准差</li>
<li>就像把所有考试成绩都换算成&quot;比平均分高/低多少个标准差&quot;</li>
</ul>
</li>
</ul>
<p><strong>2. 批量归一化 (Batch Normalization)</strong>：</p>
<ul>
<li>在训练过程中，让每一层的输入保持稳定的分布</li>
<li>就像每节课开始前，让所有学生的&quot;状态&quot;都调整到最佳</li>
</ul>
<p><strong>3. 层归一化 (Layer Normalization)</strong>：</p>
<ul>
<li>对每个样本单独进行归一化</li>
<li>特别适合处理序列数据（如文本）</li>
</ul>
<p><strong>生活中的比喻</strong>：
就像体检时，医生会根据你的年龄、性别来判断各项指标是否正常，而不是用统一的标准。归一化让AI能更好地&quot;理解&quot;不同类型的数据。</p>
<p><strong>好处</strong>：</p>
<ul>
<li>训练更稳定</li>
<li>收敛更快</li>
<li>效果更好</li>
<li>减少梯度消失/爆炸问题</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="优化技术篇">优化技术篇<a href="#优化技术篇" class="hash-link" aria-label="Direct link to 优化技术篇" title="Direct link to 优化技术篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)<a href="#lora-low-rank-adaptation" class="hash-link" aria-label="Direct link to LoRA (Low-Rank Adaptation)" title="Direct link to LoRA (Low-Rank Adaptation)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：一种&quot;省钱省力&quot;的模型微调方法。</p>
<p>想象你有一台超级复杂的机器（大模型），你想让它学会新技能，但重新训练整台机器太贵了。LoRA就像给机器加一个小插件，只训练这个小插件就能让整台机器学会新技能。</p>
<p><strong>优势</strong>：</p>
<ul>
<li>训练成本低</li>
<li>训练速度快</li>
<li>效果还不错</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="量化-quantization">量化 (Quantization)<a href="#量化-quantization" class="hash-link" aria-label="Direct link to 量化 (Quantization)" title="Direct link to 量化 (Quantization)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：给模型&quot;减肥&quot;，让它占用更少内存。</p>
<p>就像把高清电影压缩成标清，虽然质量稍微下降，但文件小了很多，更容易存储和传输。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="蒸馏-distillation">蒸馏 (Distillation)<a href="#蒸馏-distillation" class="hash-link" aria-label="Direct link to 蒸馏 (Distillation)" title="Direct link to 蒸馏 (Distillation)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：让小学生（小模型）学习大学教授（大模型）的知识。</p>
<p>大模型很聪明但太笨重，小模型轻便但不够聪明。蒸馏就是让小模型模仿大模型的行为，学到大模型的&quot;精华&quot;。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="生成技术篇">生成技术篇<a href="#生成技术篇" class="hash-link" aria-label="Direct link to 生成技术篇" title="Direct link to 生成技术篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="vae-变分自编码器">VAE (变分自编码器)<a href="#vae-变分自编码器" class="hash-link" aria-label="Direct link to VAE (变分自编码器)" title="Direct link to VAE (变分自编码器)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：一个既会&quot;理解&quot;又会&quot;创造&quot;的AI艺术家。</p>
<p>想象一个神奇的艺术家，它有两种超能力：</p>
<p><strong>第一种能力 - 理解精髓</strong>：</p>
<ul>
<li>看到一张猫的照片，能提取出&quot;猫的精髓&quot;（比如：毛茸茸、有胡须、尖耳朵等特征）</li>
<li>把这些精髓压缩成一个&quot;创意密码&quot;</li>
</ul>
<p><strong>第二种能力 - 创造新作品</strong>：</p>
<ul>
<li>拿到&quot;创意密码&quot;后，能画出一只全新的猫</li>
<li>这只猫和原来的不完全一样，但确实是一只猫</li>
</ul>
<p><strong>VAE的神奇之处</strong>：</p>
<ul>
<li><strong>不是死记硬背</strong>：它不是简单复制，而是真正&quot;理解&quot;了什么是猫</li>
<li><strong>能举一反三</strong>：学会了猫之后，给它一个&quot;介于猫和狗之间&quot;的密码，它能画出像猫又像狗的动物</li>
<li><strong>有创造力</strong>：每次生成的结果都略有不同，就像真正的艺术家一样</li>
</ul>
<p><strong>生活中的比喻</strong>：
就像一个画家学习了毕加索的风格后，不是照搬毕加索的画，而是能创作出&quot;毕加索风格&quot;的全新作品。</p>
<p><strong>应用</strong>：图像生成、艺术创作、数据增强、药物分子设计</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="扩散模型-diffusion-model">扩散模型 (Diffusion Model)<a href="#扩散模型-diffusion-model" class="hash-link" aria-label="Direct link to 扩散模型 (Diffusion Model)" title="Direct link to 扩散模型 (Diffusion Model)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：通过&quot;去噪&quot;来生成图片的技术。</p>
<p>想象你有一张被雪花覆盖的照片，扩散模型就像一个魔法师，能一点点去掉雪花，最终还原出清晰的照片。但它更厉害的是，能从纯噪声开始，生成全新的图片。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="噪声-noise">噪声 (Noise)<a href="#噪声-noise" class="hash-link" aria-label="Direct link to 噪声 (Noise)" title="Direct link to 噪声 (Noise)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：随机的干扰信息，但在AI中有特殊用途。</p>
<p>在传统理解中，噪声是坏东西。但在AI生成中，噪声是创造力的源泉。就像艺术家需要一些随机灵感来创作，AI也需要噪声来生成多样化的内容。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="采样-sampling">采样 (Sampling)<a href="#采样-sampling" class="hash-link" aria-label="Direct link to 采样 (Sampling)" title="Direct link to 采样 (Sampling)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：从大量可能性中&quot;挑选&quot;出具体结果的过程。</p>
<p>想象你面前有一个装满彩球的巨大箱子，采样就是伸手进去抓一把球的过程。在AI中，采样是从模型学到的&quot;可能性分布&quot;中选择具体输出的关键步骤。</p>
<p><strong>生活中的采样</strong>：</p>
<ul>
<li><strong>抽奖活动</strong>：从所有参与者中随机选出中奖者</li>
<li><strong>民意调查</strong>：从全体民众中选择一部分人进行调查</li>
<li><strong>质量检测</strong>：从生产线上随机抽取产品检查质量</li>
<li><strong>音乐播放</strong>：从播放列表中随机选择下一首歌</li>
</ul>
<p><strong>AI中的采样类型</strong>：</p>
<p><strong>1. 数据采样</strong>：</p>
<ul>
<li><strong>训练数据采样</strong>：从大数据集中选择一部分用于训练</li>
<li><strong>批次采样</strong>：每次训练时随机选择一批数据</li>
<li><strong>负采样</strong>：在推荐系统中选择负样本</li>
</ul>
<p><strong>2. 生成采样</strong>：</p>
<ul>
<li><strong>文本生成</strong>：从词汇表中选择下一个词</li>
<li><strong>图像生成</strong>：从潜在空间中采样生成新图片</li>
<li><strong>音频生成</strong>：采样生成音频波形</li>
</ul>
<p><strong>常见的采样策略</strong>：</p>
<p><strong>随机采样</strong>：</p>
<ul>
<li>完全随机选择，像掷骰子</li>
<li>保证多样性，但可能不够稳定</li>
</ul>
<p><strong>贪心采样</strong>：</p>
<ul>
<li>总是选择概率最高的选项</li>
<li>结果稳定，但可能缺乏创造性</li>
</ul>
<p><strong>Top-k采样</strong>：</p>
<ul>
<li>只从概率最高的k个选项中随机选择</li>
<li>平衡了质量和多样性</li>
</ul>
<p><strong>核采样 (Nucleus Sampling)</strong>：</p>
<ul>
<li>选择累积概率达到某个阈值的词汇集合</li>
<li>动态调整候选词数量</li>
</ul>
<p><strong>温度采样</strong>：</p>
<ul>
<li>通过&quot;温度&quot;参数控制随机性</li>
<li>温度高：更随机，更有创造性</li>
<li>温度低：更确定，更保守</li>
</ul>
<p><strong>采样在不同场景中的作用</strong>：</p>
<p><strong>文本生成</strong>：</p>
<ul>
<li>决定AI写作的风格：保守还是创新</li>
<li>影响对话的自然度和趣味性</li>
</ul>
<p><strong>图像生成</strong>：</p>
<ul>
<li>控制生成图片的多样性</li>
<li>在DALL-E、Midjourney中决定创作风格</li>
</ul>
<p><strong>音乐生成</strong>：</p>
<ul>
<li>影响旋律的创新程度</li>
<li>控制音乐的风格变化</li>
</ul>
<p><strong>采样的重要性</strong>：</p>
<ul>
<li><strong>控制创造性</strong>：采样策略直接影响AI的创造力</li>
<li><strong>平衡质量与多样性</strong>：好的采样能在准确性和创新性间找到平衡</li>
<li><strong>用户体验</strong>：采样质量直接影响AI产品的用户体验</li>
</ul>
<p><strong>实际应用技巧</strong>：</p>
<ul>
<li><strong>创意写作</strong>：使用较高温度，增加随机性</li>
<li><strong>技术文档</strong>：使用较低温度，保证准确性</li>
<li><strong>艺术创作</strong>：结合多种采样策略，获得最佳效果</li>
</ul>
<p><strong>生活中的比喻</strong>：
采样就像厨师做菜时的调味，同样的食材（模型），不同的调味方式（采样策略）能做出完全不同口味的菜（输出结果）。</p>
<p><strong>实际意义</strong>：采样是连接AI模型能力和实际应用效果的桥梁，掌握采样技巧能让你更好地控制AI的输出质量。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="模型架构篇">模型架构篇<a href="#模型架构篇" class="hash-link" aria-label="Direct link to 模型架构篇" title="Direct link to 模型架构篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="编码器-解码器-encoder-decoder">编码器-解码器 (Encoder-Decoder)<a href="#编码器-解码器-encoder-decoder" class="hash-link" aria-label="Direct link to 编码器-解码器 (Encoder-Decoder)" title="Direct link to 编码器-解码器 (Encoder-Decoder)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：一个负责理解，一个负责表达。</p>
<p>就像翻译过程：编码器负责理解原文的意思，解码器负责用目标语言表达出来。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="自回归-autoregressive">自回归 (Autoregressive)<a href="#自回归-autoregressive" class="hash-link" aria-label="Direct link to 自回归 (Autoregressive)" title="Direct link to 自回归 (Autoregressive)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：根据前面的内容预测下一个词。</p>
<p>就像接龙游戏，根据前面的词语来猜下一个词。GPT就是这样工作的，一个词一个词地生成文本。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="掩码语言模型-masked-language-model">掩码语言模型 (Masked Language Model)<a href="#掩码语言模型-masked-language-model" class="hash-link" aria-label="Direct link to 掩码语言模型 (Masked Language Model)" title="Direct link to 掩码语言模型 (Masked Language Model)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：填空题专家。</p>
<p>给模型一个有空白的句子，让它猜空白处应该填什么词。BERT就是这样训练的。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="评估指标篇">评估指标篇<a href="#评估指标篇" class="hash-link" aria-label="Direct link to 评估指标篇" title="Direct link to 评估指标篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="困惑度-perplexity">困惑度 (Perplexity)<a href="#困惑度-perplexity" class="hash-link" aria-label="Direct link to 困惑度 (Perplexity)" title="Direct link to 困惑度 (Perplexity)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：衡量模型对文本&quot;困惑程度&quot;的指标。</p>
<p>困惑度越低，说明模型越&quot;确定&quot;自己的预测是对的。就像一个学生做题时的自信程度。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="bleu分数">BLEU分数<a href="#bleu分数" class="hash-link" aria-label="Direct link to BLEU分数" title="Direct link to BLEU分数" translate="no">​</a></h3>
<p><strong>简单理解</strong>：衡量翻译质量的标准。</p>
<p>通过比较机器翻译和人工翻译的相似度来评分，分数越高说明翻译质量越好。</p>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="实际应用篇">实际应用篇<a href="#实际应用篇" class="hash-link" aria-label="Direct link to 实际应用篇" title="Direct link to 实际应用篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="推理-inference">推理 (Inference)<a href="#推理-inference" class="hash-link" aria-label="Direct link to 推理 (Inference)" title="Direct link to 推理 (Inference)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：训练好的AI模型正式&quot;上岗工作&quot;，处理真实任务。</p>
<p>想象你培养了一个学生，经过多年学习后，现在要让他去考试或工作了。推理就是这个&quot;正式考试&quot;的过程：</p>
<p><strong>推理 vs 训练的区别</strong>：</p>
<p><strong>训练阶段</strong>：</p>
<ul>
<li><strong>目的</strong>：让模型学习知识</li>
<li><strong>过程</strong>：反复练习，不断调整</li>
<li><strong>需要</strong>：大量数据、长时间、高计算资源</li>
<li><strong>比喻</strong>：学生在学校上课、做作业</li>
</ul>
<p><strong>推理阶段</strong>：</p>
<ul>
<li><strong>目的</strong>：让模型解决实际问题</li>
<li><strong>过程</strong>：直接给出答案，不再学习</li>
<li><strong>需要</strong>：单个输入、快速响应、相对较少资源</li>
<li><strong>比喻</strong>：学生参加考试、正式工作</li>
</ul>
<p><strong>推理的特点</strong>：</p>
<ul>
<li><strong>只有前向传播</strong>：信息只从输入流向输出，不需要反向传播</li>
<li><strong>参数固定</strong>：模型权重不再改变</li>
<li><strong>速度要求高</strong>：用户等待答案，需要快速响应</li>
<li><strong>资源消耗相对较少</strong>：不需要存储梯度等训练信息</li>
</ul>
<p><strong>推理的应用场景</strong>：</p>
<ul>
<li><strong>聊天机器人</strong>：回答用户问题</li>
<li><strong>图像识别</strong>：识别照片中的物体</li>
<li><strong>语音助手</strong>：理解和回应语音指令</li>
<li><strong>推荐系统</strong>：为用户推荐内容</li>
<li><strong>自动翻译</strong>：翻译文本或语音</li>
</ul>
<p><strong>推理优化技术</strong>：</p>
<ul>
<li><strong>量化</strong>：减少模型精度以提高速度</li>
<li><strong>剪枝</strong>：去掉不重要的连接</li>
<li><strong>蒸馏</strong>：用小模型模仿大模型</li>
<li><strong>缓存</strong>：保存常见问题的答案</li>
<li><strong>批处理</strong>：同时处理多个请求</li>
</ul>
<p><strong>推理的挑战</strong>：</p>
<ul>
<li><strong>延迟要求</strong>：用户希望立即得到回答</li>
<li><strong>并发处理</strong>：同时服务多个用户</li>
<li><strong>资源限制</strong>：服务器计算能力有限</li>
<li><strong>成本控制</strong>：推理也需要计算资源</li>
</ul>
<p><strong>生活中的比喻</strong>：
就像医生看病，医学院学习是&quot;训练&quot;，给病人诊断是&quot;推理&quot;。医生不会在给病人看病时还在学习医学知识，而是直接运用已有知识做出诊断。</p>
<p><strong>实际意义</strong>：推理是AI模型创造价值的阶段，所有用户体验到的AI功能都是在推理阶段实现的。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="微调-fine-tuning">微调 (Fine-tuning)<a href="#微调-fine-tuning" class="hash-link" aria-label="Direct link to 微调 (Fine-tuning)" title="Direct link to 微调 (Fine-tuning)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：在通用模型基础上，针对特定任务进行专门训练。</p>
<p>就像一个通才医生，经过专门培训后成为心脏病专家。模型也可以通过微调变成特定领域的专家。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="提示工程-prompt-engineering">提示工程 (Prompt Engineering)<a href="#提示工程-prompt-engineering" class="hash-link" aria-label="Direct link to 提示工程 (Prompt Engineering)" title="Direct link to 提示工程 (Prompt Engineering)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：学会如何&quot;问问题&quot;来获得更好的答案。</p>
<p>就像和人交流一样，问法不同，得到的答案质量也不同。好的提示能让AI给出更准确、更有用的回答。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="上下文学习-in-context-learning">上下文学习 (In-Context Learning)<a href="#上下文学习-in-context-learning" class="hash-link" aria-label="Direct link to 上下文学习 (In-Context Learning)" title="Direct link to 上下文学习 (In-Context Learning)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：通过例子来教AI做事，不需要重新训练。</p>
<p>就像给AI几个例子，它就能学会做类似的事情。比如给几个翻译例子，它就能翻译新的句子。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="sft-supervised-fine-tuning-监督微调">SFT (Supervised Fine-Tuning) 监督微调<a href="#sft-supervised-fine-tuning-监督微调" class="hash-link" aria-label="Direct link to SFT (Supervised Fine-Tuning) 监督微调" title="Direct link to SFT (Supervised Fine-Tuning) 监督微调" translate="no">​</a></h3>
<p><strong>简单理解</strong>：用标准答案来教AI如何正确回答问题。</p>
<p>想象你是一个家教，给学生出题并提供标准答案，让学生反复练习直到能给出正确答案。SFT就是这个过程，用大量的问题-答案对来训练模型。</p>
<p><strong>过程</strong>：</p>
<ol>
<li>准备大量高质量的问答对</li>
<li>让模型学习这些标准答案</li>
<li>模型学会按照期望的方式回答问题</li>
</ol>
<p><strong>应用</strong>：ChatGPT、Claude等对话模型都经过了SFT训练。</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="rlhf-reinforcement-learning-from-human-feedback-人类反馈强化学习">RLHF (Reinforcement Learning from Human Feedback) 人类反馈强化学习<a href="#rlhf-reinforcement-learning-from-human-feedback-人类反馈强化学习" class="hash-link" aria-label="Direct link to RLHF (Reinforcement Learning from Human Feedback) 人类反馈强化学习" title="Direct link to RLHF (Reinforcement Learning from Human Feedback) 人类反馈强化学习" translate="no">​</a></h3>
<p><strong>简单理解</strong>：通过人类的&quot;点赞&quot;和&quot;差评&quot;来训练AI。</p>
<p>就像训练宠物一样，做得好就给奖励，做得不好就批评。AI通过人类的反馈学会什么样的回答更受欢迎。</p>
<p><strong>过程</strong>：</p>
<ol>
<li>AI生成多个回答</li>
<li>人类对这些回答进行排序（哪个更好）</li>
<li>AI学习人类的偏好</li>
<li>调整自己的回答风格</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="高级架构篇">高级架构篇<a href="#高级架构篇" class="hash-link" aria-label="Direct link to 高级架构篇" title="Direct link to 高级架构篇" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="moe-mixture-of-experts-专家混合模型">MoE (Mixture of Experts) 专家混合模型<a href="#moe-mixture-of-experts-专家混合模型" class="hash-link" aria-label="Direct link to MoE (Mixture of Experts) 专家混合模型" title="Direct link to MoE (Mixture of Experts) 专家混合模型" translate="no">​</a></h3>
<p><strong>简单理解</strong>：一个由多个&quot;专家&quot;组成的超级团队。</p>
<p>想象一个咨询公司，有法律专家、财务专家、技术专家等。当客户提问时，系统会自动选择最合适的专家来回答，而不是让所有专家都参与。</p>
<p><strong>工作原理</strong>：</p>
<ul>
<li><strong>路由器</strong>：决定哪些专家参与处理当前问题</li>
<li><strong>专家网络</strong>：每个专家负责特定类型的任务</li>
<li><strong>稀疏激活</strong>：每次只激活部分专家，节省计算资源</li>
</ul>
<p><strong>优势</strong>：</p>
<ul>
<li>模型容量大但计算成本相对较低</li>
<li>不同专家可以专注不同领域</li>
<li>可以轻松扩展模型规模</li>
</ul>
<p><strong>代表模型</strong>：GPT-4、PaLM、Switch Transformer</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="多模态-multimodal">多模态 (Multimodal)<a href="#多模态-multimodal" class="hash-link" aria-label="Direct link to 多模态 (Multimodal)" title="Direct link to 多模态 (Multimodal)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：能同时理解文字、图片、声音等多种信息的AI。</p>
<p>就像人类可以同时看图片、听声音、读文字来理解一个完整的故事，多模态AI也能处理多种类型的输入。</p>
<p><strong>应用</strong>：</p>
<ul>
<li>看图说话</li>
<li>根据文字生成图片</li>
<li>视频理解和生成</li>
</ul>
<p><strong>代表模型</strong>：GPT-4V、DALL-E、Midjourney</p>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="检索增强生成-rag---retrieval-augmented-generation">检索增强生成 (RAG - Retrieval Augmented Generation)<a href="#检索增强生成-rag---retrieval-augmented-generation" class="hash-link" aria-label="Direct link to 检索增强生成 (RAG - Retrieval Augmented Generation)" title="Direct link to 检索增强生成 (RAG - Retrieval Augmented Generation)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：AI在回答问题前先&quot;查资料&quot;。</p>
<p>就像学生考试时可以查阅参考书，RAG让AI在生成答案前先从知识库中检索相关信息，然后基于这些信息给出更准确的回答。</p>
<p><strong>工作流程</strong>：</p>
<ol>
<li>用户提问</li>
<li>系统检索相关文档</li>
<li>将问题和检索到的信息一起输入模型</li>
<li>模型基于检索信息生成答案</li>
</ol>
<p><strong>优势</strong>：</p>
<ul>
<li>减少幻觉（编造信息）</li>
<li>可以获取最新信息</li>
<li>答案更准确可靠</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="思维链-chain-of-thought-cot">思维链 (Chain of Thought, CoT)<a href="#思维链-chain-of-thought-cot" class="hash-link" aria-label="Direct link to 思维链 (Chain of Thought, CoT)" title="Direct link to 思维链 (Chain of Thought, CoT)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：让AI&quot;显示解题步骤&quot;。</p>
<p>就像数学考试要求写出解题过程一样，思维链让AI一步步展示推理过程，而不是直接给出答案。</p>
<p><strong>示例</strong>：</p>
<ul>
<li>普通回答：&quot;答案是42&quot;</li>
<li>思维链回答：&quot;首先分析问题...然后计算...最后得出答案是42&quot;</li>
</ul>
<p><strong>优势</strong>：</p>
<ul>
<li>推理过程更清晰</li>
<li>更容易发现错误</li>
<li>复杂问题解决能力更强</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="涌现能力-emergent-abilities">涌现能力 (Emergent Abilities)<a href="#涌现能力-emergent-abilities" class="hash-link" aria-label="Direct link to 涌现能力 (Emergent Abilities)" title="Direct link to 涌现能力 (Emergent Abilities)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：模型变大后突然&quot;开窍&quot;了。</p>
<p>就像小孩学语言，突然有一天就能说完整的句子了。大模型也是这样，当参数量达到某个临界点时，会突然具备一些之前没有的能力。</p>
<p><strong>常见涌现能力</strong>：</p>
<ul>
<li>少样本学习</li>
<li>复杂推理</li>
<li>代码生成</li>
<li>数学解题</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_qHQm" id="幻觉-hallucination">幻觉 (Hallucination)<a href="#幻觉-hallucination" class="hash-link" aria-label="Direct link to 幻觉 (Hallucination)" title="Direct link to 幻觉 (Hallucination)" translate="no">​</a></h3>
<p><strong>简单理解</strong>：AI&quot;编造&quot;不存在的信息。</p>
<p>就像一个爱吹牛的朋友，会编造一些听起来很真实但实际不存在的故事。AI有时也会生成看似合理但实际错误的信息。</p>
<p><strong>产生原因</strong>：</p>
<ul>
<li>训练数据中的错误信息</li>
<li>模型过度自信</li>
<li>缺乏真实世界知识验证</li>
</ul>
<p><strong>解决方法</strong>：</p>
<ul>
<li>使用RAG检索验证</li>
<li>多模型交叉验证</li>
<li>人工审核</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_qHQm" id="总结">总结<a href="#总结" class="hash-link" aria-label="Direct link to 总结" title="Direct link to 总结" translate="no">​</a></h2>
<p>这些概念看起来复杂，但本质上都是为了让计算机更好地理解和生成人类语言。记住几个关键点：</p>
<ol>
<li><strong>数据基础</strong>：张量、权重、参数是AI的基本组成元素</li>
<li><strong>文本理解</strong>：分词、嵌入、向量、潜在空间让计算机理解和表示信息</li>
<li><strong>统计基础</strong>：均值、方差帮助理解数据分布和预处理</li>
<li><strong>核心架构</strong>：注意力机制和Transformer是现代AI的基础</li>
<li><strong>训练过程</strong>：前向传播、反向传播、梯度、损失函数、过拟合是学习的核心</li>
<li><strong>训练优化</strong>：学习率、超参数、均值、方差、归一化、检查点、调度器帮助控制训练过程</li>
<li><strong>训练方法</strong>：SFT、RLHF让AI学会正确回答</li>
<li><strong>优化技术</strong>：LoRA、量化、蒸馏让AI更快更省资源</li>
<li><strong>生成技术</strong>：VAE、扩散模型、噪声、采样让AI创造新内容</li>
<li><strong>高级架构</strong>：MoE、多模态、RAG让AI更强大更实用</li>
<li><strong>实际应用</strong>：推理、微调、提示工程、思维链让AI适应具体任务</li>
<li><strong>质量控制</strong>：理解幻觉问题，知道AI的局限性</li>
</ol>
<p><strong>学习路径建议</strong>：</p>
<ol>
<li><strong>基础层</strong>：先掌握张量、权重、分词、嵌入、潜在空间等基础概念</li>
<li><strong>统计层</strong>：理解均值、方差等统计基础，为数据处理打基础</li>
<li><strong>训练层</strong>：理解前向传播、反向传播、梯度、归一化等训练原理</li>
<li><strong>架构层</strong>：学习注意力机制、Transformer、MoE等架构</li>
<li><strong>应用层</strong>：掌握SFT、RLHF、RAG等实用技术</li>
<li><strong>优化层</strong>：了解LoRA、量化等效率优化方法</li>
</ol>
<p><strong>核心理解要点</strong>：</p>
<ul>
<li><strong>潜在空间是AI创造力的源泉</strong>：理解了潜在空间，就理解了AI如何&quot;想象&quot;和&quot;创造&quot;</li>
<li><strong>从表示到生成</strong>：嵌入让AI理解，潜在空间让AI创造</li>
<li><strong>连续性是关键</strong>：潜在空间的连续性让AI能够平滑地在不同概念间过渡</li>
</ul>
<p>理解了这43个核心概念，你就能更好地理解大模型的工作原理，也能更有效地使用各种AI工具了。记住，这个领域发展很快，新概念不断涌现，保持学习的心态很重要！</p>
<hr>
<p><em>这篇文章用最简单的语言解释了大模型的核心概念，希望能帮助你建立对这个领域的整体认知。</em></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI/大模型核心概念通俗解释.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_HnUL" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_DcQK"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/notes3/docs/AI/变分自编码器详解"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">变分自编码器（VAE）详解</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/notes3/docs/AI/大模型训练中的随机种子：为什么需要它？"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">大模型训练中的随机种子：为什么需要它？</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_AvJD thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#前言" class="table-of-contents__link toc-highlight">前言</a></li><li><a href="#基础概念篇" class="table-of-contents__link toc-highlight">基础概念篇</a><ul><li><a href="#分词-tokenization" class="table-of-contents__link toc-highlight">分词 (Tokenization)</a></li><li><a href="#嵌入-embedding-和向量-vector" class="table-of-contents__link toc-highlight">嵌入 (Embedding) 和向量 (Vector)</a></li><li><a href="#潜在空间-latent-space" class="table-of-contents__link toc-highlight">潜在空间 (Latent Space)</a></li><li><a href="#注意力机制-attention" class="table-of-contents__link toc-highlight">注意力机制 (Attention)</a></li><li><a href="#transformer" class="table-of-contents__link toc-highlight">Transformer</a></li></ul></li><li><a href="#训练过程篇" class="table-of-contents__link toc-highlight">训练过程篇</a><ul><li><a href="#前向传播-forward-propagation" class="table-of-contents__link toc-highlight">前向传播 (Forward Propagation)</a></li><li><a href="#反向传播-backpropagation" class="table-of-contents__link toc-highlight">反向传播 (Backpropagation)</a></li><li><a href="#梯度-gradient" class="table-of-contents__link toc-highlight">梯度 (Gradient)</a></li><li><a href="#损失函数-loss-function" class="table-of-contents__link toc-highlight">损失函数 (Loss Function)</a></li><li><a href="#过拟合-overfitting" class="table-of-contents__link toc-highlight">过拟合 (Overfitting)</a></li><li><a href="#学习率-learning-rate" class="table-of-contents__link toc-highlight">学习率 (Learning Rate)</a></li><li><a href="#超参数-hyperparameters" class="table-of-contents__link toc-highlight">超参数 (Hyperparameters)</a></li><li><a href="#张量-tensor" class="table-of-contents__link toc-highlight">张量 (Tensor)</a></li><li><a href="#权重-weights-和参数-parameters" class="table-of-contents__link toc-highlight">权重 (Weights) 和参数 (Parameters)</a></li><li><a href="#检查点-checkpoint" class="table-of-contents__link toc-highlight">检查点 (Checkpoint)</a></li><li><a href="#调度器-scheduler" class="table-of-contents__link toc-highlight">调度器 (Scheduler)</a></li><li><a href="#均值-mean" class="table-of-contents__link toc-highlight">均值 (Mean)</a></li><li><a href="#方差-variance" class="table-of-contents__link toc-highlight">方差 (Variance)</a></li><li><a href="#归一化-normalization" class="table-of-contents__link toc-highlight">归一化 (Normalization)</a></li></ul></li><li><a href="#优化技术篇" class="table-of-contents__link toc-highlight">优化技术篇</a><ul><li><a href="#lora-low-rank-adaptation" class="table-of-contents__link toc-highlight">LoRA (Low-Rank Adaptation)</a></li><li><a href="#量化-quantization" class="table-of-contents__link toc-highlight">量化 (Quantization)</a></li><li><a href="#蒸馏-distillation" class="table-of-contents__link toc-highlight">蒸馏 (Distillation)</a></li></ul></li><li><a href="#生成技术篇" class="table-of-contents__link toc-highlight">生成技术篇</a><ul><li><a href="#vae-变分自编码器" class="table-of-contents__link toc-highlight">VAE (变分自编码器)</a></li><li><a href="#扩散模型-diffusion-model" class="table-of-contents__link toc-highlight">扩散模型 (Diffusion Model)</a></li><li><a href="#噪声-noise" class="table-of-contents__link toc-highlight">噪声 (Noise)</a></li><li><a href="#采样-sampling" class="table-of-contents__link toc-highlight">采样 (Sampling)</a></li></ul></li><li><a href="#模型架构篇" class="table-of-contents__link toc-highlight">模型架构篇</a><ul><li><a href="#编码器-解码器-encoder-decoder" class="table-of-contents__link toc-highlight">编码器-解码器 (Encoder-Decoder)</a></li><li><a href="#自回归-autoregressive" class="table-of-contents__link toc-highlight">自回归 (Autoregressive)</a></li><li><a href="#掩码语言模型-masked-language-model" class="table-of-contents__link toc-highlight">掩码语言模型 (Masked Language Model)</a></li></ul></li><li><a href="#评估指标篇" class="table-of-contents__link toc-highlight">评估指标篇</a><ul><li><a href="#困惑度-perplexity" class="table-of-contents__link toc-highlight">困惑度 (Perplexity)</a></li><li><a href="#bleu分数" class="table-of-contents__link toc-highlight">BLEU分数</a></li></ul></li><li><a href="#实际应用篇" class="table-of-contents__link toc-highlight">实际应用篇</a><ul><li><a href="#推理-inference" class="table-of-contents__link toc-highlight">推理 (Inference)</a></li><li><a href="#微调-fine-tuning" class="table-of-contents__link toc-highlight">微调 (Fine-tuning)</a></li><li><a href="#提示工程-prompt-engineering" class="table-of-contents__link toc-highlight">提示工程 (Prompt Engineering)</a></li><li><a href="#上下文学习-in-context-learning" class="table-of-contents__link toc-highlight">上下文学习 (In-Context Learning)</a></li><li><a href="#sft-supervised-fine-tuning-监督微调" class="table-of-contents__link toc-highlight">SFT (Supervised Fine-Tuning) 监督微调</a></li><li><a href="#rlhf-reinforcement-learning-from-human-feedback-人类反馈强化学习" class="table-of-contents__link toc-highlight">RLHF (Reinforcement Learning from Human Feedback) 人类反馈强化学习</a></li></ul></li><li><a href="#高级架构篇" class="table-of-contents__link toc-highlight">高级架构篇</a><ul><li><a href="#moe-mixture-of-experts-专家混合模型" class="table-of-contents__link toc-highlight">MoE (Mixture of Experts) 专家混合模型</a></li><li><a href="#多模态-multimodal" class="table-of-contents__link toc-highlight">多模态 (Multimodal)</a></li><li><a href="#检索增强生成-rag---retrieval-augmented-generation" class="table-of-contents__link toc-highlight">检索增强生成 (RAG - Retrieval Augmented Generation)</a></li><li><a href="#思维链-chain-of-thought-cot" class="table-of-contents__link toc-highlight">思维链 (Chain of Thought, CoT)</a></li><li><a href="#涌现能力-emergent-abilities" class="table-of-contents__link toc-highlight">涌现能力 (Emergent Abilities)</a></li><li><a href="#幻觉-hallucination" class="table-of-contents__link toc-highlight">幻觉 (Hallucination)</a></li></ul></li><li><a href="#总结" class="table-of-contents__link toc-highlight">总结</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_YFPC"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_YFPC"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_YFPC"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/notes3/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_YFPC"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>