## 1. 为什么会有这么多“浮点数”？—— 水桶的比喻

在深度学习中，我们通常用“浮点数”（Floating Point）来存储模型参数。你可以把浮点数想象成**水桶**，用来装数字（水）。

*   **FP32 (torch.float32)**：这是最标准的“大水桶”。它的容量很大（能表示非常大和非常小的数），刻度也很细（精度高，能区分 1.000001 和 1.000002）。
    *   **优点**：最精准，训练最稳定。
    *   **缺点**：太占地方（显存占用大），搬运起来慢（计算速度慢）。

为了跑得更快、省显存，大家开始把水桶做小。这就有了 FP16 和 BF16。它们都是“半个桶”的大小（16-bit），比 FP32（32-bit）小一半。

## 2. 核心对比：float16 vs bfloat16

虽然 `float16` 和 `bfloat16` 都只占用 16 位内存，但它们的内部构造完全不同。

### 2.1 torch.float16 (FP16) —— “精细的小桶”
*   **结构**：把很多位留给了“小数部分”（Mantissa），留给“指数部分”（Exponent）的位很少。
*   **特点**：
    *   **精度较高**：在它能表示的范围内，数字很准。
    *   **范围很窄**：**这是致命伤**。它能表示的最大数只有 **65504**。一旦超过这个数，就会溢出变成 `Infinity`（无穷大），导致模型训练崩溃（Loss 变成 NaN）。
    *   **需要缩放 (Scaler)**：为了防止数字太小变成 0（下溢出），训练时必须使用 `GradScaler` 放大梯度。

### 2.2 torch.bfloat16 (BF16) —— “粗糙的大桶”
*   **结构**：**Brain Floating Point**（Google Brain 发明）。它的“指数部分”和 FP32 一模一样（8位），只是把“小数部分”砍掉了。
*   **特点**：
    *   **范围超大**：能表示的数值范围和 FP32 一样大！不用担心溢出。
    *   **精度较低**：小数点后的数字比较粗糙。比如可能分不清 1.0001 和 1.0002。
    *   **深度学习友好**：神奇的是，神经网络对“范围”很敏感，但对“微小的精度误差”非常宽容（具有鲁棒性）。因此，BF16 训练往往**不需要**梯度缩放，直接就能跑，不仅省事，而且更稳定。

### 2.3 图解对比

| 格式 | 总位数 | 指数位 (决定范围) | 小数位 (决定精度) | 范围 (Approx) | 精度 (10进制) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **FP32** | 32 | 8 | 23 | $10^{-38} \sim 10^{38}$ | ~7 位有效数字 |
| **FP16** | 16 | **5** (窄!) | **10** (较细) | $10^{-5} \sim 6.5 \times 10^4$ | ~3-4 位有效数字 |
| **BF16** | 16 | **8** (同FP32) | **7** (粗糙) | $10^{-38} \sim 10^{38}$ | ~2-3 位有效数字 |

## 3. 常见 API 与使用建议

### 3.1 核心 API 概览

1.  **`torch.float16` / `torch.half`**:
    *   传统的半精度格式。
    *   **必须配合** `torch.cuda.amp.GradScaler` 使用，否则很容易 NaN。
    *   所有现代 GPU（Pascal 架构 P100 以后）都支持。

2.  **`torch.bfloat16`**:
    *   新一代半精度格式。
    *   **不需要** `GradScaler`（通常情况下）。
    *   **硬件要求高**：需要 Ampere 架构（如 RTX 3090, A100）或更新的显卡才支持加速。在老显卡上用虽然不会报错，但速度可能比 FP32 还慢（因为要软件模拟）。

3.  **`torch.autocast`**:
    *   自动混合精度的魔法棒。它会自动判断哪些算子（如卷积、矩阵乘法）用 FP16/BF16 跑更快，哪些（如 Softmax、Loss 计算）必须用 FP32 保持稳定。

### 3.2 代码实战

#### 场景一：使用 BF16 (推荐，如果硬件支持)
这是最简单的用法，也是大模型（LLM）训练的主流选择。

```python
import torch

# 检查硬件是否支持 BF16
if torch.cuda.is_bf16_supported():
    print("恭喜，你的显卡支持 BF16 加速！")
    dtype = torch.bfloat16
else:
    print("显卡不支持 BF16，将回退到 FP32 或 FP16")
    dtype = torch.float32

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())

# 训练循环
for input, target in dataloader:
    input, target = input.cuda(), target.cuda()
    
    # 开启自动混合精度上下文，指定 dtype=torch.bfloat16
    with torch.autocast(device_type="cuda", dtype=dtype):
        output = model(input)
        loss = loss_fn(output, target)
    
    # BF16 通常不需要 Scaler，直接反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

#### 场景二：使用 FP16 (传统用法，兼容性好)
如果你的显卡比较老（比如 RTX 2080Ti, Tesla V100），只能用 FP16。注意必须加 Scaler！

```python
import torch
from torch.cuda.amp import GradScaler

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler() # 必须创建一个 Scaler

for input, target in dataloader:
    input, target = input.cuda(), target.cuda()
    
    with torch.autocast(device_type="cuda", dtype=torch.float16):
        output = model(input)
        loss = loss_fn(output, target)
    
    # 三步走：Scale -> Backward -> Step -> Update
    scaler.scale(loss).backward() # 放大 Loss
    scaler.step(optimizer)        # 如果梯度没有 Inf/NaN，则更新权重
    scaler.update()               # 更新 Scaler 的缩放因子
    optimizer.zero_grad()
```

## 4. 总结：我该选哪个？

*   **如果你有新显卡 (RTX 30系/40系, A100, H100)**：无脑选 **BF16**。它不仅快，而且甚至不需要改太多代码（不用 Scaler），训练 LLM 几乎都用它。
*   **如果你是老显卡 (RTX 20系, V100, T4)**：只能选 **FP16**。记得带上 `GradScaler` 这个“保镖”。
*   **如果你在做推理 (Inference)**：
    *   两者都可以，通常直接加载 `float16` 权重的模型即可。
    *   很多 Hugging Face 上的开源模型默认提供就是 `float16` 或 `bfloat16` 权重。

## 5. 参考资料

1.  [PyTorch Blog: Mixed Precision Training](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)
2.  [PyTorch Forums: Why bfloat16 over float16?](https://discuss.pytorch.org/t/why-to-keep-parameters-in-float32-why-not-in-b-float16/179931)
3.  [Medium: bfloat16 vs float32 vs float16](https://medium.com/@manyi.yim/bfloat16-vs-float32-vs-float16-back-to-the-basics-80d4aec49ca8)
