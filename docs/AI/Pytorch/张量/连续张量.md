“连续”这个词在张量（Tensor）中主要描述的是**内存中的物理存储顺序**与**逻辑上的索引顺序**是否一致。

简单来说，一个张量在内存里其实是一串**一维的数字排成一排**，而“连续”就是指这些数字在内存里的排列顺序，刚好和我们一行一行、一列一列看它的顺序是一模一样的。

---

### 1. 核心原理：一维存储 vs 多维逻辑
虽然我们看到的张量是多维的（比如 $2 \times 3$ 的矩阵），但计算机内存只能以**一维（线性）**的方式存储数据。

* **连续张量（Contiguous Tensor）**：数据在内存里按照“行优先”的顺序紧挨着排列。
* **非连续张量（Non-contiguous Tensor）**：逻辑上相邻的元素，在物理内存中可能跳过了某些位置，或者顺序是乱多。



---

### 2. 为什么张量会变得“不连续”？
这通常是因为 PyTorch 为了提高效率，采用了**“视图（View）”机制**。

当你对一个张量执行 `transpose()`（转置）、`permute()`（维度换位）或 `narrow()`（窄化截取）时，PyTorch **不会在内存里复制一份新数据**，而只是修改了**步长（Stride）**。

#### 什么是步长（Stride）？
步长定义了：为了在某个维度移动到下一个元素，需要在物理内存中跨过多少个位置。

* **正常情况**：对于一个 $(3, 3)$ 矩阵，步长通常是 $(3, 1)$。意味着换行要跨 3 个数，换列跨 1 个数。
* **转置后**：如果执行了转置，步长变成了 $(1, 3)$。此时逻辑上的“下一行”在内存里其实只是“下一个数”，但逻辑上的“下一列”却要跨过 3 个数。这种**步长不再随维度递减**的状态，就是“非连续”。

---

### 3. 为什么要关注连续性？
最直接的原因就是：**`.view()` 方法要求张量必须是连续的。**

* **`.view()` 的本质**：它只是简单地把内存里那排一维数字重新按照新的形状解读。如果那排数字的顺序本身和逻辑顺序不一致（不连续），`.view()` 就无法正确还原出你想要的形状，因此会报错。
* **`.reshape()` 的处理**：它会自动检查，如果不连续，它会先在后台帮你复制一份连续的数据，再进行形状变换。

---

### 4. 如何检查和处理？

在代码中，你可以通过以下方式操作：

```python
import torch

x = torch.randn(3, 2)
print(x.is_contiguous()) # True，初始创建通常是连续的

# 转置操作
y = x.transpose(0, 1)
print(y.is_contiguous()) # False，转置后逻辑顺序与内存物理顺序不一致了

# 强制转换回连续状态
z = y.contiguous() 
print(z.is_contiguous()) # True，此时 PyTorch 在内存里重新排列了数据