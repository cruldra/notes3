# ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹ - æµç¨‹å›¾

åŸºäº `ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.py` ä»£ç çš„è®­ç»ƒæµç¨‹åˆ†æã€‚

## 1. æ•´ä½“è®­ç»ƒæµç¨‹

```mermaid
flowchart TD
    Start([å¼€å§‹]) --> InitEnv[åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ]
    InitEnv --> SetSeed[è®¾ç½®éšæœºç§å­]
    SetSeed --> Config[å®šä¹‰æ¨¡å‹é…ç½® MiniMindConfig]
    Config --> CheckCKP[æ£€æŸ¥æ£€æŸ¥ç‚¹ lm_checkpoint]
    CheckCKP --> InitModel[åˆå§‹åŒ–æ¨¡å‹ä¸åˆ†è¯å™¨ init_model]
    InitModel --> InitData[åŠ è½½æ•°æ®é›† PretrainDataset]
    InitData --> InitOpt[åˆå§‹åŒ–ä¼˜åŒ–å™¨ä¸Scaler]
    InitOpt --> LoadState{æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹?}
    
    LoadState -- æ˜¯ --> RestoreState[æ¢å¤æ¨¡å‹/ä¼˜åŒ–å™¨çŠ¶æ€]
    LoadState -- å¦ --> DDPWrap
    RestoreState --> DDPWrap
    
    DDPWrap{æ˜¯å¦åˆ†å¸ƒå¼ç¯å¢ƒ?} -- æ˜¯ --> DDP[DistributedDataParallel å°è£…]
    DDPWrap -- å¦ --> LoopStart
    DDP --> LoopStart
    
    LoopStart[å¼€å§‹ Epoch å¾ªç¯] --> SetSampler[è®¾ç½® Sampler Epoch]
    SetSampler --> CheckResume{æ˜¯å¦é¦–ä¸ªEpochä¸”éœ€è·³è¿‡Step?}
    
    CheckResume -- æ˜¯ --> SkipSampler[ä½¿ç”¨ SkipBatchSampler]
    CheckResume -- å¦ --> NormalLoader[å¸¸è§„ DataLoader]
    
    SkipSampler --> TrainEpoch[[è°ƒç”¨ train_epoch]]
    NormalLoader --> TrainEpoch
    
    TrainEpoch --> StepEnd{Epoch å¾ªç¯ç»“æŸ?}
    StepEnd -- å¦ --> LoopStart
    StepEnd -- æ˜¯ --> Cleanup[æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹]
    Cleanup --> End([ç»“æŸ])
```

## 2. è®­ç»ƒå¾ªç¯è¯¦æƒ… (train_epoch)

```mermaid
flowchart TD
    Start([å¼€å§‹ train_epoch]) --> StepStart(å¼€å§‹ Step)
    StepStart --> MoveData[æ•°æ®ç§»è‡³ GPU]
    MoveData --> CalcLR[è®¡ç®—å¹¶æ›´æ–°å­¦ä¹ ç‡]
    CalcLR --> Forward[å‰å‘ä¼ æ’­ Autocast]
    Forward --> CalcLoss["è®¡ç®— Loss (Loss + AuxLoss)"]
    CalcLoss --> Backward[åå‘ä¼ æ’­ Backward]
    Backward --> OptStep{æ˜¯å¦æ¯8æ­¥æ›´æ–°?}
    
    OptStep -- æ˜¯ --> ClipGrad[æ¢¯åº¦è£å‰ª]
    ClipGrad --> UpdateParam[å‚æ•°æ›´æ–° Scaler.step]
    UpdateParam --> ZeroGrad[æ¸…ç©ºæ¢¯åº¦]
    ZeroGrad --> CheckLog
    OptStep -- å¦ --> CheckLog
    
    CheckLog{æ˜¯å¦æ‰“å°æ—¥å¿—?} -- æ˜¯ --> LogInfo[æ‰“å° Loss/LR/Time/WandB]
    CheckLog -- å¦ --> CheckSave
    LogInfo --> CheckSave
    
    CheckSave{æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹?} -- æ˜¯ --> SaveCKP[ä¿å­˜æ¨¡å‹æƒé‡ä¸è®­ç»ƒçŠ¶æ€]
    CheckSave -- å¦ --> NextStep
    SaveCKP --> NextStep
    
    NextStep{Step å¾ªç¯ç»“æŸ?} -- å¦ --> StepStart
    NextStep -- æ˜¯ --> End([ç»“æŸ train_epoch])
```

## 3. æ¨¡å‹ç»„ä»¶æ¶æ„ (Class Diagram)

```mermaid
classDiagram
    note "MiniMind æ¨¡å‹æ ¸å¿ƒç±»ç»“æ„"
    
    %% é…ç½®ä¸é¡¶å±‚
    class MiniMindConfig {
        +int hidden_size
        +int num_hidden_layers
        +int num_attention_heads
        +bool use_moe
        +int vocab_size
    }

    class MiniMindForCausalLM {
        +MiniMindConfig config
        +MiniMindModel model
        +Linear lm_head
        +forward()
    }

    %% åŸºç¡€æ¨¡å‹ç»“æ„
    class MiniMindModel {
        +Embedding embed_tokens
        +ModuleList layers
        +RMSNorm norm
        +forward()
    }

    class MiniMindBlock {
        +int layer_id
        +Attention self_attn
        +RMSNorm input_layernorm
        +RMSNorm post_attention_layernorm
        +Module mlp
        +forward()
    }

    %% æ ¸å¿ƒè®¡ç®—ç»„ä»¶
    class Attention {
        +Linear q_proj
        +Linear k_proj
        +Linear v_proj
        +Linear o_proj
        +apply_rotary_pos_emb()
        +forward()
    }

    class FeedForward {
        +Linear gate_proj
        +Linear up_proj
        +Linear down_proj
        +forward()
    }

    class RMSNorm {
        +Parameter weight
        +forward()
    }

    %% MoE ç»„ä»¶
    class MOEFeedForward {
        +MoEGate gate
        +ModuleList experts
        +ModuleList shared_experts
        +forward()
        +moe_infer()
    }

    class MoEGate {
        +Linear weight
        +forward()
    }

    %% å…³ç³»å›¾
    MiniMindForCausalLM o-- MiniMindConfig : é…ç½®ä¾èµ–
    MiniMindForCausalLM *-- MiniMindModel : åŒ…å«
    MiniMindModel "1" *-- "N" MiniMindBlock : å †å å±‚
    
    MiniMindBlock *-- Attention : è‡ªæ³¨æ„åŠ›
    MiniMindBlock *-- RMSNorm : å½’ä¸€åŒ–
    
    %% FFN çš„ä¸¤ç§å®ç°
    MiniMindBlock *-- FeedForward : FFN (å½“ use_moe=False)
    MiniMindBlock *-- MOEFeedForward : FFN (å½“ use_moe=True)
    
    MOEFeedForward *-- MoEGate : è·¯ç”±é—¨æ§
    MOEFeedForward o-- FeedForward : åŒ…å«å¤šä¸ªä¸“å®¶
```

## 4. å…³é”®ç»„ä»¶è¯´æ˜

*   **MiniMindConfig**: æ¨¡å‹çš„é…ç½®ä¸­å¿ƒï¼Œå®šä¹‰äº†æ¨¡å‹å¤§å°ã€å±‚æ•°ã€å¤´æ•°ä»¥åŠæ˜¯å¦å¼€å¯ MoE ç­‰å…³é”®è¶…å‚æ•°ã€‚
*   **MiniMindForCausalLM**: é¡¶å±‚å°è£…ï¼ŒåŒ…å«åŸºç¡€ Transformer æ¨¡å‹ (`MiniMindModel`) å’Œè¯­è¨€æ¨¡å‹å¤´ (`lm_head`)ï¼Œç”¨äºç”Ÿæˆä»»åŠ¡ã€‚
*   **MiniMindModel**: ä¸å« Head çš„åŸºç¡€æ¨¡å‹ï¼Œè´Ÿè´£ Token Embeddingã€å¤šå±‚ Decoder å †å ä»¥åŠæœ€ç»ˆçš„ RMSNormã€‚
*   **MiniMindBlock**: å•ä¸ª Transformer Decoder å±‚ï¼Œé‡‡ç”¨ Pre-Norm ç»“æ„ï¼ŒåŒ…å« Self-Attention å’Œ FFN/MoEã€‚
*   **Attention**: æ”¯æŒ GQA (Grouped Query Attention) å’Œ RoPE (Rotary Positional Embeddings) çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…å« Flash Attention ä¼˜åŒ–ã€‚
*   **FeedForward (SwiGLU)**: æ ‡å‡†çš„ SwiGLU å‰é¦ˆç½‘ç»œï¼Œç”¨äº Dense æ¨¡å¼æˆ–ä½œä¸º MoE çš„ä¸“å®¶å•å…ƒã€‚
*   **MOEFeedForward**: æ··åˆä¸“å®¶æ¨¡å—ï¼ŒåŒ…å«ä¸€ä¸ªé—¨æ§ç½‘ç»œ (`MoEGate`) å’Œå¤šä¸ªä¸“å®¶ç½‘ç»œ (`experts`/`shared_experts`)ã€‚
*   **MoEGate**: è·¯ç”±ç½‘ç»œï¼Œè®¡ç®—è¾“å…¥ Token å¯¹å„ä¸ªä¸“å®¶çš„æƒé‡ï¼ˆTop-K è·¯ç”±ï¼‰ã€‚

---


ä¸ºäº†è®©ä½ å½»åº•çœ‹æ‡‚è¿™æ®µä»£ç ï¼Œæˆ‘ä»¬å°†è´¯ç©¿ä½¿ç”¨ä¸€ä¸ªæ ¸å¿ƒä¾‹å­ï¼š**æ•™ä¸€ä¸ªå®Œå…¨ä¸æ‡‚ä¸­æ–‡çš„å¤–å›½ç•™å­¦ç”Ÿ"å°æ˜"ï¼ˆMiniMindï¼‰å­¦ä¹ å†™ä¸­æ–‡å°è¯´ã€‚**

---

## ä¸€ã€å¼€åœºå‡†å¤‡ï¼šè®¾ç½®éšæœºç§å­ (`setup_seed`)

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 114-121 è¡Œ)

```python
def setup_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šå›ºå®šè€ƒè¯•é¢˜åº“çš„é¡ºåº

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦æ•™å°æ˜å†™å°è¯´ï¼Œå‡†å¤‡äº† 1000 é“ç»ƒä¹ é¢˜ã€‚

#### é—®é¢˜åœºæ™¯ï¼š

å¦‚æœæ¯æ¬¡ç»ƒä¹ æ—¶éƒ½**éšæœºæ‰“ä¹±**é¢˜ç›®é¡ºåºï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

- **ä»Šå¤©**ï¼šå°æ˜å…ˆå­¦ä¹ "å¦‚ä½•æå†™äººç‰©"ï¼Œå†å­¦"å¦‚ä½•æ„å»ºæƒ…èŠ‚"
- **æ˜å¤©**ï¼šå°æ˜å…ˆå­¦ä¹ "å¦‚ä½•æ„å»ºæƒ…èŠ‚"ï¼Œå†å­¦"å¦‚ä½•æå†™äººç‰©"
- **ç»“æœ**ï¼šä¸¤å¤©åå¯¹æ¯”å°æ˜çš„è¿›æ­¥æ—¶ï¼Œä½ æ— æ³•åˆ¤æ–­åˆ°åº•æ˜¯æ•™å­¦æ–¹æ³•æ”¹è¿›äº†ï¼Œè¿˜æ˜¯çº¯ç²¹å› ä¸ºé¢˜ç›®é¡ºåºå˜äº†

è¿™å°±åƒç§‘å­¦å®éªŒä¸­çš„**ä¸å¯é‡å¤æ€§é—®é¢˜**â€”â€”ä½ æ— æ³•ç¡®è®¤ç»“æœæ˜¯ç”±ä½ çš„æ”¹è¿›å¯¼è‡´çš„ï¼Œè¿˜æ˜¯è¿æ°”å¥½ç¢°åˆ°äº†ç®€å•çš„é¢˜ã€‚

#### è§£å†³æ–¹æ¡ˆï¼š**å›ºå®šéšæœºç§å­** = å›ºå®šé¢˜åº“é¡ºåº

```python
random.seed(seed)          # å›ºå®š Python å†…ç½®éšæœºæ•°ç”Ÿæˆå™¨
np.random.seed(seed)       # å›ºå®š NumPy çš„éšæœºæ•°ç”Ÿæˆå™¨  
torch.manual_seed(seed)    # å›ºå®š PyTorch CPU ä¸Šçš„éšæœºæ•°
torch.cuda.manual_seed(seed)       # å›ºå®šå•ä¸ª GPU çš„éšæœºæ•°
torch.cuda.manual_seed_all(seed)   # å›ºå®šæ‰€æœ‰ GPU çš„éšæœºæ•°
```

**éšå–»å¯¹åº”**ï¼š

- `random.seed(seed)`ï¼šå›ºå®š Python è‡ªå·±çš„é¢˜åº“æ´—ç‰Œæ–¹å¼
- `np.random.seed(seed)`ï¼šå›ºå®šæ•°æ®é¢„å¤„ç†ï¼ˆæ¯”å¦‚å›¾ç‰‡å¢å¼ºï¼‰çš„éšæœºæ€§
- `torch.manual_seed(seed)`ï¼šå›ºå®šæ¨¡å‹åˆå§‹åŒ–æ—¶çš„"å¤§è„‘åˆå§‹è¿æ¥æ–¹å¼"
- `torch.cuda.manual_seed_all(seed)`ï¼šå¦‚æœå°æ˜æœ‰å¤šä¸ª"å‰¯å¤§è„‘"ï¼ˆå¤šGPUï¼‰ï¼Œè®©å®ƒä»¬çš„åˆå§‹çŠ¶æ€ä¹Ÿä¸€è‡´

#### é¢å¤–çš„ä¸¥æ ¼æªæ–½ï¼šå…³é—­è‡ªåŠ¨ä¼˜åŒ–

```python
torch.backends.cudnn.deterministic = True  # å¼ºåˆ¶ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•
torch.backends.cudnn.benchmark = False     # ç¦ç”¨è‡ªåŠ¨ç®—æ³•é€‰æ‹©
```

**éšå–»**ï¼š

- `deterministic = True`ï¼šå¼ºåˆ¶è¦æ±‚"æ¯æ¬¡è®¡ç®—éƒ½ç”¨ç›¸åŒçš„è§£é¢˜æ­¥éª¤"ï¼Œå“ªæ€•æ…¢ä¸€ç‚¹
  - ç±»ä¼¼äºï¼šè¦æ±‚å°æ˜æ¯æ¬¡å†™ä½œæ–‡éƒ½ä¸¥æ ¼æŒ‰ç…§"å¼€å¤´-å‘å±•-é«˜æ½®-ç»“å°¾"çš„æ­¥éª¤ï¼Œä¸å…è®¸è·³æ­¥
- `benchmark = False`ï¼šå…³é—­"è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•"åŠŸèƒ½
  - ç±»ä¼¼äºï¼šç¦æ­¢å°æ˜åœ¨è€ƒè¯•æ—¶"çœ‹é¢˜ç›®éš¾åº¦ä¸´æ—¶è°ƒæ•´ç­”é¢˜ç­–ç•¥"ï¼Œå¿…é¡»ç”¨å›ºå®šå¥—è·¯

#### ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆåšï¼Ÿ

| åœºæ™¯ | ä¸è®¾ç½®éšæœºç§å­ | è®¾ç½®éšæœºç§å­ |
|------|--------------|-------------|
| **å®éªŒå¯¹æ¯”** | æ— æ³•ç¡®è®¤æ”¹è¿›æ˜¯å¦æœ‰æ•ˆ | å¯ä»¥ç²¾ç¡®å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ•ˆæœ |
| **Bug è°ƒè¯•** | æ¯æ¬¡è¿è¡Œç»“æœä¸åŒï¼Œæ— æ³•å®šä½é—®é¢˜ | æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼Œæ–¹ä¾¿æ’æŸ¥ |
| **è®ºæ–‡å¤ç°** | åˆ«äººæ— æ³•å¤ç°ä½ çš„ç»“æœ | æä¾›ç§å­å€¼åï¼Œåˆ«äººå¯ä»¥å®Œå…¨å¤ç° |
| **åä½œå¼€å‘** | å›¢é˜Ÿæˆå‘˜çœ‹åˆ°ä¸åŒçš„è®­ç»ƒæ›²çº¿ | å¤§å®¶çœ‹åˆ°ç›¸åŒçš„è®­ç»ƒè¿‡ç¨‹ |

### ğŸ’¡ ç±»æ¯”æ€»ç»“

è®¾ç½®éšæœºç§å­å°±åƒï¼š

1. **æ‘„å½±æ£šæ‰“å…‰**ï¼šæ¯æ¬¡æ‹æ‘„éƒ½ç”¨ç›¸åŒçš„ç¯å…‰å¸ƒç½®ï¼Œè¿™æ ·å¯¹æ¯”ç…§ç‰‡æ—¶ï¼Œå·®å¼‚åªæ¥è‡ªæ¼”å‘˜è¡¨ç°ï¼Œè€Œä¸æ˜¯å…‰çº¿å˜åŒ–
2. **è€ƒè¯•æ ‡å‡†åŒ–**ï¼šè®©æ‰€æœ‰å­¦ç”ŸåšåŒä¸€å¥—å·å­ï¼Œå…¬å¹³æ¯”è¾ƒè°å­¦å¾—æ›´å¥½
3. **èœè°±é…æ–¹**ï¼šä¸¥æ ¼æŒ‰ç…§"ç›5å…‹ã€ç³–10å…‹"çš„é…æ–¹ï¼Œè€Œä¸æ˜¯"é€‚é‡"ï¼Œè¿™æ ·æ‰èƒ½ç¨³å®šå¤ç°ç¾å‘³

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ€§èƒ½ä»£ä»·**ï¼š
   - è®¾ç½® `deterministic=True` ä¼š**ç‰ºç‰²ä¸€äº›é€Ÿåº¦**ï¼Œå› ä¸ºç¦ç”¨äº†æŸäº›å¿«é€Ÿä½†éç¡®å®šæ€§çš„ç®—æ³•
   - é€‚åˆ**å®éªŒéªŒè¯é˜¶æ®µ**ï¼Œç”Ÿäº§ç¯å¢ƒå¯ä»¥å…³é—­ä»¥æé€Ÿ

2. **å¹¶ä¸æ˜¯100%ç¡®å®šæ€§**ï¼š
   - å³ä½¿è®¾ç½®äº†ç§å­ï¼Œåœ¨**ä¸åŒç¡¬ä»¶**ï¼ˆå¦‚ä¸åŒå‹å·çš„GPUï¼‰æˆ–**ä¸åŒPyTorchç‰ˆæœ¬**ä¸Šï¼Œç»“æœä»å¯èƒ½ç•¥æœ‰å·®å¼‚
   - å°±åƒç›¸åŒèœè°±åœ¨ä¸åŒå“ç‰Œçš„çƒ¤ç®±é‡Œï¼Œæ¸©åº¦æ›²çº¿ä¼šæœ‰å¾®å°åŒºåˆ«

3. **ç§å­å€¼çš„é€‰æ‹©**ï¼š
   - é€šå¸¸é€‰æ‹© `42`ï¼ˆç¨‹åºå‘˜æ¢—ï¼Œæºè‡ªã€Šé“¶æ²³ç³»æ¼«æ¸¸æŒ‡å—ã€‹ï¼‰ã€`0`ã€`1234` ç­‰ä»»æ„æ•´æ•°
   - åªè¦ä¿æŒä¸€è‡´å³å¯ï¼Œå…·ä½“æ•°å€¼å¹¶ä¸é‡è¦

---

## äºŒã€è®¾è®¡å¤§è„‘è“å›¾ï¼šMiniMindConfig é…ç½®ç±»

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 27-102 è¡Œ)

```python
class MiniMindConfig(PretrainedConfig):
    model_type = "minimind"

    def __init__(
        self,
        dropout: float = 0.0,
        bos_token_id: int = 1,
        eos_token_id: int = 2,
        hidden_act: str = "silu",
        hidden_size: int = 512,
        intermediate_size: int = None,
        max_position_embeddings: int = 32768,
        num_attention_heads: int = 8,
        num_hidden_layers: int = 8,
        num_key_value_heads: int = 2,
        vocab_size: int = 6400,
        rms_norm_eps: float = 1e-05,
        rope_theta: int = 1000000.0,
        inference_rope_scaling: bool = False,
        flash_attn: bool = True,
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        use_moe: bool = False,
        num_experts_per_tok: int = 2,
        n_routed_experts: int = 4,
        n_shared_experts: int = 1,
        scoring_func: str = "softmax",
        aux_loss_alpha: float = 0.01,
        seq_aux: bool = True,
        norm_topk_prob: bool = True,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # å¤–æ¨é•¿åº¦ = factor * original_max_position_embeddings = 32768
        self.rope_scaling = (
            {
                "beta_fast": 32,
                "beta_slow": 1,
                "factor": 16,
                "original_max_position_embeddings": 2048,
                "attention_factor": 1.0,
                "type": "yarn",
            }
            if self.inference_rope_scaling
            else None
        )
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°,é»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡
```

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šè®¾è®¡å°æ˜çš„å¤§è„‘è“å›¾

åœ¨æ•™å°æ˜å†™å°è¯´ä¹‹å‰,æˆ‘ä»¬éœ€è¦**ç»™å°æ˜è®¾è®¡ä¸€ä¸ª"å¤§è„‘"**ã€‚MiniMindConfig å°±åƒæ˜¯å¤§è„‘çš„è®¾è®¡å›¾çº¸,ä¸Šé¢æ ‡æ³¨äº†å„ç§å‚æ•°:è®°å¿†å®¹é‡å¤šå¤§ã€æ€è€ƒé€Ÿåº¦å¤šå¿«ã€æ³¨æ„åŠ›æœºåˆ¶æ€ä¹ˆå·¥ä½œç­‰ç­‰ã€‚

è®©æˆ‘ä»¬é€ä¸ªæ‹†è§£è¿™äº›å‚æ•°:

---

### ğŸ§  æ ¸å¿ƒç»“æ„å‚æ•°

#### 1. `hidden_size: int = 512` (å¤§è„‘çš„"ç¥ç»å…ƒæ€»æ•°")

**ä»£ç ä½ç½®**: ç¬¬ 36 è¡Œ

**ä½œç”¨**: å†³å®šæ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„ç»´åº¦å¤§å°ã€‚

**éšå–»è§£é‡Š**:

æƒ³è±¡å°æ˜çš„å¤§è„‘æœ‰ **512 ä¸ªç¥ç»å…ƒç»†èƒ**ã€‚æ¯å½“å°æ˜è¯»åˆ°ä¸€ä¸ªè¯(æ¯”å¦‚"çˆ±æƒ…"),è¿™ä¸ªè¯ä¼šè¢«è½¬æ¢æˆä¸€ä¸ªåŒ…å« 512 ä¸ªæ•°å­—çš„å‘é‡(ç±»ä¼¼äº 512 ä¸ªç¥ç»å…ƒçš„æ¿€æ´»çŠ¶æ€)ã€‚

- **`hidden_size=512`**: å°æ˜æœ‰ 512 ä¸ªç¥ç»å…ƒ,é€‚åˆç†è§£ç®€å•çš„æ•…äº‹æƒ…èŠ‚
- **`hidden_size=4096`** (åƒ Llama-7B): å°æ˜æœ‰ 4096 ä¸ªç¥ç»å…ƒ,èƒ½å¤„ç†æ›´å¤æ‚çš„é€»è¾‘å’Œç»†èŠ‚

**ç±»æ¯”**:

```
hidden_size = å¤§è„‘çš„"åƒç´ åˆ†è¾¨ç‡"

- hidden_size=128  â†’ çœ‹ä¸–ç•Œå°±åƒ 16x16 çš„é©¬èµ›å…‹å›¾ç‰‡
- hidden_size=512  â†’ çœ‹ä¸–ç•Œå°±åƒ 480p çš„æ ‡æ¸…è§†é¢‘
- hidden_size=4096 â†’ çœ‹ä¸–ç•Œå°±åƒ 4K è¶…æ¸…ç”»é¢
```

**æŠ€æœ¯ç»†èŠ‚**:

```python
# åœ¨ä»£ç ä¸­çš„åº”ç”¨ç¤ºä¾‹
self.embed_tokens = nn.Embedding(vocab_size, hidden_size)
# æŠŠæ¯ä¸ªè¯(vocab_sizeç§)æ˜ å°„æˆ hidden_size ç»´çš„å‘é‡
```

---

#### 2. `num_hidden_layers: int = 8` (å¤§è„‘çš„"æ€è€ƒå±‚æ•°")

**ä»£ç ä½ç½®**: ç¬¬ 40 è¡Œ

**ä½œç”¨**: å†³å®šæ¨¡å‹å †å äº†å¤šå°‘å±‚ Transformer Blockã€‚

**éšå–»è§£é‡Š**:

å°æ˜å¤„ç†ä¸€ä¸ªé—®é¢˜æ—¶,ä¼šç»è¿‡ **8 è½®æ·±åº¦æ€è€ƒ**:

1. **ç¬¬ 1 å±‚**: è¯†åˆ«åŸºæœ¬è¯è¯­("ç«ç‘°"ã€"çˆ±æƒ…")
2. **ç¬¬ 2 å±‚**: ç†è§£ç®€å•å…³ç³»("ç«ç‘°è±¡å¾çˆ±æƒ…")
3. **ç¬¬ 3 å±‚**: æŠŠæ¡å¥å­ç»“æ„("ä»–é€å¥¹ä¸€æœµç«ç‘°")
4. **ç¬¬ 4 å±‚**: ç†è§£æƒ…æ„Ÿè‰²å½©("æš—ç¤ºçˆ±æ„çš„è¡¨è¾¾")
5. **ç¬¬ 5-8 å±‚**: æ›´é«˜çº§çš„é€»è¾‘æ¨ç†ã€éšå–»ç†è§£ã€æƒ…èŠ‚è¿è´¯æ€§...

**å±‚æ•°è¶Šå¤š** = æ€è€ƒè¶Šæ·±å…¥,ä½†ä¹Ÿéœ€è¦æ›´å¤š"è®¡ç®—æ—¶é—´"(æ›´æ…¢,æ›´è€—æ˜¾å­˜)ã€‚

**ç±»æ¯”**:

```
å†™ä½œæ–‡æ—¶çš„æ€è€ƒæ­¥éª¤:
num_layers=1  â†’ çœ‹åˆ°é¢˜ç›®å°±ç›´æ¥å†™(å¹¼å„¿å›­æ°´å¹³)
num_layers=8  â†’ å®¡é¢˜â†’æ„æ€â†’åˆ—æçº²â†’å†™è‰ç¨¿â†’ä¿®æ”¹æ¶¦è‰²(ä¸­å­¦ç”Ÿæ°´å¹³)
num_layers=32 â†’ æ·±åº¦å“²å­¦æ€è¾¨ã€åå¤æ¨æ•²ç»†èŠ‚(å­¦æœ¯è®ºæ–‡æ°´å¹³,å¦‚ Llama-70B)
```

**ç»éªŒæ³•åˆ™**:

| æ¨¡å‹è§„æ¨¡ | å±‚æ•° | å¯¹åº”èƒ½åŠ› |
|---------|------|----------|
| å°æ¨¡å‹ (< 1B) | 8-12 å±‚ | ç®€å•å¯¹è¯ã€åŸºç¡€é—®ç­” |
| ä¸­æ¨¡å‹ (1B-10B) | 24-32 å±‚ | ä»£ç ç”Ÿæˆã€é•¿æ–‡æ‘˜è¦ |
| å¤§æ¨¡å‹ (> 10B) | 40-80 å±‚ | å¤æ‚æ¨ç†ã€åˆ›æ„å†™ä½œ |

---

#### 3. `num_attention_heads: int = 8` (å¤§è„‘çš„"æ³¨æ„åŠ›æ¢ç…§ç¯æ•°é‡")

**ä»£ç ä½ç½®**: ç¬¬ 39 è¡Œ

**ä½œç”¨**: å†³å®šè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­æœ‰å¤šå°‘ä¸ª"æ³¨æ„åŠ›å¤´"ã€‚

**éšå–»è§£é‡Š**:

å°æ˜åœ¨é˜…è¯»ä¸€å¥è¯æ—¶,ä¼šåŒæ—¶ç”¨ **8 ä¸ªæ¢ç…§ç¯** å…³æ³¨ä¸åŒçš„é‡ç‚¹:

**ç¤ºä¾‹å¥å­**: "å°çº¢å¸½èµ°è¿›æ£®æ—,é‡åˆ°äº†å¤§ç°ç‹¼"

- **æ¢ç…§ç¯ 1**: å…³æ³¨ä¸»è¯­("å°çº¢å¸½")
- **æ¢ç…§ç¯ 2**: å…³æ³¨åŠ¨ä½œ("èµ°è¿›")
- **æ¢ç…§ç¯ 3**: å…³æ³¨åœ°ç‚¹("æ£®æ—")
- **æ¢ç…§ç¯ 4**: å…³æ³¨å› æœå…³ç³»("èµ°è¿›" â†’ "é‡åˆ°")
- **æ¢ç…§ç¯ 5**: å…³æ³¨è§’è‰²å…³ç³»("å°çº¢å¸½" vs "å¤§ç°ç‹¼")
- **æ¢ç…§ç¯ 6-8**: å…³æ³¨è¯­æ°”ã€æƒ…æ„Ÿã€æ½œåœ¨å±é™©æš—ç¤º...

**ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªå¤´?**

å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½åªå…³æ³¨ä¸€ç§æ¨¡å¼(æ¯”å¦‚åªçœ‹åè¯),è€Œå¤šä¸ªå¤´èƒ½**å¹¶è¡Œå…³æ³¨ä¸åŒç»´åº¦çš„ä¿¡æ¯**,ç„¶åç»¼åˆèµ·æ¥å½¢æˆå…¨é¢ç†è§£ã€‚

**ç±»æ¯”**:

```
çœ‹ç”µå½±æ—¶çš„æ³¨æ„åŠ›åˆ†é…:
num_heads=1  â†’ åªç›¯ç€ä¸»è§’,å¿½ç•¥èƒŒæ™¯å’Œé…è§’
num_heads=8  â†’ åŒæ—¶å…³æ³¨:ä¸»è§’è¡¨æƒ…ã€èƒŒæ™¯éŸ³ä¹ã€é•œå¤´è¿åŠ¨ã€é…è§’ååº”...
num_heads=32 â†’ åƒä¸“ä¸šå½±è¯„äºº,è¿é“å…·ç»†èŠ‚ã€è‰²å½©éšå–»éƒ½ä¸æ”¾è¿‡
```

**æŠ€æœ¯ç»†èŠ‚**:

```python
# åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„åº”ç”¨
head_dim = hidden_size // num_attention_heads  
# æ¯ä¸ªæ³¨æ„åŠ›å¤´è´Ÿè´£çš„ç»´åº¦ = 512 / 8 = 64 ç»´
```

---

#### 4. `num_key_value_heads: int = 2` (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› GQA)

**ä»£ç ä½ç½®**: ç¬¬ 41 è¡Œ

**ä½œç”¨**: åœ¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)ä¸­,Key å’Œ Value ä½¿ç”¨çš„å¤´æ•°(é€šå¸¸å°‘äº Query çš„å¤´æ•°)ã€‚

**éšå–»è§£é‡Š**:

è¿™æ˜¯ä¸€ä¸ª**ä¼˜åŒ–æŠ€å·§**,ç”¨æ¥èŠ‚çœå°æ˜çš„"è®°å¿†å­˜å‚¨ç©ºé—´"ã€‚

**æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶(MHA)**:

```
Query (é—®é¢˜) æœ‰ 8 ä¸ªå¤´
Key   (ç´¢å¼•) æœ‰ 8 ä¸ªå¤´  â† æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ Key
Value (ç­”æ¡ˆ) æœ‰ 8 ä¸ªå¤´  â† æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ Value
```

**åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)**:

```
Query (é—®é¢˜) æœ‰ 8 ä¸ªå¤´
Key   (ç´¢å¼•) æœ‰ 2 ä¸ªå¤´  â† å¤šä¸ª Query å…±äº«åŒä¸€ä¸ª Key
Value (ç­”æ¡ˆ) æœ‰ 2 ä¸ªå¤´  â† å¤šä¸ª Query å…±äº«åŒä¸€ä¸ª Value

åˆ†ç»„æ–¹å¼:
- Queryå¤´ 0, 1, 2, 3 â†’ å…±äº« Key-Valueå¤´ 0
- Queryå¤´ 4, 5, 6, 7 â†’ å…±äº« Key-Valueå¤´ 1
```

**ç±»æ¯”**:

æƒ³è±¡å›¾ä¹¦é¦†çš„æ£€ç´¢ç³»ç»Ÿ:

- **MHA**(Multi-Head Attention): 8 ä¸ªè¯»è€…,æ¯äººæœ‰ä¸€æœ¬**ä¸“å±çš„å›¾ä¹¦ç›®å½•**
- **GQA**(Grouped Query Attention): 8 ä¸ªè¯»è€…,ä½†åªæœ‰ 2 æœ¬**å…±äº«çš„å›¾ä¹¦ç›®å½•**(æ¯ 4 äººå…±ç”¨ä¸€æœ¬)
- **å¥½å¤„**: ç›®å½•å†Œå°‘äº†,å­˜å‚¨ç©ºé—´çœäº† 75%,ä½†æ£€ç´¢æ•ˆç‡åŸºæœ¬ä¸é™

**å®é™…æ•ˆæœ**:

- **æ˜¾å­˜èŠ‚çœ**: KV Cache ä» `8 * seq_len * head_dim` é™åˆ° `2 * seq_len * head_dim`
- **é€Ÿåº¦æå‡**: æ¨ç†æ—¶å‡å°‘å†…å­˜è®¿é—®,å°¤å…¶åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ—¶æ•ˆæœæ˜æ˜¾
- **æ€§èƒ½æŸå¤±**: å‡ ä¹æ²¡æœ‰(Meta çš„ Llama 2 è®ºæ–‡è¯æ˜ GQA æ€§èƒ½æ¥è¿‘ MHA)

---

#### 5. `vocab_size: int = 6400` (å°æ˜çš„"è¯æ±‡é‡")

**ä»£ç ä½ç½®**: ç¬¬ 42 è¡Œ

**ä½œç”¨**: æ¨¡å‹èƒ½è¯†åˆ«çš„ä¸åŒ token(è¯æˆ–å­—ç¬¦)æ•°é‡ã€‚

**éšå–»è§£é‡Š**:

å°æ˜çš„è¯æ±‡è¡¨é‡Œæœ‰ **6400 ä¸ªè¯**ã€‚

**ä¾‹å­**:

```python
vocab = {
    0: "<PAD>",      # å¡«å……ç¬¦å·
    1: "<BOS>",      # å¥å­å¼€å§‹
    2: "<EOS>",      # å¥å­ç»“æŸ
    3: "æˆ‘",
    4: "çˆ±",
    5: "ä½ ",
    ...
    6399: "é‡å­çº ç¼ "
}
```

**ä¸åŒ vocab_size çš„å¯¹æ¯”**:

| Vocab Size | è¦†ç›–è¯­è¨€ | å…¸å‹åº”ç”¨ | 
|-----------|---------|---------|
| 6,400 | ä¸­æ–‡å¸¸ç”¨å­— + åŸºç¡€è¯æ±‡ | ä¸­æ–‡å°è¯´ç”Ÿæˆ |
| 32,000 | ä¸­è‹±æ–‡æ··åˆ | GPT-2 |
| 50,257 | è‹±æ–‡ + ç‰¹æ®Šç¬¦å· | GPT-3 |
| 100,000+ | å¤šè¯­è¨€ + ä»£ç  | Llama 2 |

**ä¸ºä»€ä¹ˆä¸è®¾ç½®å¾—è¶Šå¤§è¶Šå¥½?**

```
è¯æ±‡é‡å¤ªå°(å¦‚ 1000):
- ä¼˜ç‚¹: æ¨¡å‹å°,è®­ç»ƒå¿«
- ç¼ºç‚¹: å¾ˆå¤šè¯è¡¨è¾¾ä¸äº†,åªèƒ½ç”¨ç»„åˆ(å¦‚"é‡å­"+"çº ç¼ ")

è¯æ±‡é‡å¤ªå¤§(å¦‚ 100000):
- ä¼˜ç‚¹: è¦†ç›–æ›´å¤šç”Ÿåƒ»è¯å’Œä¸“ä¸šæœ¯è¯­
- ç¼ºç‚¹: 
  1. å¢åŠ æ¨¡å‹å‚æ•°é‡(åµŒå…¥å±‚å ç”¨ç©ºé—´å¤§)
  2. æ¯ä¸ªè¯çš„è®­ç»ƒæ ·æœ¬å˜å°‘(ç¨€ç–æ€§é—®é¢˜)
```

**æŠ€æœ¯ç»†èŠ‚**:

```python
# åµŒå…¥å±‚çš„å‚æ•°é‡è®¡ç®—
embedding_params = vocab_size * hidden_size
# 6400 * 512 = 3,276,800 ä¸ªå‚æ•°
# å¦‚æœ vocab_size æ‰©å¤§åˆ° 64000,å‚æ•°é‡å°±ä¼š x10
```

---

### âš™ï¸ ä¼˜åŒ–ä¸æŠ€å·§å‚æ•°

#### 6. `intermediate_size: int = None` (FFN çš„"ä¸­é—´å±‚æ‰©å¼ å€æ•°")

**ä»£ç ä½ç½®**: ç¬¬ 37 è¡Œ

**ä½œç”¨**: å‰é¦ˆç½‘ç»œ(FFN)ä¸­é—´å±‚çš„ç»´åº¦ã€‚å¦‚æœè®¾ä¸º `None`,ä»£ç ä¼šè‡ªåŠ¨è®¡ç®—ä¸º `hidden_size * 8/3`ã€‚

**éšå–»è§£é‡Š**:

å°æ˜åœ¨æ€è€ƒæ—¶,ä¼šç»å†ä¸€ä¸ª"**å‘æ•£æ€ç»´ â†’ æ”¶æ•›æ€»ç»“**"çš„è¿‡ç¨‹:

```
è¾“å…¥(512 ç»´) 
  â†“ 
ã€æ‰©å¼ ã€‘â†’ ä¸­é—´å±‚(1365 ç»´)  â† è¿™é‡Œæ˜¯ intermediate_size
  â†“
ã€å‹ç¼©ã€‘â†’ è¾“å‡º(512 ç»´)
```

**ä¸ºä»€ä¹ˆè¦å…ˆæ‰©å¼ å†å‹ç¼©?**

ç±»æ¯”å†™ä½œæ–‡çš„æ€è€ƒè¿‡ç¨‹:

1. **æ‰©å¼ é˜¶æ®µ**(`gate_proj` + `up_proj`):
   - çœ‹åˆ°ä¸»é¢˜è¯"æ˜¥å¤©",å¤§è„‘ç¬é—´è”æƒ³åˆ°:
     - è§†è§‰: ç»¿è‰²ã€èŠ±æœµã€é˜³å…‰
     - å¬è§‰: é¸Ÿé¸£ã€æµæ°´
     - è§¦è§‰: æ¸©æš–ã€å¾®é£
     - æƒ…æ„Ÿ: å¸Œæœ›ã€æ´»åŠ›
   - ä» 1 ä¸ªæ¦‚å¿µæ‰©å¼ åˆ° N ä¸ªç›¸å…³è”æƒ³

2. **å‹ç¼©é˜¶æ®µ**(`down_proj`):
   - ä»æ‰€æœ‰è”æƒ³ä¸­,ç­›é€‰å‡ºæœ€ç›¸å…³çš„å‡ ä¸ª
   - åˆæˆæœ€ç»ˆè¾“å‡º:"æ˜¥å¤©æ˜¯ä¸‡ç‰©å¤è‹çš„å­£èŠ‚"

**é»˜è®¤é…ç½®è®¡ç®—**:

```python
# åœ¨ FeedForward ç±»çš„ __init__ ä¸­(ç¬¬ 260-264 è¡Œ)
if config.intermediate_size is None:
    intermediate_size = int(config.hidden_size * 8 / 3)  # 512 * 8/3 â‰ˆ 1365
    # å¯¹é½åˆ° 64 çš„å€æ•°(ä¸ºäº† GPU è®¡ç®—æ•ˆç‡)
    config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
    # æœ€ç»ˆ = 1408 (21 * 64)
```

**ä¸ºä»€ä¹ˆæ˜¯ 8/3 å€?**

- è¿™æ˜¯ Llama æ¶æ„çš„ç»éªŒå€¼
- åœ¨æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—å¹³è¡¡
- GPT ç³»åˆ—é€šå¸¸ç”¨ 4 å€(å¦‚ `hidden_size=768` â†’ `intermediate_size=3072`)

---

#### 7. `dropout: float = 0.0` (é˜²æ­¢"æ­»è®°ç¡¬èƒŒ"çš„é—å¿˜ç‡)

**ä»£ç ä½ç½®**: ç¬¬ 32 è¡Œ

**ä½œç”¨**: è®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€éƒ¨åˆ†ç¥ç»å…ƒ,é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

**éšå–»è§£é‡Š**:

å°æ˜åœ¨ç»ƒä¹ å†™ä½œæ–‡æ—¶,æˆ‘ä»¬ä¼šéšæœº"å±è”½"ä»–çš„ä¸€éƒ¨åˆ†è®°å¿†:

**åœºæ™¯**:

```
å®Œæ•´è®°å¿†(dropout=0.0):
- å°æ˜è®°å¾—:"å¼€å¤´å¿…é¡»ç”¨'åœ¨ä¸€ä¸ªé£å’Œæ—¥ä¸½çš„æ—©æ™¨'"
- ç»“æœ:å†™100ç¯‡ä½œæ–‡éƒ½æ˜¯è¿™ä¸ªå¼€å¤´(æ­»è®°ç¡¬èƒŒ,æ²¡æœ‰åˆ›é€ åŠ›)

éšæœºé—å¿˜(dropout=0.1):
- è®­ç»ƒæ—¶,éšæœºè®©å°æ˜"å¿˜è®°" 10% çš„è®°å¿†
- æœ‰æ—¶å¿˜äº†å›ºå®šå¼€å¤´,è¢«è¿«è‡ªå·±åˆ›é€ æ–°çš„å¼€å¤´
- ç»“æœ:å­¦ä¼šäº†ä¸¾ä¸€åä¸‰,è€Œä¸æ˜¯æ­»è®°æ¨¡æ¿
```

**ä¸ºä»€ä¹ˆä»£ç é‡Œè®¾ç½®ä¸º 0.0?**

```python
dropout: float = 0.0  # é»˜è®¤ä¸ä½¿ç”¨ Dropout
```

**åŸå› **:

1. **å¤§æ¨¡å‹æ—¶ä»£çš„æ–°å‘ç°**: 
   - åœ¨å°æ¨¡å‹(BERT/GPT-2)æ—¶ä»£,Dropout å¾ˆé‡è¦
   - ä½†åœ¨å¤§æ¨¡å‹(Llama/GPT-3+)æ—¶ä»£,å‘ç° Dropout æ•ˆæœä¸æ˜æ˜¾,ç”šè‡³æœ‰è´Ÿä½œç”¨
   
2. **è®­ç»ƒæ•°æ®å¤Ÿå¤§**: 
   - è®­ç»ƒæ•°æ®æœ‰å‡ ç™¾ GB æ—¶,æ¨¡å‹å¾ˆéš¾"æ­»è®°ç¡¬èƒŒ"æ‰€æœ‰æ•°æ®
   - å¤©ç„¶å°±æœ‰"æ³›åŒ–èƒ½åŠ›"
   
3. **å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•**: 
   - ä½¿ç”¨äº† Weight Decay(æƒé‡è¡°å‡)
   - æ•°æ®å¢å¼º(Data Augmentation)
   - è®­ç»ƒæ—¶çš„éšæœºæ€§(å¦‚ Batch é¡ºåº)

**å¦‚æœä½ çš„åœºæ™¯éœ€è¦ Dropout**:

```python
config = MiniMindConfig(
    dropout=0.1,  # å°æ•°æ®é›†æ—¶å¯ä»¥å°è¯•
    hidden_size=512,
    ...
)
```

---

#### 8. `max_position_embeddings: int = 32768` (å°æ˜çš„"è®°å¿†é•¿åº¦ä¸Šé™")

**ä»£ç ä½ç½®**: ç¬¬ 38 è¡Œ

**ä½œç”¨**: æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦(token æ•°é‡)ã€‚

**éšå–»è§£é‡Š**:

å°æ˜çš„çŸ­æœŸè®°å¿†æœ€å¤šèƒ½è®°ä½ **32768 ä¸ªè¯**ã€‚

**å®é™…å¯¹æ¯”**:

| é•¿åº¦ | å¯¹åº”å†…å®¹ | 
|------|---------|
| 512 | ä¸€ç¯‡çŸ­æ–°é—» |
| 2048 | ä¸€ç¯‡ä¸­é•¿æ–‡ç« (GPT-3 é»˜è®¤) |
| 4096 | ä¸€æœ¬çŸ­ç¯‡å°è¯´çš„ä¸€ç«  |
| 32768 | ä¸€æœ¬ä¸­ç¯‡å°è¯´ / ä¸€ä»½é•¿ç ”ç©¶æŠ¥å‘Š |
| 128000 | GPT-4 Turbo çš„é•¿åº¦,çº¦ä¸€æœ¬ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹ |

**ä¸ºä»€ä¹ˆä¸è®¾ç½®å¾—è¶Šé•¿è¶Šå¥½?**

```
é—®é¢˜ 1: è®¡ç®—å¤æ‚åº¦çˆ†ç‚¸
- æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ = O(seq_lenÂ²)
- é•¿åº¦ç¿»å€,è®¡ç®—é‡ç¿» 4 å€!
  - 2048 token â†’ éœ€è¦è®¡ç®— 2048Â² = 419 ä¸‡æ¬¡æ³¨æ„åŠ›
  - 32768 token â†’ éœ€è¦è®¡ç®— 32768Â² = 10.7 äº¿æ¬¡æ³¨æ„åŠ›

é—®é¢˜ 2: æ˜¾å­˜å ç”¨æ¿€å¢
- KV Cache å¤§å° = 2 * num_layers * seq_len * hidden_size
- é•¿åº¦ç¿»å€,æ˜¾å­˜å ç”¨ä¹Ÿç¿»å€
```

**æŠ€æœ¯ç»†èŠ‚**:

```python
# åœ¨ precompute_freqs_cis å‡½æ•°ä¸­(ç¬¬ 608-690 è¡Œ)
# ä¼šé¢„å…ˆè®¡ç®— max_position_embeddings ä¸ªä½ç½®çš„æ—‹è½¬ç¼–ç 
freqs_cos, freqs_sin = precompute_freqs_cis(
    dim=head_dim,
    end=32768,  # æå‰è®¡ç®—å¥½ 32768 ä¸ªä½ç½®çš„ cos/sin å€¼
    rope_base=config.rope_theta,
)
```

---

#### 9. `rope_theta: int = 1000000.0` (ä½ç½®ç¼–ç çš„"æ—¶é’Ÿé¢‘ç‡")

**ä»£ç ä½ç½®**: ç¬¬ 44 è¡Œ

**ä½œç”¨**: RoPE(æ—‹è½¬ä½ç½®ç¼–ç )çš„åŸºé¢‘å‚æ•°,å½±å“ä½ç½®ä¿¡æ¯çš„ç¼–ç æ–¹å¼ã€‚

**éšå–»è§£é‡Š**:

æƒ³è±¡å°æ˜çš„å¤§è„‘é‡Œæœ‰ä¸€ä¸ª"æ—¶é’Ÿç³»ç»Ÿ",ç”¨æ¥è®°å½•æ¯ä¸ªè¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚

**ä¸¤ç§æ—¶é’Ÿå¯¹æ¯”**:

```
å¿«é€Ÿæ—¶é’Ÿ(rope_theta=10000,GPT/BERT å¸¸ç”¨):
- ä½ç½® 0: æŒ‡é’ˆæŒ‡å‘ 0Â°
- ä½ç½® 1: æŒ‡é’ˆæŒ‡å‘ 36Â°
- ä½ç½® 2: æŒ‡é’ˆæŒ‡å‘ 72Â°
- ...
- ä½ç½® 100: æŒ‡é’ˆè½¬äº† 10 åœˆ,åˆå›åˆ° 0Â°(äº§ç”Ÿæ··æ·†!)

æ…¢é€Ÿæ—¶é’Ÿ(rope_theta=1000000,Llama å¸¸ç”¨):
- ä½ç½® 0: æŒ‡é’ˆæŒ‡å‘ 0Â°
- ä½ç½® 1: æŒ‡é’ˆæŒ‡å‘ 0.36Â°
- ä½ç½® 2: æŒ‡é’ˆæŒ‡å‘ 0.72Â°
- ...
- ä½ç½® 10000: æŒ‡é’ˆæ‰è½¬ 3.6Â°(ä»ç„¶æ¸…æ™°å¯è¾¨)
```

**ä¸ºä»€ä¹ˆç”¨ 1000000?**

**ç›®çš„**: æ”¯æŒæ›´é•¿çš„åºåˆ—,é¿å…ä½ç½®ä¿¡æ¯"å¾ªç¯æ··æ·†"

**ç±»æ¯”**:

```
å°±åƒé’Ÿè¡¨çš„è®¾è®¡:
- ç§’é’ˆ: 60 ç§’è½¬ä¸€åœˆ(çŸ­å‘¨æœŸ,é€‚åˆè®¡æ—¶å‡ åˆ†é’Ÿ)
- åˆ†é’ˆ: 60 åˆ†é’Ÿè½¬ä¸€åœˆ(ä¸­å‘¨æœŸ)
- æ—¶é’ˆ: 12 å°æ—¶è½¬ä¸€åœˆ(é•¿å‘¨æœŸ,é€‚åˆè®¡æ—¶ä¸€æ•´å¤©)

rope_theta å°±æ˜¯æ§åˆ¶"æ—¶é’ˆè½¬é€Ÿ"çš„å‚æ•°
- theta=10000   â†’ åƒç§’é’ˆ,é€‚åˆçŸ­æ–‡æœ¬
- theta=1000000 â†’ åƒæ—¶é’ˆ,é€‚åˆé•¿æ–‡æœ¬
```

**æŠ€æœ¯ç»†èŠ‚**:

```python
# åœ¨ precompute_freqs_cis ä¸­çš„è®¡ç®—(ç¬¬ 632 è¡Œ)
freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2).float() / dim))
# rope_base=1000000 æ—¶,é¢‘ç‡ä¼šéå¸¸ä½,é€‚åˆç¼–ç é•¿åºåˆ—
```

---

#### 10. `flash_attn: bool = True` (å¯ç”¨"é—ªç”µæ³¨æ„åŠ›"åŠ é€Ÿ)

**ä»£ç ä½ç½®**: ç¬¬ 46 è¡Œ

**ä½œç”¨**: æ˜¯å¦ä½¿ç”¨ Flash Attention ç®—æ³•ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—ã€‚

**éšå–»è§£é‡Š**:

**æ™®é€šæ³¨æ„åŠ›è®¡ç®—**:

```
å°æ˜é˜…è¯»ä¸€ç¯‡æ–‡ç« (1000 ä¸ªè¯):
1. å…ˆæŠŠæ‰€æœ‰è¯çš„å…³ç³»å†™åœ¨ä¸€å¼ å¤§è¡¨æ ¼ä¸Š(1000x1000 = 100 ä¸‡ä¸ªæ ¼å­)
2. å†ä»è¡¨æ ¼ä¸­æŸ¥æ‰¾éœ€è¦çš„ä¿¡æ¯
3. æœ€åæŠŠè¡¨æ ¼æ‰”æ‰

é—®é¢˜: è¿™å¼ è¡¨æ ¼éå¸¸å åœ°æ–¹(æ˜¾å­˜),è€Œä¸”å¤§éƒ¨åˆ†ä¿¡æ¯å…¶å®ç”¨ä¸åˆ°
```

**Flash Attention**:

```
æ”¹è¿›æ–¹æ¡ˆ:
1. ä¸è¦ä¸€æ¬¡æ€§ç”Ÿæˆæ•´å¼ å¤§è¡¨æ ¼
2. åˆ†å—è®¡ç®—: æ¯æ¬¡åªç®— 64x64 çš„å°å—(4096 ä¸ªæ ¼å­)
3. ç®—å®Œä¸€å—,ç«‹åˆ»ç”¨æ‰,å†ç®—ä¸‹ä¸€å—
4. æœ€ç»ˆç»“æœå®Œå…¨ä¸€æ ·,ä½†èŠ‚çœäº† 95% çš„ä¸´æ—¶å­˜å‚¨ç©ºé—´!
```

**å®é™…æ•ˆæœå¯¹æ¯”**:

| æŒ‡æ ‡ | æ™®é€šæ³¨æ„åŠ› | Flash Attention |
|------|----------|----------------|
| **æ˜¾å­˜å ç”¨** | 16 GB | 4 GB |
| **é€Ÿåº¦** | 100 ms | 30 ms |
| **ç²¾åº¦** | å®Œå…¨ä¸€è‡´ | å®Œå…¨ä¸€è‡´ |

**ä»£ç ä¸­çš„åº”ç”¨**(ç¬¬ 499-513 è¡Œ):

```python
if self.flash and (seq_len > 1) and ...:
    # ä½¿ç”¨ PyTorch å†…ç½®çš„é«˜æ•ˆå®ç°
    output = F.scaled_dot_product_attention(
        xq, xk, xv,
        dropout_p=self.dropout if self.training else 0.0,
        is_causal=True,  # è‡ªåŠ¨åº”ç”¨å› æœæ©ç 
    )
else:
    # é™çº§åˆ°æ‰‹åŠ¨å®ç°(å…¼å®¹æ—§ç‰ˆæœ¬)
    scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
    ...
```

**è¦æ±‚**:

- PyTorch >= 2.0
- CUDA æ”¯æŒ

---

### ğŸ”§ ç‰¹æ®ŠTokenå‚æ•°

#### 11-12. `bos_token_id` å’Œ `eos_token_id` (å¥å­çš„"èµ·æ­¢æ ‡è®°")

**ä»£ç ä½ç½®**: ç¬¬ 33-34 è¡Œ

```python
bos_token_id: int = 1  # Beginning of Sequence
eos_token_id: int = 2  # End of Sequence
```

**éšå–»è§£é‡Š**:

å°±åƒä¹¦é¢è¯­è¨€çš„æ ‡ç‚¹ç¬¦å·:

```
bos_token_id = ã€ (å·¦ä¹¦åå·,è¡¨ç¤º"æ•…äº‹å¼€å§‹")
eos_token_id = ã€‘ (å³ä¹¦åå·,è¡¨ç¤º"æ•…äº‹ç»“æŸ")

ç¤ºä¾‹:
åŸå§‹æ–‡æœ¬: "å°çº¢å¸½å»æ£®æ—"
ç¼–ç å:   [1, 453, 234, 789, 123, 2]
           â†‘                      â†‘
         å¼€å§‹                    ç»“æŸ
```

**ä½œç”¨**:

1. **è®­ç»ƒæ—¶**: å‘Šè¯‰æ¨¡å‹"è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­"
2. **ç”Ÿæˆæ—¶**: 
   - é‡åˆ° `eos_token_id`,å°±åœæ­¢ç”Ÿæˆ
   - é¿å…æ¨¡å‹æ— ä¼‘æ­¢åœ°ç”Ÿæˆä¸‹å»

**ä¾‹å­**(ç”Ÿæˆæ–‡æœ¬æ—¶):

```python
# æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹
input_ids = [1]  # ä» <BOS> å¼€å§‹
while True:
    next_token = model.generate(input_ids)
    if next_token == 2:  # é‡åˆ° <EOS>
        break
    input_ids.append(next_token)

# æœ€ç»ˆç”Ÿæˆ: [1, 453, 234, ..., 789, 2]
# è§£ç : "<BOS> å°çº¢å¸½å»æ£®æ— <EOS>"
```

---

### ğŸ”¬ é«˜çº§å‚æ•°

#### 13. `rms_norm_eps: float = 1e-05` (å½’ä¸€åŒ–è®¡ç®—çš„"é˜²å´©æºƒä¿é™©")

**ä»£ç ä½ç½®**: ç¬¬ 43 è¡Œ

**ä½œç”¨**: RMSNorm è®¡ç®—æ—¶çš„æå°æ•°,é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚

**éšå–»è§£é‡Š**:

å°æ˜åœ¨è®¡ç®—å¹³å‡æˆç»©æ—¶:

```python
# RMSNorm çš„è®¡ç®—å…¬å¼(ç®€åŒ–ç‰ˆ)
rms = sqrt(mean(xÂ²) + eps)
output = x / rms
```

**å¦‚æœæ²¡æœ‰ eps ä¼šæ€æ ·?**

```
å‡è®¾è¾“å…¥å…¨æ˜¯ 0:
x = [0, 0, 0, 0]
mean(xÂ²) = 0
sqrt(0) = 0
x / 0 = ??? (è®¡ç®—æœºå´©æºƒ!)

åŠ ä¸Š eps å:
sqrt(0 + 0.00001) = 0.00316...
x / 0.00316 = 0 (å®‰å…¨!)
```

**ä¸ºä»€ä¹ˆæ˜¯ 1e-05?**

```
ä¸èƒ½å¤ªå¤§: 
- å¦‚æœ eps=0.1,ä¼šå½±å“æ­£å¸¸è®¡ç®—ç»“æœ

ä¸èƒ½å¤ªå°:
- å¦‚æœ eps=1e-20,åœ¨ FP16 ç²¾åº¦ä¸‹ä¼šè¢«èˆå…¥ä¸º 0

1e-05 æ˜¯ç»éªŒæœ€ä¼˜å€¼:
- è¶³å¤Ÿå°,ä¸å½±å“æ­£å¸¸æ•°å€¼
- è¶³å¤Ÿå¤§,èƒ½é˜²æ­¢é™¤é›¶
```

---

#### 14. `hidden_act: str = "silu"` (æ¿€æ´»å‡½æ•°ç±»å‹)

**ä»£ç ä½ç½®**: ç¬¬ 35 è¡Œ

**ä½œç”¨**: æŒ‡å®šå‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚

**éšå–»è§£é‡Š**:

æ¿€æ´»å‡½æ•°å†³å®šäº†ç¥ç»å…ƒçš„"å…´å¥‹æ¨¡å¼"ã€‚

**å¸¸è§æ¿€æ´»å‡½æ•°å¯¹æ¯”**:

```python
# ReLU (è€å¼æ¿€æ´»å‡½æ•°)
def relu(x):
    return max(0, x)  # è´Ÿæ•°å…¨éƒ¨å˜0,æ­£æ•°ä¿æŒä¸å˜

è¾“å…¥: [-2, -1, 0, 1, 2]
è¾“å‡º: [0, 0, 0, 1, 2]  â† è´Ÿæ•°ä¿¡æ¯å®Œå…¨ä¸¢å¤±!

# SiLU (Swish,ç°ä»£æ¿€æ´»å‡½æ•°)
def silu(x):
    return x * sigmoid(x)  # å¹³æ»‘æ›²çº¿,ä¿ç•™éƒ¨åˆ†è´Ÿæ•°ä¿¡æ¯

è¾“å…¥: [-2, -1, 0, 1, 2]
è¾“å‡º: [-0.24, -0.27, 0, 0.73, 1.76]  â† æ›´å¹³æ»‘,æ¢¯åº¦æ›´å¥½
```

**ç±»æ¯”**:

```
ReLU = ä¸¥æ ¼çš„è€å¸ˆ:
- å­¦ç”Ÿ(ç¥ç»å…ƒ)è¡¨ç°å¥½(x>0) â†’ ç»™äºˆé¼“åŠ±
- å­¦ç”Ÿè¡¨ç°ä¸å¥½(x<0) â†’ ç›´æ¥æ‰“ 0 åˆ†(ä¿¡æ¯ä¸¢å¤±)

SiLU = æ¸©å’Œçš„è€å¸ˆ:
- å­¦ç”Ÿè¡¨ç°å¥½ â†’ å¤§åŠ›é¼“åŠ±
- å­¦ç”Ÿè¡¨ç°ä¸å¥½ â†’ ä¹Ÿç»™äºˆä¸€äº›åé¦ˆ(ä¿ç•™ä¿¡æ¯)
```

**ä¸ºä»€ä¹ˆ Llama é€‰æ‹© SiLU?**

1. **æ¢¯åº¦æµåŠ¨æ›´å¥½**: è®­ç»ƒæ—¶ä¸ä¼šå‡ºç°"æ¢¯åº¦æ¶ˆå¤±"
2. **æ€§èƒ½æ›´ä¼˜**: å®éªŒè¡¨æ˜æ¯” ReLU æå‡ 1-2% å‡†ç¡®ç‡
3. **è®¡ç®—æˆæœ¬**: æ¯” ReLU ç•¥æ…¢,ä½†å¯æ¥å—

---

### ğŸ“ æ··åˆä¸“å®¶(MoE)å‚æ•°

#### 15. `use_moe: bool = False` (æ˜¯å¦å¯ç”¨"ä¸“å®¶å›¢é˜Ÿ"æ¨¡å¼)

**ä»£ç ä½ç½®**: ç¬¬ 51 è¡Œ

**ä½œç”¨**: æ˜¯å¦ä½¿ç”¨æ··åˆä¸“å®¶(Mixture of Experts)æ¶æ„ã€‚

**éšå–»è§£é‡Š**:

**æ™®é€šæ¨¡å¼** (`use_moe=False`):

```
å°æ˜ä¸€ä¸ªäººå¤„ç†æ‰€æœ‰ä»»åŠ¡:
- å†™ç§‘å¹»å°è¯´ â†’ å°æ˜å†™
- å†™çˆ±æƒ…æ•…äº‹ â†’ å°æ˜å†™  
- å†™å†å²è®ºæ–‡ â†’ å°æ˜å†™
ç»“æœ: æ¯ç§ç±»å‹éƒ½åªèƒ½åšåˆ° 60 åˆ†(ä»€ä¹ˆéƒ½ä¼š,ä½†éƒ½ä¸ç²¾)
```

**MoE æ¨¡å¼** (`use_moe=True`):

```
ç»„å»ºä¸€ä¸ªå†™ä½œå›¢é˜Ÿ:
- ä¸“å®¶A: æ“…é•¿ç§‘å¹»(è´Ÿè´£ç§‘å¹»ç±»)
- ä¸“å®¶B: æ“…é•¿çˆ±æƒ…(è´Ÿè´£çˆ±æƒ…ç±»)
- ä¸“å®¶C: æ“…é•¿å†å²(è´Ÿè´£å†å²ç±»)
- ä¸“å®¶D: å…¨èƒ½å‹(ä½œä¸ºå¤‡é€‰)

é‡åˆ°ä»»åŠ¡æ—¶:
1. è·¯ç”±å™¨(Gate)åˆ¤æ–­ä»»åŠ¡ç±»å‹
2. é€‰æ‹©æœ€åˆé€‚çš„ 2 ä¸ªä¸“å®¶å¤„ç†
3. ç»¼åˆä»–ä»¬çš„è¾“å‡º

ç»“æœ: æ¯ç§ç±»å‹éƒ½èƒ½åšåˆ° 90 åˆ†(æœ¯ä¸šæœ‰ä¸“æ”»)
```

**å®é™…åº”ç”¨**(åœ¨ MiniMindBlock ä¸­,ç¬¬ 1019 è¡Œ):

```python
self.mlp = (
    FeedForward(config) if not config.use_moe 
    else MOEFeedForward(config)
)
```

**ä¼˜ç¼ºç‚¹**:

| æ–¹é¢ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|-----|------|
| **å‚æ•°æ•ˆç‡** | æ€»å‚æ•°å¤š,ä½†æ¯æ¬¡åªæ¿€æ´»ä¸€éƒ¨åˆ† | å­˜å‚¨ç©ºé—´éœ€æ±‚å¤§ |
| **æ€§èƒ½** | åŒç­‰æ¿€æ´»å‚æ•°ä¸‹,æ•ˆæœæ›´å¥½ | è®­ç»ƒæ›´å¤æ‚ |
| **é€‚ç”¨åœºæ™¯** | å¤šé¢†åŸŸã€å¤šè¯­è¨€ä»»åŠ¡ | å•ä¸€é¢†åŸŸä»»åŠ¡æå‡æœ‰é™ |

---

#### 16-17. `n_routed_experts` å’Œ `num_experts_per_tok` (ä¸“å®¶æ•°é‡é…ç½®)

**ä»£ç ä½ç½®**: ç¬¬ 52-53 è¡Œ

```python
n_routed_experts: int = 4      # æ€»å…±æœ‰ 4 ä¸ªä¸“å®¶
num_experts_per_tok: int = 2   # æ¯ä¸ªè¯é€‰æ‹© 2 ä¸ªä¸“å®¶å¤„ç†
```

**éšå–»è§£é‡Š**:

```
å†™ä½œå›¢é˜Ÿé…ç½®:
- æ‹›è˜äº† 4 ä¸ªä¸“ä¸šä½œå®¶(n_routed_experts=4)
- æ¯ä¸ªå†™ä½œä»»åŠ¡åˆ†é…ç»™ 2 ä¸ªä½œå®¶åŒæ—¶å¤„ç†(num_experts_per_tok=2)

ç¤ºä¾‹:
è¾“å…¥è¯: "é‡å­"
â†“
è·¯ç”±å™¨è¯„åˆ†:
- ä¸“å®¶A(ç§‘å¹»): 0.9 åˆ† âœ“ é€‰ä¸­
- ä¸“å®¶B(çˆ±æƒ…): 0.1 åˆ†
- ä¸“å®¶C(å†å²): 0.3 åˆ†
- ä¸“å®¶D(é€šç”¨): 0.6 åˆ† âœ“ é€‰ä¸­
â†“
è¾“å‡º = 0.6 * ä¸“å®¶Açš„ç»“æœ + 0.4 * ä¸“å®¶Dçš„ç»“æœ
       (æƒé‡å½’ä¸€åŒ–: 0.9/(0.9+0.6)=0.6, 0.6/(0.9+0.6)=0.4)
```

**é…ç½®ç­–ç•¥**:

```python
# æ¿€è¿›é…ç½®(è¿½æ±‚æ€§èƒ½)
n_routed_experts=16    # 16 ä¸ªä¸“å®¶
num_experts_per_tok=4  # æ¯æ¬¡é€‰ 4 ä¸ª
â†’ è¡¨è¾¾èƒ½åŠ›å¼º,ä½†è®¡ç®—æˆæœ¬é«˜

# ä¿å®ˆé…ç½®(è¿½æ±‚æ•ˆç‡)
n_routed_experts=4     # 4 ä¸ªä¸“å®¶
num_experts_per_tok=1  # æ¯æ¬¡é€‰ 1 ä¸ª
â†’ è®¡ç®—å¿«,ä½†æ•ˆæœå¯èƒ½ä¸å¦‚æ™®é€šFFN

# å¹³è¡¡é…ç½®(æ¨è)
n_routed_experts=8     # 8 ä¸ªä¸“å®¶
num_experts_per_tok=2  # æ¯æ¬¡é€‰ 2 ä¸ª
â†’ åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡
```

---

#### 18. `n_shared_experts: int = 1` (é€šç”¨ä¸“å®¶æ•°é‡)

**ä»£ç ä½ç½®**: ç¬¬ 54 è¡Œ

**ä½œç”¨**: æ— è®ºä»€ä¹ˆä»»åŠ¡,éƒ½ä¼šè¢«æ¿€æ´»çš„"é€šç”¨ä¸“å®¶"æ•°é‡ã€‚

**éšå–»è§£é‡Š**:

```
å†™ä½œå›¢é˜Ÿä¸­çš„"ä¸‡é‡‘æ²¹"æˆå‘˜:

è·¯ç”±ä¸“å®¶(routed_experts):
- ä¸“å®¶A: åªå¤„ç†è¢«åˆ†é…çš„ç§‘å¹»ä»»åŠ¡
- ä¸“å®¶B: åªå¤„ç†è¢«åˆ†é…çš„çˆ±æƒ…ä»»åŠ¡

å…±äº«ä¸“å®¶(shared_experts):
- ä¸“å®¶S: æ— è®ºä»€ä¹ˆä»»åŠ¡,éƒ½ä¼šå‚ä¸
  - è´Ÿè´£é€šç”¨æŠ€èƒ½:è¯­æ³•æ£€æŸ¥ã€é€»è¾‘è¿è´¯æ€§ã€æ–‡å­—æ¶¦è‰²

æœ€ç»ˆè¾“å‡º = è·¯ç”±ä¸“å®¶çš„ç»“æœ + å…±äº«ä¸“å®¶çš„ç»“æœ
```

**ä»£ç å®ç°**(åœ¨ MOEFeedForward.forward ä¸­,ç¬¬ 919-923 è¡Œ):

```python
y = self.moe_infer(x, ...)  # è·¯ç”±ä¸“å®¶çš„è¾“å‡º

if self.config.n_shared_experts > 0:
    for expert in self.shared_experts:
        y = y + expert(identity)  # å åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º
```

**ä¸ºä»€ä¹ˆéœ€è¦å…±äº«ä¸“å®¶?**

1. **ç¨³å®šæ€§**: é˜²æ­¢æŸäº›ä¸“å®¶å®Œå…¨ä¸è¢«æ¿€æ´»
2. **é€šç”¨çŸ¥è¯†**: æ•æ‰è·¨é¢†åŸŸçš„å…±æ€§ç‰¹å¾
3. **æ€§èƒ½æå‡**: DeepSeek-MoE è®ºæ–‡è¯æ˜,åŠ å…¥å…±äº«ä¸“å®¶èƒ½æå‡ 2-3%

---

#### 19-21. è¾…åŠ©æŸå¤±å‚æ•°

**ä»£ç ä½ç½®**: ç¬¬ 56-58 è¡Œ

```python
aux_loss_alpha: float = 0.01   # è¾…åŠ©æŸå¤±çš„æƒé‡
seq_aux: bool = True           # æ˜¯å¦ä½¿ç”¨åºåˆ—çº§è¾…åŠ©æŸå¤±
norm_topk_prob: bool = True    # æ˜¯å¦å½’ä¸€åŒ– Top-K æ¦‚ç‡
```

**éšå–»è§£é‡Š**:

**é—®é¢˜åœºæ™¯**:

```
å†™ä½œå›¢é˜Ÿçš„"å·æ‡’"é—®é¢˜:
- ä¸“å®¶A å¤ªå—æ¬¢è¿,æ¯å¤©å¤„ç† 1000 ä¸ªä»»åŠ¡(è¿‡è½½!)
- ä¸“å®¶B æ²¡äººæ‰¾,æ¯å¤©åªå¤„ç† 10 ä¸ªä»»åŠ¡(æ‘¸é±¼!)

ç»“æœ:
- ä¸“å®¶A ç´¯åäº†,è´¨é‡ä¸‹é™
- ä¸“å®¶B æŠ€èƒ½é€€åŒ–
- å›¢é˜Ÿæ•´ä½“æ•ˆç‡ä½ä¸‹
```

**è§£å†³æ–¹æ¡ˆ**: **è¾…åŠ©æŸå¤±**(Auxiliary Loss)

```python
# åœ¨è®­ç»ƒæ—¶é¢å¤–æ·»åŠ ä¸€ä¸ª"è´Ÿè½½å‡è¡¡æŸå¤±"
total_loss = ä¸»ä»»åŠ¡æŸå¤± + aux_loss_alpha * è´Ÿè½½å‡è¡¡æŸå¤±

è´Ÿè½½å‡è¡¡æŸå¤± = é¼“åŠ±æ¯ä¸ªä¸“å®¶çš„å·¥ä½œé‡æ¥è¿‘å¹³å‡å€¼
```

**å‚æ•°è¯¦è§£**:

1. **`aux_loss_alpha=0.01`**: 
   ```
   æ§åˆ¶"è´Ÿè½½å‡è¡¡"çš„é‡è¦æ€§
   - å¤ªå°(å¦‚ 0.001): ä¸“å®¶ä»ç„¶ä¼šå·æ‡’
   - å¤ªå¤§(å¦‚ 0.1): ä¸ºäº†å‡è¡¡è´Ÿè½½,ç‰ºç‰²äº†ä»»åŠ¡æ•ˆæœ
   - 0.01 æ˜¯ç»éªŒæœ€ä¼˜å€¼
   ```

2. **`seq_aux=True`**:
   ```
   True:  åœ¨æ¯ä¸ªå¥å­å†…éƒ¨ä¿è¯è´Ÿè½½å‡è¡¡
   False: åœ¨æ•´ä¸ªbatchä¸­ä¿è¯è´Ÿè½½å‡è¡¡
   
   ä¾‹å­:
   å¥å­A: "è®¨è®ºé‡å­åŠ›å­¦" â†’ åº”è¯¥å¤šç”¨ç§‘å­¦ä¸“å®¶
   å¥å­B: "æè¿°çˆ±æƒ…æ•…äº‹" â†’ åº”è¯¥å¤šç”¨æ–‡å­¦ä¸“å®¶
   
   seq_aux=True æ—¶,ä¼šåˆ†åˆ«ç»Ÿè®¡å¥å­Aå’Œå¥å­Bçš„ä¸“å®¶åˆ†å¸ƒ
   ```

3. **`norm_topk_prob=True`**:
   ```python
   # è·¯ç”±å™¨è¾“å‡ºçš„åŸå§‹åˆ†æ•°
   scores = [0.5, 0.3, 0.15, 0.05]  # 4ä¸ªä¸“å®¶
   top2 = [0.5, 0.3]  # é€‰æ‹©å‰2ä¸ª
   
   norm_topk_prob=False: ç›´æ¥ä½¿ç”¨ [0.5, 0.3]
   norm_topk_prob=True:  å½’ä¸€åŒ–ä¸º [0.625, 0.375]
                         (0.5/0.8=0.625, 0.3/0.8=0.375)
   
   å¥½å¤„: å½’ä¸€åŒ–åçš„æƒé‡å’Œä¸º1,æ•°å€¼æ›´ç¨³å®š
   ```

---

### ğŸ“Š é…ç½®æ€»ç»“è¡¨

| å‚æ•°ç±»åˆ« | å‚æ•°å | é»˜è®¤å€¼ | ä½œç”¨ | ç±»æ¯” |
|---------|--------|--------|------|------|
| **æ ¸å¿ƒç»“æ„** | `hidden_size` | 512 | æ¨¡å‹ç»´åº¦ | å¤§è„‘ç¥ç»å…ƒæ•°é‡ |
| | `num_hidden_layers` | 8 | Transformerå±‚æ•° | æ€è€ƒæ·±åº¦ |
| | `num_attention_heads` | 8 | æ³¨æ„åŠ›å¤´æ•° | åŒæ—¶å…³æ³¨çš„ç„¦ç‚¹æ•° |
| | `num_key_value_heads` | 2 | KVå¤´æ•°(GQA) | å…±äº«è®°å¿†ç´¢å¼•æ•° |
| | `vocab_size` | 6400 | è¯è¡¨å¤§å° | è¯æ±‡é‡ |
| **æ€§èƒ½ä¼˜åŒ–** | `intermediate_size` | None (auto) | FFNä¸­é—´å±‚ç»´åº¦ | è”æƒ³æ‰©å±•å€æ•° |
| | `dropout` | 0.0 | Dropoutæ¯”ä¾‹ | éšæœºé—å¿˜ç‡ |
| | `flash_attn` | True | æ˜¯å¦ç”¨Flash Attn | æ˜¯å¦ç”¨å¿«é€Ÿç®—æ³• |
| **ä½ç½®ç¼–ç ** | `max_position_embeddings` | 32768 | æœ€å¤§åºåˆ—é•¿åº¦ | çŸ­æœŸè®°å¿†å®¹é‡ |
| | `rope_theta` | 1000000 | RoPEåŸºé¢‘ | ä½ç½®æ—¶é’Ÿçš„è½¬é€Ÿ |
| **ç‰¹æ®ŠToken** | `bos_token_id` | 1 | å¥å­å¼€å§‹ç¬¦ | ã€ |
| | `eos_token_id` | 2 | å¥å­ç»“æŸç¬¦ | ã€‘ |
| **MoEæ¶æ„** | `use_moe` | False | æ˜¯å¦å¯ç”¨MoE | æ˜¯å¦ç”¨ä¸“å®¶å›¢é˜Ÿ |
| | `n_routed_experts` | 4 | è·¯ç”±ä¸“å®¶æ•° | ä¸“ä¸šä½œå®¶æ•°é‡ |
| | `num_experts_per_tok` | 2 | æ¯è¯é€‰æ‹©ä¸“å®¶æ•° | æ¯ä»»åŠ¡åˆ†é…äººæ•° |
| | `n_shared_experts` | 1 | å…±äº«ä¸“å®¶æ•° | é€šç”¨åŠ©æ‰‹æ•°é‡ |
| | `aux_loss_alpha` | 0.01 | è¾…åŠ©æŸå¤±æƒé‡ | è´Ÿè½½å‡è¡¡é‡è¦æ€§ |

---

### ğŸ’¡ é…ç½®å®æˆ˜å»ºè®®

#### åœºæ™¯1: è®­ç»ƒä¸­æ–‡å¯¹è¯å°æ¨¡å‹

```python
config = MiniMindConfig(
    hidden_size=512,          # å°æ¨¡å‹,é™ä½è®¡ç®—æˆæœ¬
    num_hidden_layers=8,      # 8å±‚è¶³å¤Ÿç®€å•å¯¹è¯
    num_attention_heads=8,
    num_key_value_heads=2,    # ä½¿ç”¨GQAèŠ‚çœæ˜¾å­˜
    vocab_size=6400,          # ä¸­æ–‡å¸¸ç”¨å­—
    max_position_embeddings=2048,  # çŸ­å¯¹è¯åœºæ™¯
    dropout=0.0,              # å¤§è§„æ¨¡æ•°æ®ä¸éœ€è¦dropout
    flash_attn=True,          # å¿…å¼€,æé€Ÿæ˜æ˜¾
    use_moe=False,            # å•ä¸€ä»»åŠ¡ä¸éœ€è¦MoE
)
```

#### åœºæ™¯2: è®­ç»ƒå¤šè¯­è¨€å¤§æ¨¡å‹

```python
config = MiniMindConfig(
    hidden_size=2048,         # å¤§æ¨¡å‹
    num_hidden_layers=24,     # æ·±å±‚ç½‘ç»œ
    num_attention_heads=32,
    num_key_value_heads=8,    
    vocab_size=64000,         # å¤šè¯­è¨€è¯è¡¨
    max_position_embeddings=8192,  # æ”¯æŒé•¿æ–‡æ¡£
    flash_attn=True,
    use_moe=True,             # å¯ç”¨MoEå¤„ç†å¤šè¯­è¨€
    n_routed_experts=16,      # 16ä¸ªè¯­è¨€ä¸“å®¶
    num_experts_per_tok=4,    # æ¯è¯é€‰4ä¸ªä¸“å®¶
    n_shared_experts=2,       # 2ä¸ªé€šç”¨ä¸“å®¶
    aux_loss_alpha=0.01,
)
```

#### åœºæ™¯3: æ¨ç†ä¼˜åŒ–é…ç½®(é•¿æ–‡æœ¬ç”Ÿæˆ)

```python
config = MiniMindConfig(
    hidden_size=1024,
    num_hidden_layers=12,
    num_attention_heads=16,
    num_key_value_heads=2,    # GQAå¤§å¹…å‡å°‘KV Cache
    max_position_embeddings=32768,  # æ”¯æŒé•¿æ–‡æœ¬
    rope_theta=1000000.0,     # é•¿ä¸Šä¸‹æ–‡ä¸“ç”¨
    inference_rope_scaling=True,  # å¯ç”¨YaRNå¤–æ¨
    flash_attn=True,          # æ¨ç†æ—¶ä¹Ÿèƒ½åŠ é€Ÿ
)
```
---

## ä¸‰ã€å­¦ä¹ è¿›åº¦çš„å­˜æ¡£ä¸è¯»æ¡£ï¼šæ¨¡å‹æ£€æŸ¥ç‚¹ (lm_checkpoint)

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 124-246 è¡Œ)

```python
def lm_checkpoint(
    lm_config,
    weight="full_sft",
    model=None,
    optimizer=None,
    epoch=0,
    step=0,
    wandb=None,
    save_dir="../checkpoints",
    **kwargs,
):
    """
    æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ä¸åŠ è½½å‡½æ•°ã€‚

    Args:
        lm_config: æ¨¡å‹é…ç½®å¯¹è±¡,ç”¨äºè·å– hidden_size å’Œæ˜¯å¦ä½¿ç”¨ MoEã€‚
        weight: æƒé‡æ–‡ä»¶åæ ‡è¯†,é»˜è®¤ä¸º 'full_sft'ã€‚
        model: æ¨¡å‹å®ä¾‹ã€‚å¦‚æœä¸ä¸º None,åˆ™æ‰§è¡Œä¿å­˜æ“ä½œ;ä¸º None åˆ™æ‰§è¡ŒåŠ è½½æ“ä½œã€‚
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹,ä»…åœ¨ä¿å­˜æ—¶éœ€è¦ã€‚
        epoch: å½“å‰è®­ç»ƒè½®æ•°ã€‚
        step: å½“å‰è®­ç»ƒæ­¥æ•°ã€‚
        wandb: Weights & Biases å®ä¾‹,ç”¨äºè®°å½• run id ä»¥ä¾¿æ–­ç‚¹ç»­ä¼ ã€‚
        save_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•ã€‚
        **kwargs: å…¶ä»–éœ€è¦ä¿å­˜çš„å¯¹è±¡(å¦‚ scheduler),å¦‚æœå¯¹è±¡æœ‰ state_dict æ–¹æ³•ä¼šè‡ªåŠ¨è°ƒç”¨ã€‚

    Returns:
        åŠ è½½æ¨¡å¼ä¸‹è¿”å›åŒ…å«æ¢å¤ä¿¡æ¯çš„å­—å…¸ (dict),å¦åˆ™è¿”å› Noneã€‚
    """

    # 1. å‡†å¤‡ä¿å­˜ç›®å½•å’Œè·¯å¾„
    os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    moe_path = "_moe" if lm_config.use_moe else ""  # MoE æ¨¡å‹æ·»åŠ ç‰¹æ®Šåç¼€
    # çº¯æƒé‡æ–‡ä»¶è·¯å¾„ (ä»…åŒ…å« model state_dict,ä½“ç§¯å°,ç”¨äºæ¨ç†)
    ckp_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.pth"
    # æ¢å¤æ–‡ä»¶è·¯å¾„ (åŒ…å« model, optimizer, step, epoch ç­‰,ç”¨äºæ–­ç‚¹ç»­è®­)
    resume_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_resume.pth"

    if model is not None:
        # ==================== ä¿å­˜æ¨¡å¼ ====================

        # 2. è§£åŒ…æ¨¡å‹ (Unwrap)
        # å¦‚æœæ˜¯ DDP (åˆ†å¸ƒå¼) æ¨¡å‹,å–å…¶ .module
        raw_model = (
            model.module if isinstance(model, DistributedDataParallel) else model
        )
        # å¦‚æœæ˜¯ torch.compile ç¼–è¯‘åçš„æ¨¡å‹,å–å…¶ _orig_mod
        raw_model = getattr(raw_model, "_orig_mod", raw_model)

        # 3. å¤„ç†æ¨¡å‹æƒé‡ (Save Model Weights)
        state_dict = raw_model.state_dict()
        # å°†æƒé‡è½¬ä¸ºåŠç²¾åº¦ (FP16) å¹¶ç§»åŠ¨åˆ° CPU,èŠ‚çœå­˜å‚¨ç©ºé—´å’Œæ˜¾å­˜
        state_dict = {k: v.half().cpu() for k, v in state_dict.items()}

        # 4. åŸå­ä¿å­˜æƒé‡æ–‡ä»¶ (Atomic Save)
        ckp_tmp = ckp_path + ".tmp"
        torch.save(state_dict, ckp_tmp)  # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
        os.replace(ckp_tmp, ckp_path)  # åŸå­æ›¿æ¢,é˜²æ­¢å†™å…¥ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå

        # 5. è·å– WandB Run ID (ç”¨äºæ¢å¤æ›²çº¿)
        wandb_id = None
        if wandb:
            if hasattr(wandb, "get_run"):
                run = wandb.get_run()
                wandb_id = getattr(run, "id", None) if run else None
            else:
                wandb_id = getattr(wandb, "id", None)

        # 6. æ„å»ºæ¢å¤æ•°æ®å­—å…¸ (Resume Data)
        resume_data = {
            "model": state_dict,  # æ¨¡å‹æƒé‡
            "optimizer": optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
            "epoch": epoch,  # å½“å‰ Epoch
            "step": step,  # å½“å‰ Step
            "world_size": dist.get_world_size()
            if dist.is_initialized()
            else 1,  # ä¿å­˜æ—¶çš„ GPU æ•°é‡
            "wandb_id": wandb_id,  # WandB ID
        }

        # 7. å¤„ç†é¢å¤–çš„ kwargs (å¦‚ LR Scheduler)
        for key, value in kwargs.items():
            if value is not None:
                if hasattr(value, "state_dict"):
                    # å¦‚æœæ˜¯ DDP æˆ–ç¼–è¯‘åçš„å¯¹è±¡,åŒæ ·éœ€è¦è§£åŒ…
                    raw_value = (
                        value.module
                        if isinstance(value, DistributedDataParallel)
                        else value
                    )
                    raw_value = getattr(raw_value, "_orig_mod", raw_value)
                    resume_data[key] = raw_value.state_dict()
                else:
                    resume_data[key] = value

        # 8. åŸå­ä¿å­˜æ¢å¤æ–‡ä»¶
        resume_tmp = resume_path + ".tmp"
        torch.save(resume_data, resume_tmp)
        os.replace(resume_tmp, resume_path)

        # 9. æ¸…ç†èµ„æº
        del state_dict, resume_data
        torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

    else:
        # ==================== åŠ è½½æ¨¡å¼ ====================

        if os.path.exists(resume_path):
            # 1. åŠ è½½æ¢å¤æ–‡ä»¶åˆ° CPU
            ckp_data = torch.load(resume_path, map_location="cpu")

            # 2. å¤„ç† GPU æ•°é‡å˜åŒ–å¸¦æ¥çš„ Step å·®å¼‚
            # åœºæ™¯:ä¾‹å¦‚ 4 å¡å˜ 8 å¡,Global Batch Size ç¿»å€,æ€» Step æ•°åº”å‡åŠ
            saved_ws = ckp_data.get("world_size", 1)
            current_ws = dist.get_world_size() if dist.is_initialized() else 1

            if saved_ws != current_ws:
                ckp_data["step"] = ckp_data["step"] * saved_ws // current_ws
                print(
                    f"GPUæ•°é‡å˜åŒ–({saved_ws}â†’{current_ws}),stepå·²è‡ªåŠ¨è½¬æ¢ä¸º{ckp_data['step']}"
                )

            return ckp_data
        return None
```

---

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šå°æ˜å­¦ä¹ çš„"å­˜æ¡£"ä¸"è¯»æ¡£"ç³»ç»Ÿ

æƒ³è±¡å°æ˜åœ¨å­¦ä¹ å†™å°è¯´çš„è¿‡ç¨‹å°±åƒç©ä¸€æ¬¾ RPG æ¸¸æˆ,è€Œ `lm_checkpoint` å‡½æ•°å°±æ˜¯æ¸¸æˆçš„**å­˜æ¡£/è¯»æ¡£ç³»ç»Ÿ**ã€‚

---

### ğŸ® æ ¸å¿ƒæ¦‚å¿µï¼šä¸ºä»€ä¹ˆéœ€è¦æ£€æŸ¥ç‚¹?

#### é—®é¢˜åœºæ™¯

```
å°æ˜è®­ç»ƒäº† 7 å¤© (è®­ç»ƒæ¨¡å‹):
- ç¬¬ 1 å¤©: å­¦ä¼šäº†åŸºç¡€è¯­æ³•
- ç¬¬ 2 å¤©: å­¦ä¼šäº†æå†™äººç‰©
- ...
- ç¬¬ 7 å¤©: æ­£åœ¨å­¦ä¹ å¤æ‚çš„æƒ…èŠ‚æ„å»º

çªç„¶åœç”µäº†! (è®­ç»ƒä¸­æ–­)
- å¦‚æœæ²¡æœ‰å­˜æ¡£: 7 å¤©çš„å­¦ä¹ æˆæœå…¨éƒ¨ä¸¢å¤±,å¿…é¡»ä»å¤´å¼€å§‹!
- å¦‚æœæœ‰å­˜æ¡£: å¯ä»¥ä»ç¬¬ 6 å¤©çš„çŠ¶æ€ç»§ç»­,åªæŸå¤± 1 å¤©çš„è¿›åº¦
```

**ç°å®ä¸­çš„è®­ç»ƒä¸­æ–­åŸå› **:
- ğŸ”Œ åœç”µ/æœåŠ¡å™¨é‡å¯
- ğŸ’¥ ç¨‹åºå´©æºƒ(OOMã€CUDAé”™è¯¯)
- â° é›†ç¾¤ä»»åŠ¡æ—¶é—´åˆ°æœŸ(å¾ˆå¤šå…¬å¸GPUèµ„æºæœ‰æ—¶é—´é™åˆ¶)
- ğŸ”§ éœ€è¦è°ƒæ•´è¶…å‚æ•°
- ğŸ“Š å®šæœŸä¿å­˜æœ€ä½³æ¨¡å‹

---

### ğŸ“¦ ä¸¤ç§å­˜æ¡£æ–‡ä»¶ï¼šè½»é‡æ¨ç† vs å®Œæ•´æ¢å¤

ä»£ç ä¸­ç”Ÿæˆäº†**ä¸¤ä¸ªæ–‡ä»¶**,å°±åƒæ¸¸æˆçš„"å¿«é€Ÿå­˜æ¡£"å’Œ"å®Œæ•´å­˜æ¡£":

```python
# ç¬¬ 157-159 è¡Œ
ckp_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.pth"
# ä¾‹å¦‚: checkpoints/full_sft_512.pth

resume_path = f"{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_resume.pth"
# ä¾‹å¦‚: checkpoints/full_sft_512_resume.pth
```

#### ğŸ“„ æ–‡ä»¶ 1: çº¯æƒé‡æ–‡ä»¶ (ckp_path)

**ç±»æ¯”**: å°æ˜çš„"æŠ€èƒ½å¡ç‰‡"

```
åªåŒ…å«:
âœ“ å°æ˜çš„å†™ä½œèƒ½åŠ› (æ¨¡å‹å‚æ•°)

ä¸åŒ…å«:
âœ— å°æ˜çš„å­¦ä¹ è¿›åº¦ (epoch/step)
âœ— å°æ˜çš„"å­¦ä¹ ç¬”è®°" (optimizerçŠ¶æ€)
âœ— è®­ç»ƒæ—¥å¿—çš„ID (wandb_id)

ç”¨é€”:
â†’ ç”¨äºæ¨ç†/éƒ¨ç½² (åªéœ€è¦"ä¼šå†™ä½œ",ä¸éœ€è¦"ç»§ç»­å­¦ä¹ ")
â†’ åˆ†äº«ç»™åˆ«äººä½¿ç”¨
â†’ æ–‡ä»¶è¾ƒå° (çº¦ 500MB)
```

**ä»£ç å®ç°** (ç¬¬ 173-180 è¡Œ):

```python
state_dict = raw_model.state_dict()  # è·å–æ‰€æœ‰å‚æ•°
state_dict = {k: v.half().cpu() for k, v in state_dict.items()}  
# â†‘ è½¬ä¸º FP16 (åŸæœ¬æ˜¯ FP32),å‡å° 50% ä½“ç§¯

ckp_tmp = ckp_path + ".tmp"
torch.save(state_dict, ckp_tmp)  # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
os.replace(ckp_tmp, ckp_path)    # åŸå­æ›¿æ¢ (é˜²æ­¢ä¿å­˜åˆ°ä¸€åŠæ–­ç”µ)
```

**ä¸ºä»€ä¹ˆç”¨ `.tmp` ä¸´æ—¶æ–‡ä»¶?**

```
é”™è¯¯åšæ³•:
torch.save(state_dict, "model.pth")  # ç›´æ¥ä¿å­˜
â†’ å¦‚æœä¿å­˜åˆ°ä¸€åŠæ–­ç”µ,model.pth æ–‡ä»¶æŸå,æ— æ³•ä½¿ç”¨!

æ­£ç¡®åšæ³•:
torch.save(state_dict, "model.pth.tmp")  # å…ˆå†™ä¸´æ—¶æ–‡ä»¶
os.replace("model.pth.tmp", "model.pth")  # åŸå­æ“ä½œ (è¦ä¹ˆå…¨æˆåŠŸ,è¦ä¹ˆå…¨å¤±è´¥)
â†’ å³ä½¿æ–­ç”µ,ä¹Ÿåªä¼šæŸå¤±ä¸´æ—¶æ–‡ä»¶,åŸæ–‡ä»¶ä¾ç„¶å®Œå¥½
```

**ç±»æ¯”**: å°±åƒç¼–è¾‘æ–‡æ¡£æ—¶,è½¯ä»¶ä¼šå…ˆä¿å­˜åˆ° `æ–‡æ¡£.docx.tmp`,ç¡®è®¤å†™å…¥æˆåŠŸåå†æ›¿æ¢åŸæ–‡ä»¶ã€‚

---

#### ğŸ“š æ–‡ä»¶ 2: å®Œæ•´æ¢å¤æ–‡ä»¶ (resume_path)

**ç±»æ¯”**: å°æ˜çš„"å®Œæ•´å­¦ä¹ æ¡£æ¡ˆ"

```
åŒ…å«æ‰€æœ‰è®­ç»ƒçŠ¶æ€:
âœ“ å°æ˜çš„å†™ä½œèƒ½åŠ› (model state_dict)
âœ“ å°æ˜çš„å­¦ä¹ è¿›åº¦ (epoch: 7, step: 15000)
âœ“ å°æ˜çš„"å­¦ä¹ ç¬”è®°" (optimizer state_dict)
  â†‘ è®°å½•äº†å“ªäº›çŸ¥è¯†ç‚¹éœ€è¦é‡ç‚¹å¤ä¹  (æ¢¯åº¦åŠ¨é‡ã€å­¦ä¹ ç‡ç­‰)
âœ“ è®­ç»ƒæ—¥å¿—çš„ID (wandb_id)
âœ“ å½“æ—¶ç”¨äº†å‡ ä¸ªGPU (world_size: 4)
âœ“ å…¶ä»–è®­ç»ƒç»„ä»¶ (å¦‚å­¦ä¹ ç‡è°ƒåº¦å™¨ scheduler)

ç”¨é€”:
â†’ æ–­ç‚¹ç»­è®­ (ä»ä¸­æ–­çš„åœ°æ–¹ç»§ç»­è®­ç»ƒ)
â†’ æ–‡ä»¶è¾ƒå¤§ (çº¦ 1GB,å› ä¸ºåŒ…å« optimizer çŠ¶æ€)
```

**ä»£ç å®ç°** (ç¬¬ 192-221 è¡Œ):

```python
resume_data = {
    "model": state_dict,                    # æ¨¡å‹æƒé‡
    "optimizer": optimizer.state_dict(),    # ä¼˜åŒ–å™¨çŠ¶æ€
    "epoch": epoch,                         # å½“å‰ç¬¬å‡ è½®
    "step": step,                           # å½“å‰ç¬¬å‡ æ­¥
    "world_size": dist.get_world_size() if dist.is_initialized() else 1,
    "wandb_id": wandb_id,                   # è®­ç»ƒæ›²çº¿çš„ID
}

# å¤„ç†é¢å¤–çš„è®­ç»ƒç»„ä»¶ (å¦‚ LR Scheduler)
for key, value in kwargs.items():
    if hasattr(value, "state_dict"):
        resume_data[key] = value.state_dict()  # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€
```

---

### ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

#### 1. æ¨¡å‹"è§£åŒ…" (ç¬¬ 164-170 è¡Œ)

```python
# é—®é¢˜: è®­ç»ƒæ—¶çš„æ¨¡å‹å¯èƒ½è¢«"åŒ…è£…"äº†å¤šå±‚
raw_model = model.module if isinstance(model, DistributedDataParallel) else model
raw_model = getattr(raw_model, "_orig_mod", raw_model)
```

**éšå–»è§£é‡Š**:

```
è®­ç»ƒæ—¶çš„æ¨¡å‹å°±åƒ"ä¿„ç½—æ–¯å¥—å¨ƒ":

å¤–å±‚åŒ…è£… 1: DistributedDataParallel (å¤šGPUå¹¶è¡Œè®­ç»ƒ)
  â†“
å¤–å±‚åŒ…è£… 2: torch.compile (PyTorch 2.0 ç¼–è¯‘ä¼˜åŒ–)
  â†“
å†…æ ¸: çœŸæ­£çš„ MiniMindForCausalLM æ¨¡å‹

ä¿å­˜æ—¶éœ€è¦"è§£åŒ…"åˆ°æœ€é‡Œå±‚,å¦åˆ™:
- ä¿å­˜çš„æ–‡ä»¶åŒ…å«å¤šä½™çš„åŒ…è£…ä»£ç 
- åŠ è½½æ—¶å¯èƒ½å› ä¸ºç¯å¢ƒä¸åŒ (å¦‚å•GPU) è€Œå¤±è´¥
```

**ä»£ç é€»è¾‘**:

```python
# æ­¥éª¤ 1: æ£€æŸ¥æ˜¯å¦æ˜¯ DDP åŒ…è£…
if isinstance(model, DistributedDataParallel):
    raw_model = model.module  # å–å‡ºå†…éƒ¨çš„åŸå§‹æ¨¡å‹
else:
    raw_model = model

# æ­¥éª¤ 2: æ£€æŸ¥æ˜¯å¦æ˜¯ compile åŒ…è£…
if hasattr(raw_model, "_orig_mod"):
    raw_model = raw_model._orig_mod  # å–å‡ºç¼–è¯‘å‰çš„åŸå§‹æ¨¡å‹
```

---

#### 2. FP16 å‹ç¼© (ç¬¬ 175 è¡Œ)

```python
state_dict = {k: v.half().cpu() for k, v in state_dict.items()}
```

**éšå–»è§£é‡Š**:

```
åŸå§‹æ¨¡å‹å‚æ•° (FP32):
æ¯ä¸ªæ•°å­—ç”¨ 32 ä½å­˜å‚¨: 3.141592653589793
â†’ ç²¾åº¦æé«˜,ä½†æ–‡ä»¶å¾ˆå¤§

å‹ç¼©å (FP16):
æ¯ä¸ªæ•°å­—ç”¨ 16 ä½å­˜å‚¨: 3.141
â†’ ç²¾åº¦ç•¥é™ (å°æ•°ç‚¹å 4 ä½),ä½†æ–‡ä»¶å‡å° 50%

ç±»æ¯”:
FP32 = 4K è¶…æ¸…ç”µå½± (10GB)
FP16 = 1080p é«˜æ¸…ç”µå½± (5GB)
â†’ å¯¹äº"å­˜æ¡£"æ¥è¯´,1080p è¶³å¤Ÿæ¸…æ™°äº†
```

**ä¸ºä»€ä¹ˆæ¨ç†æ—¶ç”¨ FP16 æ²¡é—®é¢˜?**

```
è®­ç»ƒæ—¶:
- éœ€è¦è®¡ç®—æ¢¯åº¦ (å¾®å°çš„æ•°å€¼å˜åŒ–éƒ½å¾ˆé‡è¦)
- éœ€è¦é«˜ç²¾åº¦ (FP32 æˆ– BF16)

æ¨ç†æ—¶:
- åªåšå‰å‘ä¼ æ’­,ä¸è®¡ç®—æ¢¯åº¦
- FP16 ç²¾åº¦å®Œå…¨å¤Ÿç”¨,ä¸”é€Ÿåº¦æ›´å¿«
```

---

#### 3. åŸå­ä¿å­˜ (ç¬¬ 178-180 è¡Œ)

```python
ckp_tmp = ckp_path + ".tmp"
torch.save(state_dict, ckp_tmp)   # å†™å…¥ä¸´æ—¶æ–‡ä»¶
os.replace(ckp_tmp, ckp_path)     # åŸå­æ›¿æ¢
```

**éšå–»è§£é‡Š**:

```
åœºæ™¯: ä¿å­˜ä¸€ä¸ª 5GB çš„æ¨¡å‹æ–‡ä»¶

é”™è¯¯åšæ³•:
torch.save(state, "model.pth")
â†’ ä¿å­˜è¿›åº¦: 10% ... 50% ... ğŸ’¥ æ–­ç”µ!
â†’ ç»“æœ: model.pth æŸå (åªå†™äº†ä¸€åŠ,æ— æ³•åŠ è½½)

æ­£ç¡®åšæ³•:
torch.save(state, "model.pth.tmp")  # å†™ä¸´æ—¶æ–‡ä»¶
â†’ ä¿å­˜è¿›åº¦: 10% ... 50% ... ğŸ’¥ æ–­ç”µ!
â†’ ç»“æœ: model.pth.tmp æŸå,ä½†åŸæ¥çš„ model.pth å®Œå¥½æ— æŸ!

å¦‚æœæˆåŠŸ:
os.replace("model.pth.tmp", "model.pth")  # ç¬é—´å®Œæˆ (åŸå­æ“ä½œ)
â†’ è¦ä¹ˆæˆåŠŸæ›¿æ¢,è¦ä¹ˆä¿æŒåŸæ–‡ä»¶ä¸å˜
```

**`os.replace` çš„ç‰¹æ€§**:

```python
# åŸå­æ“ä½œ (Atomic Operation):
# - åœ¨æ“ä½œç³»ç»Ÿå±‚é¢ä¿è¯"è¦ä¹ˆå…¨æˆåŠŸ,è¦ä¹ˆå…¨å¤±è´¥"
# - å³ä½¿æ“ä½œåˆ°ä¸€åŠæ–­ç”µ,ä¹Ÿä¸ä¼šäº§ç”Ÿ"åŠæˆå“"æ–‡ä»¶
os.replace(src, dst)  # æ¨è!

# éåŸå­æ“ä½œ (æœ‰é£é™©):
os.remove(dst)        # å…ˆåˆ é™¤æ—§æ–‡ä»¶
os.rename(src, dst)   # å†é‡å‘½åæ–°æ–‡ä»¶
# â†‘ å¦‚æœåœ¨ä¸¤æ­¥ä¹‹é—´æ–­ç”µ,dst æ–‡ä»¶å°±å½»åº•ä¸¢å¤±äº†!
```

---

#### 4. WandB ID ä¿å­˜ (ç¬¬ 183-189 è¡Œ)

```python
wandb_id = None
if wandb:
    if hasattr(wandb, "get_run"):
        run = wandb.get_run()
        wandb_id = getattr(run, "id", None) if run else None
    else:
        wandb_id = getattr(wandb, "id", None)
```

**éšå–»è§£é‡Š**:

```
WandB (Weights & Biases) = è®­ç»ƒè¿‡ç¨‹çš„"è¡Œè½¦è®°å½•ä»ª"

ä½œç”¨:
- è®°å½•æ¯ä¸€æ­¥çš„ Lossã€Learning Rateã€Accuracy
- ç”Ÿæˆæ¼‚äº®çš„è®­ç»ƒæ›²çº¿å›¾
- æ”¯æŒå¤šäººåä½œæŸ¥çœ‹

é—®é¢˜:
å¦‚æœè®­ç»ƒä¸­æ–­åé‡æ–°å¼€å§‹,WandB ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ Run ID
â†’ æ›²çº¿ä¼šæ–­å¼€,çœ‹èµ·æ¥åƒä¸¤æ¬¡ç‹¬ç«‹çš„è®­ç»ƒ

è§£å†³æ–¹æ¡ˆ:
ä¿å­˜åŸæ¥çš„ wandb_id,æ¢å¤è®­ç»ƒæ—¶ç”¨åŒä¸€ä¸ª ID
â†’ æ›²çº¿ä¼šæ— ç¼è¡”æ¥,çœ‹èµ·æ¥åƒä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒ
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# æ¢å¤è®­ç»ƒæ—¶
ckp_data = lm_checkpoint(config, model=None)  # åŠ è½½æ£€æŸ¥ç‚¹
if ckp_data:
    wandb_id = ckp_data.get("wandb_id")
    wandb.init(project="my_project", id=wandb_id, resume="allow")
    # â†‘ ä½¿ç”¨ä¿å­˜çš„ ID,æ›²çº¿ä¼šæ¥ç€ä¹‹å‰çš„ç”»
```

---

#### 5. å¤„ç†é¢å¤–çš„è®­ç»ƒç»„ä»¶ (ç¬¬ 203-216 è¡Œ)

```python
for key, value in kwargs.items():
    if value is not None:
        if hasattr(value, "state_dict"):
            raw_value = value.module if isinstance(value, DistributedDataParallel) else value
            raw_value = getattr(raw_value, "_orig_mod", raw_value)
            resume_data[key] = raw_value.state_dict()
        else:
            resume_data[key] = value
```

**éšå–»è§£é‡Š**:

```
é™¤äº†æ¨¡å‹å’Œä¼˜åŒ–å™¨,è®­ç»ƒè¿˜å¯èƒ½ç”¨åˆ°:

1. LR Scheduler (å­¦ä¹ ç‡è°ƒåº¦å™¨):
   - è®°å½•äº†"å½“å‰å­¦ä¹ ç‡åº”è¯¥æ˜¯å¤šå°‘"
   - å¦‚æœä¸ä¿å­˜,æ¢å¤åå­¦ä¹ ç‡ä¼šé‡ç½®,å½±å“æ”¶æ•›

2. Scaler (æ··åˆç²¾åº¦ç¼©æ”¾å™¨):
   - è®°å½•äº† FP16 è®­ç»ƒçš„ç¼©æ”¾å› å­
   - å¦‚æœä¸ä¿å­˜,å¯èƒ½å¯¼è‡´æ¢¯åº¦æº¢å‡º

3. å…¶ä»–è‡ªå®šä¹‰ç»„ä»¶:
   - å¦‚ EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡) çš„å½±å­æ¨¡å‹
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# ä¿å­˜æ—¶
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)
lm_checkpoint(
    config,
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,  # â† ä¼ å…¥ scheduler
)

# æ¢å¤æ—¶
ckp_data = lm_checkpoint(config, model=None)
optimizer.load_state_dict(ckp_data["optimizer"])
scheduler.load_state_dict(ckp_data["scheduler"])  # â† æ¢å¤ scheduler çŠ¶æ€
```

---

### ğŸ”„ åŠ è½½æ¨¡å¼ï¼šä»å­˜æ¡£ä¸­æ¢å¤

#### ä»£ç é€»è¾‘ (ç¬¬ 227-246 è¡Œ)

```python
else:
    # ==================== åŠ è½½æ¨¡å¼ ====================

    if os.path.exists(resume_path):
        # 1. åŠ è½½æ¢å¤æ–‡ä»¶åˆ° CPU
        ckp_data = torch.load(resume_path, map_location="cpu")

        # 2. å¤„ç† GPU æ•°é‡å˜åŒ–å¸¦æ¥çš„ Step å·®å¼‚
        saved_ws = ckp_data.get("world_size", 1)
        current_ws = dist.get_world_size() if dist.is_initialized() else 1

        if saved_ws != current_ws:
            ckp_data["step"] = ckp_data["step"] * saved_ws // current_ws
            print(f"GPUæ•°é‡å˜åŒ–({saved_ws}â†’{current_ws}),stepå·²è‡ªåŠ¨è½¬æ¢ä¸º{ckp_data['step']}")

        return ckp_data
    return None
```

---

#### å…³é”®ç‚¹ 1: `map_location="cpu"`

```python
ckp_data = torch.load(resume_path, map_location="cpu")
```

**éšå–»è§£é‡Š**:

```
é—®é¢˜åœºæ™¯:
ä¿å­˜æ—¶ç”¨çš„æ˜¯ 4 å— GPU,æ¯å— GPU çš„æ•°æ®åˆ†åˆ«åœ¨:
- cuda:0
- cuda:1
- cuda:2
- cuda:3

åŠ è½½æ—¶å¯èƒ½:
- åªæœ‰ 1 å— GPU â†’ å¦‚æœç›´æ¥åŠ è½½åˆ° cuda:1,ä¼šæŠ¥é”™ (è®¾å¤‡ä¸å­˜åœ¨)
- ç”¨çš„æ˜¯ CPU â†’ å¦‚æœç›´æ¥åŠ è½½åˆ° cuda:0,ä¼šæŠ¥é”™

è§£å†³æ–¹æ¡ˆ:
å…ˆç»Ÿä¸€åŠ è½½åˆ° CPU,å†æ ¹æ®å®é™…æƒ…å†µåˆ†é…åˆ° GPU
â†’ ç±»ä¼¼äº"å…ˆæŠŠè´§ç‰©å¸åˆ°ä»“åº“,å†æ ¹æ®éœ€è¦é…é€"
```

---

#### å…³é”®ç‚¹ 2: GPU æ•°é‡å˜åŒ–çš„ Step è°ƒæ•´

```python
saved_ws = ckp_data.get("world_size", 1)  # ä¿å­˜æ—¶ç”¨äº†å‡ å— GPU
current_ws = dist.get_world_size() if dist.is_initialized() else 1  # ç°åœ¨ç”¨å‡ å—

if saved_ws != current_ws:
    ckp_data["step"] = ckp_data["step"] * saved_ws // current_ws
```

**éšå–»è§£é‡Š**:

```
è®­ç»ƒåœºæ™¯:
- åŸæ¥ç”¨ 4 å— GPU è®­ç»ƒåˆ°ç¬¬ 1000 æ­¥
- ç°åœ¨æ¢æˆ 8 å— GPU ç»§ç»­è®­ç»ƒ

é—®é¢˜:
Global Batch Size = Per-GPU Batch Size Ã— GPU æ•°é‡
- 4 GPU æ—¶: Batch=32 Ã— 4 = 128
- 8 GPU æ—¶: Batch=32 Ã— 8 = 256 (ç¿»å€!)

å½±å“:
åŸæ¥ 1 æ­¥çœ‹ 128 ä¸ªæ ·æœ¬,ç°åœ¨ 1 æ­¥çœ‹ 256 ä¸ªæ ·æœ¬
â†’ å®é™…è¿›åº¦åº”è¯¥æ˜¯åŸæ¥çš„ 2 å€

è°ƒæ•´å…¬å¼:
new_step = old_step Ã— old_gpu_count / new_gpu_count
new_step = 1000 Ã— 4 / 8 = 500

è§£é‡Š:
è™½ç„¶è·‘äº† 1000 æ­¥,ä½†å› ä¸º batch ç¿»å€,
å®é™…ç›¸å½“äº 8 å¡ä¸‹çš„ 500 æ­¥
```

**å®é™…ä¾‹å­**:

```
åœºæ™¯ 1: ä» 4 å¡æ¢åˆ° 8 å¡
saved_ws = 4, current_ws = 8
step = 1000 Ã— 4 / 8 = 500
â†’ é¿å…å­¦ä¹ ç‡è°ƒåº¦å™¨æå‰ç»“æŸ

åœºæ™¯ 2: ä» 8 å¡æ¢åˆ° 4 å¡
saved_ws = 8, current_ws = 4
step = 500 Ã— 8 / 4 = 1000
â†’ é¿å…å­¦ä¹ ç‡è°ƒåº¦å™¨å»¶åç»“æŸ
```

---

### ğŸ’¡ å®Œæ•´ä½¿ç”¨æµç¨‹

#### è®­ç»ƒè„šæœ¬ä¸­çš„å…¸å‹ç”¨æ³•

```python
# ==================== åˆå§‹åŒ– ====================
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config).cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10000)

start_epoch = 0
start_step = 0

# ==================== å°è¯•åŠ è½½æ£€æŸ¥ç‚¹ ====================
ckp_data = lm_checkpoint(config, model=None, save_dir="./checkpoints")

if ckp_data:
    print("å‘ç°æ£€æŸ¥ç‚¹,æ­£åœ¨æ¢å¤...")
    model.load_state_dict(ckp_data["model"])
    optimizer.load_state_dict(ckp_data["optimizer"])
    scheduler.load_state_dict(ckp_data.get("scheduler", scheduler.state_dict()))
    start_epoch = ckp_data["epoch"]
    start_step = ckp_data["step"]
    print(f"ä» Epoch {start_epoch}, Step {start_step} ç»§ç»­è®­ç»ƒ")
else:
    print("æœªå‘ç°æ£€æŸ¥ç‚¹,ä»å¤´å¼€å§‹è®­ç»ƒ")

# ==================== è®­ç»ƒå¾ªç¯ ====================
for epoch in range(start_epoch, 10):
    for step, batch in enumerate(dataloader):
        if epoch == start_epoch and step < start_step:
            continue  # è·³è¿‡å·²ç»è®­ç»ƒè¿‡çš„æ­¥éª¤
        
        # ... è®­ç»ƒä»£ç  ...
        
        # æ¯ 1000 æ­¥ä¿å­˜ä¸€æ¬¡
        if step % 1000 == 0:
            lm_checkpoint(
                config,
                model=model,
                optimizer=optimizer,
                epoch=epoch,
                step=step,
                scheduler=scheduler,
                save_dir="./checkpoints",
            )
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: Epoch {epoch}, Step {step}")
```

---

### âš ï¸ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1: åŠ è½½æ£€æŸ¥ç‚¹æ—¶æŠ¥é”™ "Missing keys" æˆ– "Unexpected keys"

**åŸå› **:
```
ä¿å­˜æ—¶çš„æ¨¡å‹ç»“æ„:
MiniMindForCausalLM(
    model: MiniMindModel(...),
    lm_head: Linear(...)
)

åŠ è½½æ—¶çš„æ¨¡å‹ç»“æ„:
MiniMindModel(...)  # å°‘äº† lm_head!
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ³• 1: ä½¿ç”¨ strict=False (ä¸æ¨è,å¯èƒ½éšè—çœŸæ­£çš„é—®é¢˜)
model.load_state_dict(ckp_data["model"], strict=False)

# æ–¹æ³• 2: æ‰‹åŠ¨è¿‡æ»¤ä¸åŒ¹é…çš„é”® (æ¨è)
model_state = ckp_data["model"]
model_keys = set(model.state_dict().keys())
ckp_keys = set(model_state.keys())

missing_keys = model_keys - ckp_keys
unexpected_keys = ckp_keys - model_keys

if missing_keys:
    print(f"è­¦å‘Š: ç¼ºå°‘é”® {missing_keys}")
if unexpected_keys:
    print(f"è­¦å‘Š: å¤šä½™é”® {unexpected_keys}")
    model_state = {k: v for k, v in model_state.items() if k in model_keys}

model.load_state_dict(model_state)
```

---

#### é—®é¢˜ 2: æ˜¾å­˜ä¸è¶³,æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹

**åŸå› **:
```
åŠ è½½ checkpoint æ—¶:
1. å…ˆåŠ è½½åˆ° CPU (å ç”¨ 2GB å†…å­˜)
2. å†å¤åˆ¶åˆ° GPU (å ç”¨ 2GB æ˜¾å­˜)
3. æ¨¡å‹å·²ç»åœ¨ GPU ä¸Š (å ç”¨ 2GB æ˜¾å­˜)
â†’ å³°å€¼æ˜¾å­˜: 4GB!
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ³• 1: å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹
ckp_data = lm_checkpoint(config, model=None)
model = MiniMindForCausalLM(config)  # å…ˆåˆ›å»ºç©ºæ¨¡å‹ (æ˜¾å­˜å ç”¨å°)
model.load_state_dict(ckp_data["model"])  # å†åŠ è½½æƒé‡
model.cuda()  # æœ€åç§»åŠ¨åˆ° GPU

# æ–¹æ³• 2: ä½¿ç”¨ mmap æ¨¡å¼ (é€‚åˆè¶…å¤§æ¨¡å‹)
ckp_data = torch.load(resume_path, map_location="cpu", mmap=True)
# â†‘ ä¸ä¼šä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜,è€Œæ˜¯æŒ‰éœ€è¯»å–
```

---

#### é—®é¢˜ 3: å¤šæœºè®­ç»ƒæ—¶,åªæœ‰ Rank 0 ä¿å­˜æ£€æŸ¥ç‚¹

```python
# åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­,åº”è¯¥åªè®©ä¸»è¿›ç¨‹ä¿å­˜
if dist.get_rank() == 0:  # åªæœ‰ Rank 0 (ä¸»è¿›ç¨‹) æ‰§è¡Œä¿å­˜
    lm_checkpoint(
        config,
        model=model,
        optimizer=optimizer,
        ...
    )

# å…¶ä»–è¿›ç¨‹ç­‰å¾…ä¿å­˜å®Œæˆ
if dist.is_initialized():
    dist.barrier()  # åŒæ­¥,ç¡®ä¿ Rank 0 ä¿å­˜å®Œæˆåå†ç»§ç»­
```

---

### ğŸ“Š ä¸¤ç§æ–‡ä»¶çš„å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | çº¯æƒé‡æ–‡ä»¶ (ckp_path) | å®Œæ•´æ¢å¤æ–‡ä»¶ (resume_path) |
|------|---------------------|-------------------------|
| **åŒ…å«å†…å®¹** | ä»…æ¨¡å‹å‚æ•° | æ¨¡å‹ + ä¼˜åŒ–å™¨ + è®­ç»ƒçŠ¶æ€ |
| **æ–‡ä»¶å¤§å°** | ~500MB | ~1GB |
| **ç²¾åº¦** | FP16 | FP16 (å¯æ”¹ä¸º FP32) |
| **ç”¨é€”** | æ¨ç†/éƒ¨ç½² | æ–­ç‚¹ç»­è®­ |
| **æ˜¯å¦å¯è·¨GPUæ•°é‡** | âœ“ æ˜¯ | âœ“ æ˜¯ (ä¼šè‡ªåŠ¨è°ƒæ•´ step) |
| **æ˜¯å¦å¯è·¨ä¼˜åŒ–å™¨** | âœ“ æ˜¯ | âœ— å¦ (å¿…é¡»ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨) |
| **åŠ è½½é€Ÿåº¦** | å¿« | æ…¢ (æ–‡ä»¶æ›´å¤§) |

---

### ğŸ“ ç±»æ¯”æ€»ç»“

```
lm_checkpoint å‡½æ•°å°±åƒ:

1. æ¸¸æˆçš„å­˜æ¡£ç³»ç»Ÿ:
   - çº¯æƒé‡æ–‡ä»¶ = å¿«é€Ÿå­˜æ¡£ (åªä¿å­˜è§’è‰²å±æ€§,ç”¨äºå¿«é€Ÿè¯»å–)
   - å®Œæ•´æ¢å¤æ–‡ä»¶ = å®Œæ•´å­˜æ¡£ (ä¿å­˜æ‰€æœ‰çŠ¶æ€,ç”¨äºå®Œç¾å¤åŸ)

2. æ–‡æ¡£ç¼–è¾‘è½¯ä»¶:
   - çº¯æƒé‡æ–‡ä»¶ = å¯¼å‡ºä¸º PDF (åªè¯»,ç”¨äºåˆ†äº«)
   - å®Œæ•´æ¢å¤æ–‡ä»¶ = ä¿å­˜ä¸º .docx (å¯ç»§ç»­ç¼–è¾‘)

3. è§†é¢‘å‰ªè¾‘è½¯ä»¶:
   - çº¯æƒé‡æ–‡ä»¶ = å¯¼å‡ºä¸º MP4 (æˆå“,ç”¨äºæ’­æ”¾)
   - å®Œæ•´æ¢å¤æ–‡ä»¶ = ä¿å­˜ä¸ºå·¥ç¨‹æ–‡ä»¶ (åŒ…å«æ‰€æœ‰ç´ æå’Œæ—¶é—´è½´,å¯ç»§ç»­å‰ªè¾‘)
```

---

### ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„å…³ç³»

```mermaid
graph TD
    A[è®­ç»ƒè„šæœ¬] -->|è°ƒç”¨| B[lm_checkpoint]
    B -->|ä¿å­˜| C[çº¯æƒé‡æ–‡ä»¶ .pth]
    B -->|ä¿å­˜| D[å®Œæ•´æ¢å¤æ–‡ä»¶ _resume.pth]
    
    E[æ¨ç†è„šæœ¬] -->|åŠ è½½| C
    F[æ–­ç‚¹ç»­è®­] -->|åŠ è½½| D
    
    D -->|æ¢å¤| G[model.load_state_dict]
    D -->|æ¢å¤| H[optimizer.load_state_dict]
    D -->|æ¢å¤| I[scheduler.load_state_dict]
    D -->|æ¢å¤| J[wandb.init with resume]
```

---
## å››ã€æ•™å°æ˜è¯†å­—:åˆ†è¯å™¨ (Tokenizer) ä¸è¯åµŒå…¥ (Embedding)

### ğŸ“ æ ¸å¿ƒé—®é¢˜:è®¡ç®—æœºå¦‚ä½•ç†è§£æ–‡å­—?

**é—®é¢˜åœºæ™¯**:

```
å°æ˜çš„å¤§è„‘(ç¥ç»ç½‘ç»œ)åªèƒ½å¤„ç†æ•°å­—:
[0.5, -0.3, 0.8, ...]

ä½†æˆ‘ä»¬è¦æ•™ä»–å†™çš„æ˜¯æ–‡å­—:
"å°çº¢å¸½å»æ£®æ—"

å¦‚ä½•æŠŠæ–‡å­—å˜æˆæ•°å­—?
```

**è§£å†³æ–¹æ¡ˆ**:ä¸¤æ­¥è½¬æ¢

```
æ–‡å­— â†’ [åˆ†è¯å™¨] â†’ Token ID â†’ [è¯åµŒå…¥] â†’ å‘é‡
"å°çº¢å¸½" â†’ 453 â†’ [0.2, -0.5, 0.3, ...]
```

---

### ğŸ”¤ ç¬¬ä¸€æ­¥:åˆ†è¯å™¨ (Tokenizer) - å»ºç«‹"è¯æ±‡è¡¨"

#### ä»£ç ä½ç½®:ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.py:1393

```python
# ç¬¬ 1393 è¡Œ
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
```

#### ğŸ¯ éšå–»:ç»™æ¯ä¸ªè¯åˆ†é…ä¸€ä¸ª"èº«ä»½è¯å·"

**ç±»æ¯”åœºæ™¯**:

```
å­¦æ ¡ç»™æ¯ä¸ªå­¦ç”Ÿåˆ†é…å­¦å·:
- å¼ ä¸‰ â†’ 001
- æå›› â†’ 002
- ç‹äº” â†’ 003

åŒæ ·,åˆ†è¯å™¨ç»™æ¯ä¸ªè¯åˆ†é… ID:
- "å°çº¢å¸½" â†’ 453
- "å»" â†’ 234
- "æ£®æ—" â†’ 789
```

---

#### ğŸ“– åˆ†è¯å™¨çš„"è¯å…¸"ç»“æ„

**è¯æ±‡è¡¨ç¤ºä¾‹** (`vocab_size=6400`):

```python
tokenizer.vocab = {
    # ç‰¹æ®Šç¬¦å·
    "<PAD>": 0,      # å¡«å……ç¬¦å· (ç”¨äºå¯¹é½åºåˆ—é•¿åº¦)
    "<BOS>": 1,      # å¥å­å¼€å§‹ (Beginning of Sequence)
    "<EOS>": 2,      # å¥å­ç»“æŸ (End of Sequence)
    "<UNK>": 3,      # æœªçŸ¥è¯ (Unknown)
    
    # å¸¸ç”¨å­—è¯
    "æˆ‘": 4,
    "çš„": 5,
    "ä½ ": 6,
    "æ˜¯": 7,
    "åœ¨": 8,
    
    # å¸¸ç”¨è¯ç»„
    "å°çº¢å¸½": 453,
    "å»": 234,
    "æ£®æ—": 789,
    "å¤§ç°ç‹¼": 1024,
    
    # ... (å…± 6400 ä¸ªè¯)
    "é‡å­çº ç¼ ": 6399
}
```

---

#### ğŸ”§ åˆ†è¯è¿‡ç¨‹:æ–‡æœ¬ â†’ Token IDs

**ç¤ºä¾‹ä»£ç **:

```python
from transformers import AutoTokenizer

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("./tokenizer_config")

# è¾“å…¥æ–‡æœ¬
text = "å°çº¢å¸½å»æ£®æ—"

# åˆ†è¯å¹¶ç¼–ç 
token_ids = tokenizer.encode(text)
print(token_ids)
# è¾“å‡º: [1, 453, 234, 789, 2]
#        â†‘   â†‘   â†‘   â†‘   â†‘
#      BOS å°çº¢å¸½ å» æ£®æ— EOS
```

**è¯¦ç»†æ­¥éª¤è§£æ**:

```
Step 1: åˆ‡åˆ†è¯è¯­ (Tokenization)
åŸæ–‡: "å°çº¢å¸½å»æ£®æ—"
â†“
åˆ‡åˆ†å: ["å°çº¢å¸½", "å»", "æ£®æ—"]
(ä¸­æ–‡åˆ†è¯å™¨å¯èƒ½æŒ‰å­—åˆ‡åˆ†æˆ–æŒ‰è¯åˆ‡åˆ†,å–å†³äºè®­ç»ƒæ–¹å¼)

Step 2: æŸ¥è¯å…¸ (Lookup)
"å°çº¢å¸½" â†’ åœ¨è¯å…¸ä¸­æ‰¾åˆ° â†’ ID = 453
"å»"     â†’ åœ¨è¯å…¸ä¸­æ‰¾åˆ° â†’ ID = 234
"æ£®æ—"   â†’ åœ¨è¯å…¸ä¸­æ‰¾åˆ° â†’ ID = 789

Step 3: æ·»åŠ ç‰¹æ®Šç¬¦å·
åŸå§‹ IDs: [453, 234, 789]
â†“
æ·»åŠ  BOS å’Œ EOS:
[1, 453, 234, 789, 2]
```

---

#### ğŸ¤” å¦‚æœé‡åˆ°"ç”Ÿåƒ»è¯"æ€ä¹ˆåŠ?

**åœºæ™¯**:

```python
text = "å°çº¢å¸½å»é‡å­æ£®æ—"
#               â†‘â†‘
#            è¯å…¸é‡Œæ²¡æœ‰è¿™ä¸ªè¯!
```

**ä¸‰ç§å¤„ç†ç­–ç•¥**:

##### ç­–ç•¥ 1: æ ‡è®°ä¸º `<UNK>` (Unknown)

```python
# è€å¼æ–¹æ³• (BERT æ—¶ä»£)
"é‡å­æ£®æ—" â†’ è¯å…¸ä¸­ä¸å­˜åœ¨ â†’ ID = 3 (<UNK>)

ç¼ºç‚¹:
- ä¿¡æ¯å®Œå…¨ä¸¢å¤±!
- "é‡å­æ£®æ—" å’Œ "æ ¸èšå˜" éƒ½å˜æˆ 3,æ— æ³•åŒºåˆ†
```

##### ç­–ç•¥ 2: æ‹†åˆ†ä¸ºå­è¯ (Subword Tokenization)

```python
# ç°ä»£æ–¹æ³• (GPT/Llama å¸¸ç”¨: BPE, WordPiece, SentencePiece)
"é‡å­æ£®æ—" â†’ æ‹†åˆ†ä¸ºæ›´å°çš„å•å…ƒ

ä¾‹å¦‚ (BPE ç®—æ³•):
"é‡å­æ£®æ—" â†’ ["é‡", "å­", "æ£®æ—"]
          â†’ [4521, 4522, 789]

ä¼˜ç‚¹:
- ä»»ä½•è¯éƒ½èƒ½è¡¨ç¤º (æœ€åæƒ…å†µæ‹†æˆå•å­—)
- å¸¸ç”¨è¯ä¿æŒå®Œæ•´ (å¦‚"æ£®æ—")
- ç”Ÿåƒ»è¯æ‹†æˆå°å— (å¦‚"é‡å­" â†’ "é‡"+"å­")
```

##### ç­–ç•¥ 3: å­—èŠ‚çº§ç¼–ç  (Byte-level Encoding)

```python
# æœ€ç°ä»£çš„æ–¹æ³• (GPT-4, Llama 2)
ä»»ä½• Unicode å­—ç¬¦éƒ½èƒ½è¡¨ç¤ºä¸º UTF-8 å­—èŠ‚åºåˆ—

"é‡å­æ£®æ—" â†’ UTF-8 å­—èŠ‚ â†’ [0xE9, 0x87, 0x8F, ...] â†’ Token IDs

ä¼˜ç‚¹:
- 100% è¦†ç›–ç‡,æ²¡æœ‰ <UNK>
- æ”¯æŒæ‰€æœ‰è¯­è¨€,åŒ…æ‹¬è¡¨æƒ…ç¬¦å· ğŸ‰
```

---

#### ğŸ“Š ä¸åŒåˆ†è¯æ–¹å¼çš„å¯¹æ¯”

| åˆ†è¯æ–¹æ³• | ä»£è¡¨æ¨¡å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | è¯è¡¨å¤§å° |
|---------|---------|------|------|---------|
| **è¯çº§åˆ«** (Word-level) | Word2Vec | ç›´è§‚æ˜“æ‡‚ | è¯è¡¨å·¨å¤§,ç”Ÿåƒ»è¯å¤š | 50,000+ |
| **å­—ç¬¦çº§åˆ«** (Char-level) | CharRNN | è¯è¡¨å°,æ—  OOV | åºåˆ—å¤ªé•¿,å­¦ä¹ å›°éš¾ | 256 (ASCII) |
| **å­è¯çº§åˆ«** (Subword: BPE/WordPiece) | BERT, GPT-2 | å¹³è¡¡è¯è¡¨å’Œåºåˆ—é•¿åº¦ | åˆ‡åˆ†æ–¹å¼ä¸ç›´è§‚ | 30,000-50,000 |
| **å­—èŠ‚çº§åˆ«** (Byte-level BPE) | GPT-4, Llama 2 | å®Œç¾è¦†ç›–æ‰€æœ‰å­—ç¬¦ | ä¸­æ–‡ç­‰è¯­è¨€åºåˆ—å˜é•¿ | 50,257 |

---

### ğŸ§® ç¬¬äºŒæ­¥:è¯åµŒå…¥ (Embedding) - æŠŠ ID å˜æˆ"æœ‰æ„ä¹‰çš„å‘é‡"

#### ä»£ç ä½ç½®:ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.py:1096

```python
# ç¬¬ 1096 è¡Œ
self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
# ä¾‹å¦‚: nn.Embedding(6400, 512)
#                   â†‘      â†‘
#                è¯è¡¨å¤§å°  å‘é‡ç»´åº¦
```

---

#### ğŸ¯ éšå–»:ä»"èº«ä»½è¯å·"åˆ°"ä¸ªäººæ¡£æ¡ˆ"

**é—®é¢˜**:

```
Token ID åªæ˜¯ä¸€ä¸ªç¼–å·:
453 (ä»£è¡¨"å°çº¢å¸½")

ä½†è¿™ä¸ªæ•°å­—æœ¬èº«æ²¡æœ‰ä»»ä½•æ„ä¹‰!
- è®¡ç®—æœºä¸çŸ¥é“ 453 å’Œ 454 æœ‰ä»€ä¹ˆå…³ç³»
- ä¸çŸ¥é“ 453 ("å°çº¢å¸½") å’Œ 1024 ("å¤§ç°ç‹¼") åœ¨è¯­ä¹‰ä¸Šæœ‰å…³è”
```

**è§£å†³æ–¹æ¡ˆ:è¯åµŒå…¥ (Embedding)**

```
æŠŠæ¯ä¸ª Token ID æ˜ å°„åˆ°ä¸€ä¸ª"ç‰¹å¾å‘é‡"

Token ID 453 ("å°çº¢å¸½") â†’ å‘é‡ [0.2, -0.5, 0.3, 0.8, ...]
                                  â†‘     â†‘     â†‘     â†‘
                               æ€§åˆ«ç‰¹å¾ å¹´é¾„ç‰¹å¾ ç«¥è¯ç‰¹å¾ ...

è¿™ä¸ªå‘é‡åŒ…å«äº†è¯çš„"è¯­ä¹‰ä¿¡æ¯"!
```

---

#### ğŸ“– Embedding å±‚çš„å·¥ä½œåŸç†

**æœ¬è´¨:ä¸€ä¸ªæŸ¥æ‰¾è¡¨ (Lookup Table)**

```python
# Embedding å±‚çš„å†…éƒ¨ç»“æ„
class Embedding(nn.Module):
    def __init__(self, vocab_size, hidden_size):
        # åˆ›å»ºä¸€ä¸ªæŸ¥æ‰¾è¡¨ (æƒé‡çŸ©é˜µ)
        # å½¢çŠ¶: [vocab_size, hidden_size]
        # ä¾‹å¦‚: [6400, 512]
        self.weight = nn.Parameter(torch.randn(vocab_size, hidden_size))
    
    def forward(self, input_ids):
        # input_ids: [batch_size, seq_len]
        # ä¾‹å¦‚: [[1, 453, 234, 789, 2]]
        
        # ç´¢å¼•æŸ¥æ‰¾ (ç±»ä¼¼äºå­—å…¸æŸ¥è¯¢)
        return self.weight[input_ids]
        # è¾“å‡º: [batch_size, seq_len, hidden_size]
        # ä¾‹å¦‚: [1, 5, 512]
```

---

#### ğŸ” å…·ä½“ä¾‹å­:ä» ID åˆ°å‘é‡

**å‡è®¾**:
- `vocab_size = 6400`
- `hidden_size = 512`

**Embedding æƒé‡è¡¨**:

```python
# å½¢çŠ¶: [6400, 512]
# æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¯çš„å‘é‡

embedding_table = [
    # ID 0: <PAD>
    [0.0, 0.0, 0.0, ..., 0.0],  # 512 ä¸ª 0 (å¡«å……ç¬¦å·æ²¡æœ‰æ„ä¹‰)
    
    # ID 1: <BOS>
    [0.1, -0.2, 0.5, ..., 0.3],
    
    # ID 453: "å°çº¢å¸½"
    [0.2, -0.5, 0.3, 0.8, 0.1, ..., -0.4],  # 512 ç»´å‘é‡
    
    # ID 234: "å»"
    [-0.1, 0.3, -0.2, 0.5, ..., 0.7],
    
    # ID 789: "æ£®æ—"
    [0.4, -0.3, 0.6, -0.1, ..., 0.2],
    
    # ... (å…± 6400 è¡Œ)
]
```

**æŸ¥æ‰¾è¿‡ç¨‹**:

```python
# è¾“å…¥: Token IDs
input_ids = torch.tensor([[1, 453, 234, 789, 2]])  # shape: [1, 5]

# Embedding æŸ¥æ‰¾
embeddings = embed_tokens(input_ids)  # shape: [1, 5, 512]

# ç­‰ä»·äº:
embeddings = torch.stack([
    embedding_table[1],    # <BOS> çš„å‘é‡
    embedding_table[453],  # "å°çº¢å¸½" çš„å‘é‡
    embedding_table[234],  # "å»" çš„å‘é‡
    embedding_table[789],  # "æ£®æ—" çš„å‘é‡
    embedding_table[2],    # <EOS> çš„å‘é‡
], dim=1)
```

**ç»“æœ**:

```
åŸå§‹è¾“å…¥ (ç¦»æ•£ç¬¦å·):
"å°çº¢å¸½å»æ£®æ—"

ç»è¿‡ Tokenizer:
[1, 453, 234, 789, 2]

ç»è¿‡ Embedding:
[
  [0.1, -0.2, 0.5, ..., 0.3],    # <BOS>
  [0.2, -0.5, 0.3, ..., -0.4],   # å°çº¢å¸½
  [-0.1, 0.3, -0.2, ..., 0.7],   # å»
  [0.4, -0.3, 0.6, ..., 0.2],    # æ£®æ—
  [0.05, 0.1, -0.1, ..., 0.0]    # <EOS>
]
å½¢çŠ¶: [1, 5, 512]
      â†‘  â†‘  â†‘
    batch seq hidden
```

---

#### ğŸ¤” è¿™äº›å‘é‡çš„"æ„ä¹‰"ä»å“ªæ¥?

**å…³é”®:è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨å­¦ä¹ !**

```
åˆå§‹çŠ¶æ€ (éšæœºåˆå§‹åŒ–):
"å°çº¢å¸½" â†’ [0.01, -0.02, 0.03, ...]  (éšæœºå™ªå£°,æ²¡æœ‰æ„ä¹‰)
"å¤§ç°ç‹¼" â†’ [0.05, 0.01, -0.04, ...]  (ä¹Ÿæ˜¯éšæœºçš„)

è®­ç»ƒè¿‡ç¨‹:
æ¨¡å‹çœ‹äº†å¤§é‡ç«¥è¯æ•…äº‹å,å‘ç°:
- "å°çº¢å¸½" å’Œ "å¤§ç°ç‹¼" ç»å¸¸ä¸€èµ·å‡ºç°
- "å°çº¢å¸½" å’Œ "æ£®æ—" ç»å¸¸ä¸€èµ·å‡ºç°
- "å°çº¢å¸½" å’Œ "é‡å­åŠ›å­¦" å‡ ä¹ä¸å‡ºç°

é€šè¿‡åå‘ä¼ æ’­,Embedding å‘é‡é€æ¸è°ƒæ•´:

è®­ç»ƒåçš„çŠ¶æ€:
"å°çº¢å¸½" â†’ [0.8, -0.2, 0.9, ...]  (ç«¥è¯ç‰¹å¾=0.8, ç°ä»£ç§‘æŠ€ç‰¹å¾=-0.2)
"å¤§ç°ç‹¼" â†’ [0.7, -0.3, 0.8, ...]  (ç«¥è¯ç‰¹å¾=0.7, ç›¸ä¼¼!)
"é‡å­åŠ›å­¦" â†’ [-0.5, 0.9, -0.6, ...] (ç«¥è¯ç‰¹å¾=-0.5, å¾ˆä¸åŒ!)
```

**ç±»æ¯”**:

```
å°±åƒä½ é€šè¿‡è§‚å¯Ÿå­¦ä¹ è¯ä¹‰:

ä½ è§è¿‡å¾ˆå¤šå¥å­:
- "å°çº¢å¸½åœ¨æ£®æ—é‡Œé‡åˆ°äº†å¤§ç°ç‹¼"
- "å°çº¢å¸½å®³æ€•å¤§ç°ç‹¼"
- "å¤§ç°ç‹¼è¿½é€å°çº¢å¸½"

å¤§è„‘è‡ªåŠ¨æ€»ç»“:
â†’ "å°çº¢å¸½" å’Œ "å¤§ç°ç‹¼" æœ‰å…³è”
â†’ å®ƒä»¬çš„"å‘é‡"åº”è¯¥æ¯”è¾ƒæ¥è¿‘

æ¨¡å‹ä¹Ÿæ˜¯è¿™æ ·å­¦ä¹ çš„!
é€šè¿‡å¤§é‡æ–‡æœ¬,è‡ªåŠ¨å­¦ä¼šè¯ä¸è¯ä¹‹é—´çš„å…³ç³»
```

---

### ğŸ’¡ æ€»ç»“:ä»æ–‡å­—åˆ°å‘é‡çš„å®Œæ•´æµç¨‹

```mermaid
graph TD
    A[åŸå§‹æ–‡æœ¬<br/>'å°çº¢å¸½å»æ£®æ—'] -->|åˆ†è¯| B[Tokenizer]
    B -->|åˆ‡åˆ†è¯è¯­| C[Tokens<br/>å°çº¢å¸½,å»,æ£®æ—]
    C -->|æŸ¥è¯å…¸| D[Token IDs<br/>1,453,234,789,2]
    D -->|ç´¢å¼•æŸ¥æ‰¾| E[Embedding Table<br/>6400Ã—512çŸ©é˜µ]
    E -->|è·å–å‘é‡| F[Embedding Vectors<br/>5Ã—512çŸ©é˜µ]
    F -->|é€å…¥æ¨¡å‹| G[Transformer Layers]
    G --> H[é¢„æµ‹ä¸‹ä¸€ä¸ªè¯]
```

---

## äº”ã€å°æ˜çš„"è”æƒ³å‘æ•£"èƒ½åŠ›ï¼šFeedForward å‰é¦ˆç½‘ç»œ (SwiGLU)

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 249-298 è¡Œ)

```python
class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œå±‚ (Feed-Forward Network, FFN)ã€‚
    è¿™é‡Œé‡‡ç”¨çš„æ˜¯ Llama æ¶æ„ä¸­ç»å…¸çš„ SwiGLU (Swish-Gated Linear Unit) å˜ä½“ã€‚
    ç›¸æ¯”ä¼ ç»Ÿçš„ ReLU FFN (up_proj -> relu -> down_proj)ï¼ŒSwiGLU å¢åŠ äº†é—¨æ§æœºåˆ¶ï¼Œæ€§èƒ½é€šå¸¸æ›´å¥½ã€‚
    """

    def __init__(self, config: MiniMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)

        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)

        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.dropout(
            self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        )
```

---

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šå°æ˜çš„"æ€ç»´å‘æ•£ä¸æ”¶æ•›"èƒ½åŠ›

åœ¨ Transformer æ¶æ„ä¸­ï¼Œæ¯ä¸€å±‚éƒ½æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
1. **Attention (æ³¨æ„åŠ›æœºåˆ¶)**ï¼šè®©å°æ˜"çœ‹åˆ°"å¥å­ä¸­æ‰€æœ‰è¯çš„å…³ç³»
2. **FeedForward (å‰é¦ˆç½‘ç»œ)**ï¼šè®©å°æ˜å¯¹çœ‹åˆ°çš„å†…å®¹è¿›è¡Œ"æ·±åº¦æ€è€ƒ"

FeedForward å°±åƒå°æ˜çš„**è”æƒ³æ€ç»´èƒ½åŠ›**â€”â€”çœ‹åˆ°ä¸€ä¸ªè¯åï¼Œå¤§è„‘ä¼šè¿…é€Ÿè”æƒ³åˆ°å¾ˆå¤šç›¸å…³æ¦‚å¿µï¼Œç„¶åç­›é€‰å‡ºæœ€é‡è¦çš„å‡ ä¸ªã€‚

---

### ğŸ§  æ ¸å¿ƒè®¾è®¡ï¼šä¸ºä»€ä¹ˆè¦"å…ˆæ‰©å¼ ï¼Œå†å‹ç¼©"ï¼Ÿ

#### ä¼ ç»Ÿ FFN ç»“æ„ (ReLU æ—¶ä»£)

```
è¾“å…¥ (512 ç»´)
    â†“
ã€æ‰©å¼ ã€‘Linear: 512 â†’ 2048
    â†“
ã€æ¿€æ´»ã€‘ReLU
    â†“
ã€å‹ç¼©ã€‘Linear: 2048 â†’ 512
    â†“
è¾“å‡º (512 ç»´)
```

**é—®é¢˜**ï¼šReLU ä¼šæŠŠæ‰€æœ‰è´Ÿæ•°å˜æˆ 0ï¼Œä¿¡æ¯ä¸¢å¤±ä¸¥é‡

#### ç°ä»£ FFN ç»“æ„ (SwiGLUï¼ŒLlama/GPT-4 ä½¿ç”¨)

```
è¾“å…¥ (512 ç»´)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚           â”‚
â†“           â†“           â”‚
gate_proj   up_proj     â”‚
512â†’1408    512â†’1408    â”‚
    â†“           â†“       â”‚
ã€æ¿€æ´»ã€‘SiLU     â”‚       â”‚
    â†“           â”‚       â”‚
    â”œâ”€â”€â”€â”€â”€ Ã— â”€â”€â”€â”¤       â”‚  â† é€å…ƒç´ ç›¸ä¹˜ (é—¨æ§æœºåˆ¶)
    â†“                   â”‚
down_proj               â”‚
1408â†’512                â”‚
    â†“                   â”‚
è¾“å‡º (512 ç»´)           â”‚
```

**ä¼˜åŠ¿**ï¼šé—¨æ§æœºåˆ¶è®©æ¨¡å‹è‡ªå·±å†³å®š"å“ªäº›ä¿¡æ¯è¯¥é€šè¿‡ï¼Œå“ªäº›è¯¥æŠ‘åˆ¶"

---

### ğŸ”‘ éšå–»è§£æï¼šä¸‰ä¸ªæŠ•å½±å±‚çš„ä½œç”¨

æƒ³è±¡å°æ˜åœ¨å†™ä½œæ–‡ï¼Œçœ‹åˆ°é¢˜ç›®"æ˜¥å¤©"åçš„æ€è€ƒè¿‡ç¨‹ï¼š

#### 1. `gate_proj` (é—¨æ§æŠ•å½±)ï¼š"è¿™ä¸ªè”æƒ³é‡è¦å—ï¼Ÿ"

```python
self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
# 512 â†’ 1408
```

**ä½œç”¨**ï¼šè®¡ç®—æ¯ä¸ªè”æƒ³çš„"é‡è¦æ€§åˆ†æ•°"

**éšå–»**ï¼š

```
å°æ˜çœ‹åˆ°"æ˜¥å¤©"åï¼Œå¤§è„‘å¼€å§‹è¯„ä¼°å„ç§è”æƒ³çš„é‡è¦æ€§ï¼š
- è”æƒ³"èŠ±æœµ" â†’ é‡è¦æ€§åˆ†æ•° 0.9 (éå¸¸ç›¸å…³ï¼)
- è”æƒ³"è´è¶" â†’ é‡è¦æ€§åˆ†æ•° 0.7 (æ¯”è¾ƒç›¸å…³)
- è”æƒ³"å†¬å¤©" â†’ é‡è¦æ€§åˆ†æ•° 0.2 (ä¸å¤ªç›¸å…³)
- è”æƒ³"é‡å­åŠ›å­¦" â†’ é‡è¦æ€§åˆ†æ•° -0.8 (å®Œå…¨ä¸ç›¸å…³ï¼)

ç»è¿‡ SiLU æ¿€æ´»åï¼š
- æ­£åˆ†æ•°è¢«æ”¾å¤§ (ç›¸å…³è”æƒ³æ›´çªå‡º)
- è´Ÿåˆ†æ•°è¢«æŠ‘åˆ¶ä½†ä¸å®Œå…¨å½’é›¶ (ä¿ç•™ä¸€ç‚¹ç‚¹ä¿¡æ¯)
```

#### 2. `up_proj` (ä¸Šè¡ŒæŠ•å½±)ï¼š"æ‰€æœ‰å¯èƒ½çš„è”æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ"

```python
self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
# 512 â†’ 1408
```

**ä½œç”¨**ï¼šç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è”æƒ³ç‰¹å¾

**éšå–»**ï¼š

```
å°æ˜çœ‹åˆ°"æ˜¥å¤©"åï¼Œå¤§è„‘äº§ç”Ÿçš„æ‰€æœ‰åŸå§‹è”æƒ³ï¼š
- ç‰¹å¾1: å…³äºé¢œè‰²çš„è”æƒ³ â†’ 0.5 (ç»¿è‰²ã€ç²‰è‰²...)
- ç‰¹å¾2: å…³äºæ¸©åº¦çš„è”æƒ³ â†’ 0.8 (æ¸©æš–ã€èˆ’é€‚...)
- ç‰¹å¾3: å…³äºæƒ…æ„Ÿçš„è”æƒ³ â†’ 0.3 (å¸Œæœ›ã€æ–°ç”Ÿ...)
- ç‰¹å¾4: å…³äºå£°éŸ³çš„è”æƒ³ â†’ 0.4 (é¸Ÿé¸£ã€æµæ°´...)
...
- ç‰¹å¾1408: å…³äºæŸç§æŠ½è±¡æ¦‚å¿µ â†’ 0.1
```

#### 3. é—¨æ§ä¹˜æ³• (Gate Ã— Up)ï¼š"ç­›é€‰æœ‰ä»·å€¼çš„è”æƒ³"

```python
self.act_fn(self.gate_proj(x)) * self.up_proj(x)
# SiLU(gate) Ã— up
```

**ä½œç”¨**ï¼šç”¨"é‡è¦æ€§åˆ†æ•°"è¿‡æ»¤"åŸå§‹è”æƒ³"

**éšå–»**ï¼š

```
é—¨æ§æœºåˆ¶ = ç­›é€‰å™¨

åŸå§‹è”æƒ³ (up_proj):
[0.5, 0.8, 0.3, 0.4, ..., 0.1]

é‡è¦æ€§åˆ†æ•° (gate_proj ç»è¿‡ SiLU):
[0.9, 0.7, 0.8, 0.2, ..., 0.01]

é€å…ƒç´ ç›¸ä¹˜å:
[0.45, 0.56, 0.24, 0.08, ..., 0.001]
   â†‘     â†‘     â†‘     â†‘
  ç›¸å…³è”æƒ³è¢«ä¿ç•™  ä¸ç›¸å…³çš„å‡ ä¹ä¸º0

ç»“æœï¼š
- "èŠ±æœµ"çš„è”æƒ³: 0.5 Ã— 0.9 = 0.45 (ä¿ç•™å¤§éƒ¨åˆ†)
- "é‡å­åŠ›å­¦"çš„è”æƒ³: 0.1 Ã— 0.01 = 0.001 (å‡ ä¹æ¶ˆå¤±)
```

#### 4. `down_proj` (ä¸‹è¡ŒæŠ•å½±)ï¼š"æ€»ç»“å¹¶è¾“å‡º"

```python
self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
# 1408 â†’ 512
```

**ä½œç”¨**ï¼šå°†ç­›é€‰åçš„è”æƒ³å‹ç¼©å›åŸå§‹ç»´åº¦

**éšå–»**ï¼š

```
ç»è¿‡å‘æ•£æ€è€ƒåï¼Œå°æ˜è„‘ä¸­æœ‰ 1408 ä¸ªè”æƒ³ç¢ç‰‡
éœ€è¦æ•´ç†æˆ 512 ç»´çš„"æ€è€ƒç»“è®º"

ç±»ä¼¼äºï¼š
å¤´è„‘é£æš´é˜¶æ®µï¼šå†™æ»¡ä¸€æ•´å¼ ç™½æ¿çš„æƒ³æ³• (1408 ä¸ª)
    â†“
æ€»ç»“é˜¶æ®µï¼šæç‚¼æˆä¸€æ®µè¯ (512 ç»´)

down_proj çš„ä½œç”¨å°±æ˜¯"å»ç²—å–ç²¾ï¼Œæç‚¼ç²¾å"
```

---

### ğŸ“ ç»´åº¦è®¡ç®—ï¼šä¸ºä»€ä¹ˆæ˜¯ `8/3` å€ï¼Ÿ

```python
if config.intermediate_size is None:
    intermediate_size = int(config.hidden_size * 8 / 3)  # 512 Ã— 8/3 â‰ˆ 1365
    config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)  # å¯¹é½åˆ° 64
    # æœ€ç»ˆ: 1408
```

**å†å²èƒŒæ™¯**ï¼š

| æ¶æ„ | æ‰©å¼ å€æ•° | intermediate_size (hidden=512) |
|------|---------|-------------------------------|
| GPT-2/BERT (ReLU FFN) | 4Ã— | 2048 |
| Llama/GPT-4 (SwiGLU) | 8/3Ã— | 1408 |

**ä¸ºä»€ä¹ˆ SwiGLU ç”¨æ›´å°çš„å€æ•°ï¼Ÿ**

```
ä¼ ç»Ÿ ReLU FFN:
- åªæœ‰ 1 ä¸ªæŠ•å½± (up_proj)
- ä¸­é—´ç»´åº¦: 4 Ã— hidden = 2048
- å‚æ•°é‡: 2 Ã— (512 Ã— 2048) = 2.1M

SwiGLU FFN:
- æœ‰ 3 ä¸ªæŠ•å½± (gate_proj, up_proj, down_proj æœ‰ä¸¤ä¸ª up æ–¹å‘)
- ä¸­é—´ç»´åº¦: 8/3 Ã— hidden = 1408
- å‚æ•°é‡: 3 Ã— (512 Ã— 1408) = 2.16M

ç»“è®ºï¼š
- å‚æ•°é‡å·®ä¸å¤š
- ä½† SwiGLU æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ› (é—¨æ§æœºåˆ¶)
- 8/3 æ˜¯ Llama å›¢é˜Ÿå®éªŒå‡ºçš„æœ€ä½³æ¯”ä¾‹
```

**ä¸ºä»€ä¹ˆå¯¹é½åˆ° 64ï¼Ÿ**

```python
config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
```

**åŸå› **ï¼šGPU çš„ Tensor Core é€šå¸¸ä»¥ 64 æˆ– 128 ä¸ºå•ä½å¤„ç†æ•°æ®

```
ä¸å¯¹é½: intermediate_size = 1365
    â†’ GPU éœ€è¦è¡¥é›¶åˆ° 1408ï¼Œæµªè´¹è®¡ç®—èµ„æº
    â†’ çŸ©é˜µä¹˜æ³•æ•ˆç‡ä½ä¸‹

å¯¹é½å: intermediate_size = 1408 (64 çš„æ•´æ•°å€)
    â†’ GPU å¯ä»¥é«˜æ•ˆåˆ©ç”¨æ‰€æœ‰è®¡ç®—å•å…ƒ
    â†’ é€Ÿåº¦æå‡ 10%-20%
```

---

### ğŸ”¥ æ¿€æ´»å‡½æ•°ï¼šä¸ºä»€ä¹ˆç”¨ SiLU (Swish) è€Œä¸æ˜¯ ReLUï¼Ÿ

```python
self.act_fn = ACT2FN[config.hidden_act]  # é€šå¸¸æ˜¯ 'silu'
```

#### ReLU çš„é—®é¢˜ï¼š"ä¸€åˆ€åˆ‡"

```python
def relu(x):
    return max(0, x)

è¾“å…¥: [-2, -1, 0, 1, 2]
è¾“å‡º: [ 0,  0, 0, 1, 2]
        â†‘  â†‘
      ä¿¡æ¯å®Œå…¨ä¸¢å¤±!
```

**éšå–»**ï¼š

```
ReLU è€å¸ˆæ‰¹æ”¹ä½œæ–‡ï¼š
- åˆ†æ•° > 0ï¼š"åŠæ ¼ï¼ä¿ç•™ä½ çš„åˆ†æ•°"
- åˆ†æ•° â‰¤ 0ï¼š"é›¶åˆ†ï¼æ»šå»é‡å†™"

é—®é¢˜ï¼š
- å¾ˆå¤šæœ‰ä»·å€¼çš„"æ¥è¿‘åŠæ ¼"çš„æƒ³æ³•è¢«ç›´æ¥æŠ¹æ€
- "ç¥ç»å…ƒæ­»äº¡"é—®é¢˜ï¼šå¦‚æœæŸä¸ªç¥ç»å…ƒæ€»æ˜¯è¾“å‡ºè´Ÿæ•°ï¼Œå®ƒå°±æ°¸è¿œä¸ä¼šè¢«æ¿€æ´»
```

#### SiLU çš„ä¼˜åŠ¿ï¼š"æŸ”æ€§ç­›é€‰"

```python
def silu(x):
    return x * sigmoid(x)

è¾“å…¥: [-2,   -1,    0,   1,    2]
è¾“å‡º: [-0.24, -0.27, 0, 0.73, 1.76]
        â†‘      â†‘
     ä¿ç•™äº†ä¸€ç‚¹è´Ÿæ•°ä¿¡æ¯ï¼
```

**éšå–»**ï¼š

```
SiLU è€å¸ˆæ‰¹æ”¹ä½œæ–‡ï¼š
- é«˜åˆ† (x >> 0)ï¼šç»™äºˆé«˜åº¦è‚¯å®šï¼Œåˆ†æ•°å‡ ä¹ä¸å˜
- ä¸­ç­‰åˆ† (x â‰ˆ 0)ï¼šè°¨æ…è¯„ä¼°ï¼Œé€‚å½“ç¼©å‡
- ä½åˆ† (x < 0)ï¼šç»™äºˆæ”¹è¿›å»ºè®®ï¼Œä¿ç•™ä¸€ç‚¹ç‚¹åé¦ˆä¿¡æ¯

ä¼˜åŠ¿ï¼š
- å¹³æ»‘å¯å¯¼ï¼Œæ¢¯åº¦æµåŠ¨æ›´å¥½
- è´Ÿæ•°åŒºåŸŸä»æœ‰å¾®å°æ¢¯åº¦ï¼Œé¿å…"ç¥ç»å…ƒæ­»äº¡"
- å®éªŒè¯æ˜ï¼ŒSiLU æ¯” ReLU æå‡ 1-2% å‡†ç¡®ç‡
```

**æ•°å­¦å…¬å¼å¯¹æ¯”**ï¼š

```
ReLU:  f(x) = max(0, x)
       å¯¼æ•°: x > 0 â†’ 1; x â‰¤ 0 â†’ 0

SiLU:  f(x) = x Ã— sigmoid(x) = x Ã— (1 / (1 + e^(-x)))
       å¯¼æ•°: å¤„å¤„å¯å¯¼ï¼Œè´Ÿæ•°åŒºåŸŸä¹Ÿæœ‰å¾®å°æ¢¯åº¦
```

---

### ğŸ’¡ å®Œæ•´å‰å‘ä¼ æ’­è¿‡ç¨‹

```python
def forward(self, x):
    return self.dropout(
        self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
    )
```

**æ­¥éª¤åˆ†è§£**ï¼š

```
Step 1: è¾“å…¥
x.shape = [batch_size, seq_len, hidden_size]
x.shape = [32, 128, 512]

Step 2: é—¨æ§æŠ•å½±
gate = self.gate_proj(x)
gate.shape = [32, 128, 1408]

Step 3: SiLU æ¿€æ´»
gate = self.act_fn(gate)  # SiLU
gate.shape = [32, 128, 1408]

Step 4: ä¸Šè¡ŒæŠ•å½±
up = self.up_proj(x)
up.shape = [32, 128, 1408]

Step 5: é—¨æ§ä¹˜æ³• (æ ¸å¿ƒï¼)
hidden = gate * up  # é€å…ƒç´ ç›¸ä¹˜
hidden.shape = [32, 128, 1408]

Step 6: ä¸‹è¡ŒæŠ•å½± (å‹ç¼©)
output = self.down_proj(hidden)
output.shape = [32, 128, 512]  # æ¢å¤åŸå§‹ç»´åº¦

Step 7: Dropout (è®­ç»ƒæ—¶)
output = self.dropout(output)
output.shape = [32, 128, 512]
```

---

### ğŸ“Š ä¸ Attention çš„åˆ†å·¥

```mermaid
graph LR
    A[è¾“å…¥ Token å‘é‡] --> B[Attention]
    B --> C[æ•æ‰è¯ä¸è¯çš„å…³ç³»]
    C --> D[FeedForward]
    D --> E[æ·±åº¦åŠ å·¥æ¯ä¸ªè¯çš„ç‰¹å¾]
    E --> F[è¾“å‡ºåˆ°ä¸‹ä¸€å±‚]
```

**ç±»æ¯”**ï¼š

```
Attention = ç¤¾äº¤èƒ½åŠ›
- çœ‹çœ‹å‘¨å›´çš„äººåœ¨åšä»€ä¹ˆ
- ç†è§£ä¸Šä¸‹æ–‡å…³ç³»
- "å°çº¢å¸½"çœ‹åˆ°"å¤§ç°ç‹¼"ï¼ŒçŸ¥é“æœ‰å±é™©

FeedForward = ç‹¬ç«‹æ€è€ƒèƒ½åŠ›
- å¯¹çœ‹åˆ°çš„ä¿¡æ¯è¿›è¡Œæ·±åº¦åŠ å·¥
- èåˆèƒŒæ™¯çŸ¥è¯†
- "å±é™©"è¿™ä¸ªæ¦‚å¿µ â†’ è”æƒ³åˆ°"é€ƒè·‘"ã€"å¤–å©†å®¶"ã€"çŒäºº"...
```

---

### ğŸ”— åœ¨ Transformer Block ä¸­çš„ä½ç½®

```python
# MiniMindBlock ä¸­çš„ä»£ç  (ç¬¬ 1019 è¡Œ)
self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
```

**å®Œæ•´æµç¨‹**ï¼š

```
è¾“å…¥
  â†“
RMSNorm (å½’ä¸€åŒ–)
  â†“
Attention (æ•æ‰å…³ç³»)
  â†“
Residual Add (æ®‹å·®è¿æ¥)
  â†“
RMSNorm (å½’ä¸€åŒ–)
  â†“
FeedForward (æ·±åº¦æ€è€ƒ)  â† æˆ‘ä»¬åˆšå­¦çš„è¿™ä¸ªï¼
  â†“
Residual Add (æ®‹å·®è¿æ¥)
  â†“
è¾“å‡ºåˆ°ä¸‹ä¸€å±‚
```

---

### âš¡ å‚æ•°é‡è®¡ç®—

```python
# å‡è®¾ hidden_size=512, intermediate_size=1408

# gate_proj: 512 Ã— 1408 = 720,896 å‚æ•°
# up_proj:   512 Ã— 1408 = 720,896 å‚æ•°
# down_proj: 1408 Ã— 512 = 720,896 å‚æ•°
# æ€»è®¡: 2,162,688 å‚æ•° (çº¦ 2.16M)

# å¯¹æ¯”ä¼ ç»Ÿ ReLU FFN (intermediate_size=2048):
# up_proj:   512 Ã— 2048 = 1,048,576 å‚æ•°
# down_proj: 2048 Ã— 512 = 1,048,576 å‚æ•°
# æ€»è®¡: 2,097,152 å‚æ•° (çº¦ 2.1M)

# ç»“è®ºï¼šSwiGLU å‚æ•°é‡ç•¥å¤š (~3%)ï¼Œä½†æ€§èƒ½æå‡æ˜æ˜¾
```

---

### ğŸ’¼ å®é™…åº”ç”¨å»ºè®®

#### åœºæ™¯ 1: å°æ¨¡å‹ (hidden_size < 1024)

```python
config = MiniMindConfig(
    hidden_size=512,
    intermediate_size=None,  # è‡ªåŠ¨è®¡ç®—ä¸º 1408
    hidden_act="silu",       # ä½¿ç”¨ SiLU
    dropout=0.0,             # å¤§è§„æ¨¡æ•°æ®ä¸éœ€è¦ dropout
)
```

#### åœºæ™¯ 2: è°ƒä¼˜ä¸­é—´å±‚å¤§å°

```python
# å¦‚æœæ˜¾å­˜ç´§å¼ ï¼Œå¯ä»¥æ‰‹åŠ¨å‡å° intermediate_size
config = MiniMindConfig(
    hidden_size=512,
    intermediate_size=1024,  # 2Ã— è€Œä¸æ˜¯ 2.67Ã—
    # å‚æ•°é‡å‡å°‘çº¦ 30%ï¼Œä½†æ€§èƒ½å¯èƒ½ç•¥é™
)
```

#### åœºæ™¯ 3: ä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°

```python
# å°è¯•å…¶ä»–æ¿€æ´»å‡½æ•°
config = MiniMindConfig(
    hidden_act="gelu",   # BERT ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°
    # æˆ– "relu" (ä¼ ç»Ÿæ–¹æ³•)
    # æˆ– "silu" (Llama é»˜è®¤ï¼Œæ¨è)
)
```

---

### ğŸ“ ç±»æ¯”æ€»ç»“

```
FeedForward å±‚å°±åƒå°æ˜çš„"è”æƒ³æ€è€ƒ"è¿‡ç¨‹ï¼š

1. å‘æ•£é˜¶æ®µ (gate_proj + up_proj):
   - çœ‹åˆ°"æ˜¥å¤©"ï¼Œè„‘æµ·ä¸­æµ®ç° 1408 ç§è”æƒ³
   - åŒæ—¶è¯„ä¼°æ¯ç§è”æƒ³çš„é‡è¦æ€§

2. ç­›é€‰é˜¶æ®µ (é—¨æ§ä¹˜æ³•):
   - ç”¨"é‡è¦æ€§åˆ†æ•°"è¿‡æ»¤è”æƒ³
   - ç›¸å…³çš„ä¿ç•™ï¼Œä¸ç›¸å…³çš„æŠ‘åˆ¶

3. æ”¶æ•›é˜¶æ®µ (down_proj):
   - æŠŠ 1408 ä¸ªè”æƒ³ç¢ç‰‡
   - æ€»ç»“æˆ 512 ç»´çš„"æ€è€ƒç»“è®º"

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå«"å‰é¦ˆ"â€”â€”
ä¿¡æ¯åªæœä¸€ä¸ªæ–¹å‘æµåŠ¨ï¼Œæ²¡æœ‰å¾ªç¯ï¼Œ
åƒæµæ°´çº¿ä¸€æ ·é«˜æ•ˆå¤„ç†æ¯ä¸ªè¯çš„ç‰¹å¾ã€‚
```

---

## å…­ã€å°æ˜çš„"éŸ³é‡å¹³è¡¡å™¨"ï¼šRMSNorm å‡æ–¹æ ¹å½’ä¸€åŒ–

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 552-605 è¡Œ)

```python
class RMSNorm(torch.nn.Module):
    """
    å‡æ–¹æ ¹å±‚å½’ä¸€åŒ– (Root Mean Square Layer Normalization, RMSNorm)ã€‚
    """

    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼Œå½¢çŠ¶ä¸º [dim]
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # RMS(x) = sqrt(mean(x^2) + eps)
        # è¿”å› x / RMS(x)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # åœ¨ FP32 ç²¾åº¦ä¸‹è®¡ç®—ï¼Œç„¶åè½¬å›åŸç²¾åº¦
        return self.weight * self._norm(x.float()).type_as(x)
```

---

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šæ•™å®¤é‡Œçš„"éŸ³é‡å¹³è¡¡å™¨"

æƒ³è±¡å°æ˜æ­£åœ¨ä¸€ä¸ªæ•™å®¤é‡Œå¬è¯¾ï¼Œæ•™å®¤é‡Œæœ‰å¾ˆå¤šå­¦ç”Ÿï¼Œæ¯ä¸ªå­¦ç”Ÿéƒ½åœ¨å‘è¨€ï¼Œä½†é—®é¢˜æ˜¯ï¼š

**é—®é¢˜åœºæ™¯**ï¼š

```
æ•™å®¤é‡Œ 512 ä¸ªå­¦ç”ŸåŒæ—¶è¯´è¯ (å¯¹åº” hidden_size=512):

å­¦ç”ŸA (ç‰¹å¾1): å£°éŸ³å¾ˆå¤§ï¼Œå–Šå«çº§åˆ« â†’ æ•°å€¼ = 100.0
å­¦ç”ŸB (ç‰¹å¾2): æ­£å¸¸è¯´è¯ â†’ æ•°å€¼ = 1.0
å­¦ç”ŸC (ç‰¹å¾3): è½»å£°ç»†è¯­ â†’ æ•°å€¼ = 0.001
å­¦ç”ŸD (ç‰¹å¾4): å‡ ä¹å¬ä¸è§ â†’ æ•°å€¼ = 0.00001

é—®é¢˜:
1. å¤§å£°å–Šå«çš„å­¦ç”Ÿä¼š"æ·¹æ²¡"å…¶ä»–äººçš„å£°éŸ³
2. ç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¼šè¢«å¤§æ•°å€¼ä¸»å¯¼
3. æ¨¡å‹éš¾ä»¥å­¦ä¹ åˆ°ç»†å¾®çš„ç‰¹å¾å·®å¼‚
```

**è§£å†³æ–¹æ¡ˆ**ï¼š**å½’ä¸€åŒ– = å®‰è£…ä¸€ä¸ª"æ™ºèƒ½éŸ³é‡å¹³è¡¡å™¨"**

```
å½’ä¸€åŒ–å:
å­¦ç”ŸA: åŸæ¥ 100.0 â†’ ç°åœ¨ 0.8 (è¢«å‹ä½)
å­¦ç”ŸB: åŸæ¥ 1.0 â†’ ç°åœ¨ 0.3 (é€‚å½“è°ƒæ•´)
å­¦ç”ŸC: åŸæ¥ 0.001 â†’ ç°åœ¨ 0.2 (è¢«æ”¾å¤§)
å­¦ç”ŸD: åŸæ¥ 0.00001 â†’ ç°åœ¨ 0.1 (è¢«å¤§å¹…æ”¾å¤§)

ç»“æœ: æ¯ä¸ªå­¦ç”Ÿçš„"ç›¸å¯¹éŸ³é‡"è¢«ä¿ç•™ï¼Œä½†ç»å¯¹æ•°å€¼èŒƒå›´å˜å¾—å¯æ§
```

---

### ğŸ§® æ•°å­¦åŸç†ï¼šRMSNorm vs LayerNorm

#### ä¼ ç»Ÿ LayerNorm (BERT/GPT-2 ä½¿ç”¨)

```python
# LayerNorm çš„è®¡ç®—å…¬å¼
def layer_norm(x):
    mean = x.mean(-1, keepdim=True)      # æ­¥éª¤1: è®¡ç®—å‡å€¼
    var = x.var(-1, keepdim=True)         # æ­¥éª¤2: è®¡ç®—æ–¹å·®
    x_norm = (x - mean) / sqrt(var + eps) # æ­¥éª¤3: å‡å‡å€¼ï¼Œé™¤æ ‡å‡†å·®
    return gamma * x_norm + beta          # æ­¥éª¤4: ç¼©æ”¾å’Œåç§»
```

**ç±»æ¯”**ï¼š

```
LayerNorm å°±åƒ"å…¨é¢çš„æˆç»©è°ƒæ•´":
1. å…ˆè®¡ç®—ç­çº§å¹³å‡åˆ† (mean)
2. å†è®¡ç®—åˆ†æ•°æ³¢åŠ¨å¹…åº¦ (variance)
3. æ¯ä¸ªäººçš„åˆ†æ•°å‡å»å¹³å‡åˆ†ï¼Œå†é™¤ä»¥æ³¢åŠ¨å¹…åº¦
4. æœ€åä¹˜ä»¥æƒé‡ã€åŠ ä¸Šåç½®

é—®é¢˜: è®¡ç®— mean å’Œ var ä¸¤ä¸ªç»Ÿè®¡é‡ï¼Œè®¡ç®—é‡è¾ƒå¤§
```

#### ç°ä»£ RMSNorm (Llama/GPT-4 ä½¿ç”¨)

```python
# RMSNorm çš„è®¡ç®—å…¬å¼
def rms_norm(x):
    rms = sqrt(mean(x^2) + eps)  # åªè®¡ç®—å‡æ–¹æ ¹
    x_norm = x / rms              # ç›´æ¥é™¤ä»¥ RMS
    return gamma * x_norm         # åªæœ‰ç¼©æ”¾ï¼Œæ²¡æœ‰åç½®
```

**ç±»æ¯”**ï¼š

```
RMSNorm å°±åƒ"ç®€åŒ–çš„éŸ³é‡è°ƒèŠ‚":
1. è®¡ç®—æ‰€æœ‰å£°éŸ³çš„"èƒ½é‡"å¹³å‡å€¼ (mean of squares)
2. å¼€æ–¹å¾—åˆ°"å‡æ–¹æ ¹éŸ³é‡" (RMS)
3. æŠŠæ¯ä¸ªäººçš„å£°éŸ³é™¤ä»¥è¿™ä¸ªå‡æ–¹æ ¹

ä¼˜åŠ¿:
- çœç•¥äº†"å‡å»å‡å€¼"è¿™ä¸€æ­¥ â†’ è®¡ç®—é‡å‡å°‘
- çœç•¥äº†åç½®é¡¹ (beta) â†’ å‚æ•°æ›´å°‘
- å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§æ¨¡å‹ä¸­æ•ˆæœç›¸å½“ç”šè‡³æ›´å¥½ï¼
```

---

### ğŸ”¢ ä»£ç é€è¡Œè§£æ

#### 1. åˆå§‹åŒ– (`__init__`)

```python
def __init__(self, dim: int, eps: float = 1e-5):
    super().__init__()
    self.eps = eps  # é˜²æ­¢é™¤é›¶çš„å°æ•°
    self.weight = nn.Parameter(torch.ones(dim))  # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
```

**å‚æ•°è§£é‡Š**ï¼š

| å‚æ•° | å€¼ | ä½œç”¨ | éšå–» |
|------|-----|------|------|
| `dim` | 512 | è¾“å…¥ç‰¹å¾ç»´åº¦ | æ•™å®¤é‡Œæœ‰ 512 ä¸ªå­¦ç”Ÿ |
| `eps` | 1e-5 | é˜²æ­¢é™¤é›¶ | éŸ³é‡è°ƒèŠ‚å™¨çš„"æœ€å°éŸ³é‡é˜ˆå€¼" |
| `weight` | [1,1,...,1] (512ä¸ª) | å¯å­¦ä¹ çš„ç¼©æ”¾å› å­ | æ¯ä¸ªå­¦ç”Ÿçš„"éº¦å…‹é£å¢ç›Š" |

**ä¸ºä»€ä¹ˆ `weight` åˆå§‹åŒ–ä¸ºå…¨ 1ï¼Ÿ**

```
åˆå§‹çŠ¶æ€:
weight = [1, 1, 1, ..., 1]

ä½œç”¨:
- å½’ä¸€åŒ–åï¼Œè¾“å‡º = è¾“å…¥ Ã— 1 = è¾“å…¥ (ä¸æ”¹å˜)
- è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œweight ä¼šå­¦ä¹ è°ƒæ•´
- æœ€ç»ˆï¼Œé‡è¦ç‰¹å¾çš„ weight ä¼šå˜å¤§ï¼Œä¸é‡è¦çš„ä¼šå˜å°

ç±»æ¯”:
ä¸€å¼€å§‹æ‰€æœ‰å­¦ç”Ÿçš„éº¦å…‹é£éƒ½æ˜¯é»˜è®¤éŸ³é‡
è®­ç»ƒåï¼Œ"é‡è¦å‘è¨€"çš„å­¦ç”Ÿéº¦å…‹é£ä¼šè¢«è°ƒå¤§
```

---

#### 2. æ ¸å¿ƒå½’ä¸€åŒ–å‡½æ•° (`_norm`)

```python
def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

**åˆ†æ­¥è§£æ**ï¼š

```python
# å‡è®¾è¾“å…¥ x.shape = [batch_size, seq_len, hidden_size]
# ä¾‹å¦‚ x.shape = [32, 128, 512]

# Step 1: å¹³æ–¹
x_squared = x.pow(2)
# x_squared.shape = [32, 128, 512]
# æ¯ä¸ªå…ƒç´ éƒ½è¢«å¹³æ–¹

# Step 2: æ²¿æœ€åä¸€ç»´æ±‚å‡å€¼ (Mean Square)
mean_square = x_squared.mean(-1, keepdim=True)
# mean_square.shape = [32, 128, 1]
# 512 ä¸ªç‰¹å¾çš„å¹³æ–¹å€¼è¢«å¹³å‡æˆ 1 ä¸ªæ•°

# Step 3: åŠ ä¸Š epsilon é˜²æ­¢é™¤é›¶
safe_ms = mean_square + self.eps
# safe_ms.shape = [32, 128, 1]

# Step 4: è®¡ç®—å¹³æ–¹æ ¹çš„å€’æ•° (rsqrt = 1/sqrt)
scale = torch.rsqrt(safe_ms)
# scale.shape = [32, 128, 1]
# è¿™å°±æ˜¯ 1 / RMS(x)

# Step 5: ä¹˜ä»¥åŸè¾“å…¥
x_norm = x * scale
# x_norm.shape = [32, 128, 512]
# å¹¿æ’­æœºåˆ¶ï¼š[32,128,512] * [32,128,1] â†’ [32,128,512]
```

**éšå–»**ï¼š

```
Step 1: æµ‹é‡æ¯ä¸ªå­¦ç”Ÿè¯´è¯çš„"èƒ½é‡" (å¹³æ–¹)
- ä¸ç®¡æ˜¯æ­£æ•°è¿˜æ˜¯è´Ÿæ•°ï¼Œèƒ½é‡éƒ½æ˜¯æ­£çš„
- å–Šå« (100) â†’ èƒ½é‡ 10000
- è½»è¯­ (0.01) â†’ èƒ½é‡ 0.0001

Step 2: è®¡ç®—å…¨ç­çš„"å¹³å‡èƒ½é‡"
- æŠŠ 512 ä¸ªå­¦ç”Ÿçš„èƒ½é‡åŠ èµ·æ¥ï¼Œé™¤ä»¥ 512

Step 3-4: è®¡ç®—"éŸ³é‡è°ƒèŠ‚ç³»æ•°"
- 1 / sqrt(å¹³å‡èƒ½é‡)
- å¹³å‡èƒ½é‡è¶Šå¤§ï¼Œè°ƒèŠ‚ç³»æ•°è¶Šå° (å‹ä½æ•´ä½“éŸ³é‡)

Step 5: åº”ç”¨è°ƒèŠ‚
- æ¯ä¸ªå­¦ç”Ÿçš„å£°éŸ³ Ã— è°ƒèŠ‚ç³»æ•°
- æ•´ä½“éŸ³é‡è¢«"æ‹‰å¹³"åˆ°å¯æ§èŒƒå›´
```

---

#### 3. å‰å‘ä¼ æ’­ (`forward`)

```python
def forward(self, x):
    return self.weight * self._norm(x.float()).type_as(x)
```

**åˆ†æ­¥è§£æ**ï¼š

```python
# Step 1: è½¬æ¢ä¸º FP32 ç²¾åº¦
x_fp32 = x.float()
# åŸå› ï¼šå½’ä¸€åŒ–è®¡ç®—æ¶‰åŠæ±‚å’Œã€é™¤æ³•ï¼ŒFP16 å®¹æ˜“æº¢å‡º

# Step 2: æ‰§è¡Œå½’ä¸€åŒ–
x_norm = self._norm(x_fp32)

# Step 3: è½¬å›åŸå§‹ç²¾åº¦
x_norm = x_norm.type_as(x)
# å¦‚æœè¾“å…¥æ˜¯ FP16/BF16ï¼Œè¾“å‡ºä¹Ÿæ˜¯ FP16/BF16

# Step 4: ä¹˜ä»¥å¯å­¦ä¹ æƒé‡
output = self.weight * x_norm
# self.weight.shape = [512]
# x_norm.shape = [32, 128, 512]
# å¹¿æ’­åï¼šoutput.shape = [32, 128, 512]
```

**ä¸ºä»€ä¹ˆè¦ç”¨ FP32 è®¡ç®—ï¼Ÿ**

```
é—®é¢˜åœºæ™¯:
å‡è®¾è¾“å…¥æœ‰ 512 ä¸ªæ•°ï¼Œå¹³å‡å€¼çº¦ 1.0
åœ¨ FP16 ä¸­:
- æ±‚å’Œ: 1.0 + 1.0 + ... (512æ¬¡) = 512.0 âœ“
- å¹³æ–¹åæ±‚å’Œ: 1.0Â² + 1.0Â² + ... = 512.0 âœ“

ä½†å¦‚æœæ•°å€¼èŒƒå›´å¤§:
- å¤§æ•°: 1000.0Â² = 1,000,000 (FP16 æœ€å¤§çº¦ 65504ï¼Œæº¢å‡ºï¼)
- å°æ•°: 0.0001Â² = 0.00000001 (FP16 ç²¾åº¦ä¸å¤Ÿï¼Œå˜æˆ 0)

è§£å†³æ–¹æ¡ˆ:
åœ¨ FP32 ä¸­è®¡ç®— (èŒƒå›´çº¦ 10^38)ï¼Œè®¡ç®—å®Œå†è½¬å› FP16
```

---

### ğŸ“Š RMSNorm vs LayerNorm å¯¹æ¯”

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| **è®¡ç®—å…¬å¼** | `(x - mean) / std * Î³ + Î²` | `x / RMS(x) * Î³` |
| **ç»Ÿè®¡é‡** | å‡å€¼ + æ–¹å·® (2ä¸ª) | ä»…å‡æ–¹æ ¹ (1ä¸ª) |
| **å¯å­¦ä¹ å‚æ•°** | Î³ (scale) + Î² (bias) | ä»… Î³ (scale) |
| **è®¡ç®—é€Ÿåº¦** | è¾ƒæ…¢ | å¿« ~7-15% |
| **å‚æ•°é‡** | 2 Ã— dim | 1 Ã— dim |
| **ä½¿ç”¨æ¨¡å‹** | BERT, GPT-2 | Llama, GPT-4, Mistral |

**ä¸ºä»€ä¹ˆå¤§æ¨¡å‹æ›´å–œæ¬¢ RMSNormï¼Ÿ**

```
1. è®¡ç®—æ•ˆç‡:
   - LayerNorm éœ€è¦ä¸¤æ¬¡ reduce (mean + var)
   - RMSNorm åªéœ€è¦ä¸€æ¬¡ reduce (mean of squares)
   - åœ¨ 512 ç»´åº¦ä¸Šï¼ŒèŠ‚çœçº¦ 15% è®¡ç®—æ—¶é—´

2. å‚æ•°é‡:
   - LayerNorm: 2 Ã— hidden_size = 1024 å‚æ•°/å±‚
   - RMSNorm: 1 Ã— hidden_size = 512 å‚æ•°/å±‚
   - åœ¨ 32 å±‚æ¨¡å‹ä¸­ï¼Œå‡å°‘çº¦ 16K å‚æ•°

3. æ€§èƒ½:
   - å¤šç¯‡è®ºæ–‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§æ¨¡å‹ä¸­ä¸¤è€…æ€§èƒ½ç›¸å½“
   - æœ‰æ—¶ RMSNorm ç”šè‡³æ›´å¥½ (å¯èƒ½å› ä¸ºæ›´ç®€å•çš„å½’çº³åç½®)
```

---

### ğŸ“ åœ¨ Transformer ä¸­çš„ä½ç½®

```python
# MiniMindBlock ä¸­çš„ä½¿ç”¨ (ç¬¬ 1010-1014 è¡Œ)
self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

**å®Œæ•´æµç¨‹**ï¼š

```
è¾“å…¥
   â†“
â”Œâ”€ RMSNorm (input_layernorm) â† è¿™é‡Œï¼
â”‚     â†“
â”‚  Attention
â”‚     â†“
â””â”€ Residual Add (x + Attention(x))
       â†“
â”Œâ”€ RMSNorm (post_attention_layernorm) â† è¿˜æœ‰è¿™é‡Œï¼
â”‚     â†“
â”‚  FeedForward
â”‚     â†“
â””â”€ Residual Add (x + FFN(x))
       â†“
   è¾“å‡º
```

**ä¸ºä»€ä¹ˆæ¯ä¸ªå­å±‚å‰éƒ½è¦å½’ä¸€åŒ–ï¼Ÿ**

```
ç±»æ¯”: æ¥åŠ›èµ›è·‘

å¦‚æœæ²¡æœ‰å½’ä¸€åŒ–:
- ç¬¬ 1 æ£’é€‰æ‰‹å†²åˆºåˆ°ç»ˆç‚¹ï¼Œé€Ÿåº¦ 10m/s
- æŠŠæ£’ä¼ ç»™ç¬¬ 2 æ£’ï¼Œç¬¬ 2 æ£’ç”¨ 10m/s çš„åˆé€Ÿåº¦ç»§ç»­åŠ é€Ÿ
- ç¬¬ 3 æ£’æ¥åˆ°æ£’æ—¶ï¼Œé€Ÿåº¦å·²ç» 100m/s (çˆ†è¡¨ï¼)
- ç¬¬ 32 æ£’æ—¶...é€Ÿåº¦å·²ç»æ˜¯å…‰é€Ÿçš„ 1000 å€ (æ•°å€¼çˆ†ç‚¸ï¼)

æœ‰äº†å½’ä¸€åŒ–:
- æ¯ä¸€æ£’ç»“æŸåï¼Œå…ˆ"ä¼‘æ¯ä¸€ä¸‹"ï¼ŒæŠŠçŠ¶æ€è°ƒæ•´åˆ°æ­£å¸¸èŒƒå›´
- ç¬¬ 1 æ£’: 10m/s â†’ å½’ä¸€åŒ– â†’ 1.0
- ç¬¬ 2 æ£’: ä» 1.0 å¼€å§‹ï¼Œå†å†²åˆºåˆ° 10m/s â†’ å½’ä¸€åŒ– â†’ 1.0
- æ‰€æœ‰æ£’æ¬¡éƒ½åœ¨å¯æ§èŒƒå›´å†…
```

---

### ğŸ’¡ epsilon (eps) çš„ä½œç”¨

```python
self.eps = eps  # é»˜è®¤ 1e-5 = 0.00001
```

**é—®é¢˜åœºæ™¯**ï¼š

```python
# å¦‚æœè¾“å…¥å…¨æ˜¯ 0
x = torch.tensor([0.0, 0.0, 0.0, 0.0])

# è®¡ç®— RMS
mean_square = (0Â² + 0Â² + 0Â² + 0Â²) / 4 = 0
rms = sqrt(0) = 0
scale = 1 / 0 = âˆ  # é™¤é›¶é”™è¯¯ï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# åŠ ä¸Šä¸€ä¸ªæå°çš„æ•°
rms = sqrt(0 + 0.00001) = 0.00316...
scale = 1 / 0.00316 = 316.2  # å®‰å…¨ï¼

# è¾“å‡º
x_norm = [0, 0, 0, 0] * 316.2 = [0, 0, 0, 0]  # ä»ç„¶æ˜¯ 0ï¼Œç¬¦åˆé¢„æœŸ
```

**eps çš„é€‰æ‹©**ï¼š

| eps å€¼ | é€‚ç”¨åœºæ™¯ | é£é™© |
|--------|---------|------|
| 1e-5 | é»˜è®¤å€¼ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ | æ—  |
| 1e-6 | æ›´é«˜ç²¾åº¦éœ€æ±‚ | FP16 ä¸‹å¯èƒ½å˜æˆ 0 |
| 1e-4 | æç«¯æ•°å€¼èŒƒå›´ | å¯èƒ½è½»å¾®å½±å“ç²¾åº¦ |

---

### ğŸ”¬ æ•°å€¼ç¨³å®šæ€§ï¼šä¸ºä»€ä¹ˆ FP32 å¾ˆé‡è¦ï¼Ÿ

**å®éªŒå¯¹æ¯”**ï¼š

```python
import torch

# æ¨¡æ‹Ÿ hidden_size=512 çš„ç‰¹å¾å‘é‡
x = torch.randn(512) * 1000  # æ•°å€¼èŒƒå›´è¾ƒå¤§

# FP16 è®¡ç®—
x_fp16 = x.half()
mean_sq_fp16 = x_fp16.pow(2).mean()  # å¯èƒ½æº¢å‡ºï¼
print(f"FP16 mean_square: {mean_sq_fp16}")  # å¯èƒ½æ˜¯ inf æˆ– nan

# FP32 è®¡ç®—
x_fp32 = x.float()
mean_sq_fp32 = x_fp32.pow(2).mean()
print(f"FP32 mean_square: {mean_sq_fp32}")  # æ­£å¸¸æ•°å€¼
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
FP16 mean_square: inf  # çˆ†äº†ï¼
FP32 mean_square: 998234.5  # æ­£å¸¸
```

---

### ğŸ“ å‚æ•°é‡è®¡ç®—

```python
# å‡è®¾ hidden_size=512, num_layers=8

# æ¯å±‚æœ‰ 2 ä¸ª RMSNorm
#   - input_layernorm: 512 å‚æ•°
#   - post_attention_layernorm: 512 å‚æ•°

# åŠ ä¸Šæœ€åä¸€å±‚çš„ norm
#   - model.norm: 512 å‚æ•°

total_rmsnorm_params = 8 * (512 + 512) + 512 = 8704 å‚æ•°

# å¯¹æ¯” LayerNorm:
total_layernorm_params = 8 * (512*2 + 512*2) + 512*2 = 17408 å‚æ•°

# èŠ‚çœäº† 50% çš„å½’ä¸€åŒ–å±‚å‚æ•°ï¼
```

---

### ğŸ¯ ç±»æ¯”æ€»ç»“

```
RMSNorm å°±åƒæ•™å®¤é‡Œçš„"æ™ºèƒ½éŸ³é‡å¹³è¡¡å™¨":

1. é—®é¢˜:
   512 ä¸ªå­¦ç”ŸåŒæ—¶è¯´è¯ï¼Œå£°éŸ³å¤§å°å·®å¼‚æ‚¬æ®Š
   - æœ‰äººåœ¨å–Šå« (æ•°å€¼ 1000)
   - æœ‰äººåœ¨ä½è¯­ (æ•°å€¼ 0.001)

2. è§£å†³æ–¹æ¡ˆ:
   æµ‹é‡å…¨ç­çš„"å¹³å‡èƒ½é‡" â†’ è®¡ç®—ä¸€ä¸ª"è°ƒèŠ‚ç³»æ•°"
   æ¯ä¸ªäººçš„å£°éŸ³éƒ½é™¤ä»¥è¿™ä¸ªç³»æ•°

3. æ•ˆæœ:
   - åŸæœ¬çš„"ç›¸å¯¹å¤§å°"è¢«ä¿ç•™
   - æ•´ä½“æ•°å€¼èŒƒå›´å˜å¾—å¯æ§
   - é˜²æ­¢æŸäº›"å¤§å—“é—¨"ä¸»å¯¼æ•´ä¸ªè¯¾å ‚

4. å¯å­¦ä¹ æƒé‡ (weight):
   - ä¸€å¼€å§‹æ‰€æœ‰å­¦ç”Ÿçš„éº¦å…‹é£å¢ç›Šæ˜¯ 1
   - è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šå­¦ä¹ è°ƒæ•´
   - é‡è¦çš„"å‘è¨€"ä¼šè¢«æ”¾å¤§ï¼Œä¸é‡è¦çš„ä¼šè¢«å‹ä½

5. ä¸ LayerNorm çš„åŒºåˆ«:
   - LayerNorm: å…ˆå‡å»å¹³å‡åˆ†ï¼Œå†é™¤ä»¥æ³¢åŠ¨å¹…åº¦ï¼Œå†ç¼©æ”¾+åç§»
   - RMSNorm: ç›´æ¥é™¤ä»¥"èƒ½é‡å‡æ–¹æ ¹"ï¼Œå†ç¼©æ”¾
   - RMSNorm æ›´ç®€å•ã€æ›´å¿«ã€å‚æ•°æ›´å°‘
```

---

### ğŸ”— ç›¸å…³ä»£ç ä½ç½®

| ç”¨é€” | ä»£ç ä½ç½® | è¯´æ˜ |
|------|---------|------|
| RMSNorm å®šä¹‰ | 552-605 è¡Œ | ç±»å®šä¹‰å’Œå®ç° |
| Attention å‰ä½¿ç”¨ | 1047 è¡Œ | `self.input_layernorm(hidden_states)` |
| FFN å‰ä½¿ç”¨ | 1068 è¡Œ | `self.post_attention_layernorm(hidden_states)` |
| æœ€ç»ˆè¾“å‡ºå‰ä½¿ç”¨ | 1185 è¡Œ | `self.norm(hidden_states)` |

---

## ä¸ƒã€å°æ˜çš„"ç¤¾äº¤èƒ½åŠ›"ï¼šæ³¨æ„åŠ›æœºåˆ¶ (Attention)

### ğŸ“ ä»£ç ç‰‡æ®µ (ç¬¬ 392-549 è¡Œ)

```python
class Attention(nn.Module):
    """
    æ³¨æ„åŠ›æœºåˆ¶æ¨¡å— (Attention Mechanism)ã€‚
    æœ¬å®ç°æ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (Grouped Query Attention, GQA) ä»¥åŠ Flash Attention åŠ é€Ÿã€‚
    åŒæ—¶é›†æˆäº†æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) å’Œ KV Cache æœºåˆ¶ã€‚
    """

    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = (
            args.num_attention_heads
            if args.num_key_value_heads is None
            else args.num_key_value_heads
        )
        assert args.num_attention_heads % self.num_key_value_heads == 0

        self.n_local_heads = args.num_attention_heads  # Query å¤´æ•°
        self.n_local_kv_heads = self.num_key_value_heads  # Key/Value å¤´æ•°
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.hidden_size // args.num_attention_heads

        # çº¿æ€§æŠ•å½±å±‚
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)

        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        self.flash = hasattr(torch.nn.functional, "scaled_dot_product_attention") and args.flash_attn
```

---

### ğŸ¯ éšå–»ç¿»è¯‘ï¼šå°æ˜å‚åŠ "åœ†æ¡Œè®¨è®ºä¼š"

æƒ³è±¡å°æ˜åœ¨å†™å°è¯´æ—¶ï¼Œéœ€è¦ç†è§£å¥å­ä¸­æ¯ä¸ªè¯ä¸å…¶ä»–è¯çš„å…³ç³»ã€‚**æ³¨æ„åŠ›æœºåˆ¶**å°±åƒä¸€åœº"åœ†æ¡Œè®¨è®ºä¼š"ï¼š

**åœºæ™¯è®¾å®š**ï¼š

```
å¥å­: "å°çº¢å¸½ èµ°è¿› æ£®æ— é‡åˆ° äº† å¤§ç°ç‹¼"

å°æ˜çš„å¤§è„‘åŒæ—¶å¤„ç† 6 ä¸ªè¯:
ä½ç½®0: å°çº¢å¸½ (ä¸»è§’)
ä½ç½®1: èµ°è¿›   (åŠ¨ä½œ)
ä½ç½®2: æ£®æ—   (åœ°ç‚¹)
ä½ç½®3: é‡åˆ°   (è½¬æŠ˜åŠ¨ä½œ)
ä½ç½®4: äº†     (è¯­æ°”è¯)
ä½ç½®5: å¤§ç°ç‹¼ (å¦ä¸€ä¸ªè§’è‰²)

é—®é¢˜: å½“å°æ˜æƒ³ç†è§£"é‡åˆ°"è¿™ä¸ªè¯æ—¶ï¼Œåº”è¯¥é‡ç‚¹å…³æ³¨å“ªäº›è¯ï¼Ÿ
```

**æ³¨æ„åŠ›æœºåˆ¶çš„å›ç­”**ï¼š

```
"é‡åˆ°"éœ€è¦å…³æ³¨:
- "å°çº¢å¸½" (è°é‡åˆ°çš„ï¼Ÿ) â†’ æ³¨æ„åŠ›æƒé‡ 0.35
- "å¤§ç°ç‹¼" (é‡åˆ°äº†è°ï¼Ÿ) â†’ æ³¨æ„åŠ›æƒé‡ 0.45
- "æ£®æ—"   (åœ¨å“ªé‡Œé‡åˆ°çš„ï¼Ÿ) â†’ æ³¨æ„åŠ›æƒé‡ 0.15
- "èµ°è¿›"   (ä¹‹å‰å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ) â†’ æ³¨æ„åŠ›æƒé‡ 0.05
- "äº†"     (è¯­æ³•è¾…åŠ©) â†’ æ³¨æ„åŠ›æƒé‡ 0.00

ç»“æœ: "é‡åˆ°"çš„è¯­ä¹‰ = 35%çš„å°çº¢å¸½ä¿¡æ¯ + 45%çš„å¤§ç°ç‹¼ä¿¡æ¯ + 15%çš„æ£®æ—ä¿¡æ¯ + ...
```

---

### ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µï¼šQueryã€Keyã€Value (QKV)

è¿™æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ä¸‰ä¸ªæ ¸å¿ƒè§’è‰²ï¼Œç”¨"å›¾ä¹¦é¦†æŸ¥è¯¢"æ¥ç±»æ¯”æœ€ç›´è§‚ï¼š

#### åœºæ™¯ï¼šå»å›¾ä¹¦é¦†æ‰¾ä¹¦

```
ä½ æƒ³æ‰¾ä¸€æœ¬å…³äº"ç«¥è¯æ•…äº‹"çš„ä¹¦:

1. Query (æŸ¥è¯¢/é—®é¢˜): ä½ è„‘æµ·ä¸­çš„éœ€æ±‚
   "æˆ‘æƒ³æ‰¾ç«¥è¯æ•…äº‹ç›¸å…³çš„ä¹¦"
   
2. Key (ç´¢å¼•/ç›®å½•å¡): ä¹¦æ¶ä¸Šæ¯æœ¬ä¹¦çš„æ ‡ç­¾
   "æ ¼æ—ç«¥è¯"ã€"å®‰å¾’ç”Ÿç«¥è¯"ã€"å“ˆåˆ©æ³¢ç‰¹"ã€"é«˜ç­‰æ•°å­¦"...
   
3. Value (å†…å®¹/ä¹¦æœ¬èº«): æ¯æœ¬ä¹¦çš„å®é™…å†…å®¹
   ã€Šæ ¼æ—ç«¥è¯ã€‹çš„æ•…äº‹å†…å®¹ã€ã€Šé«˜ç­‰æ•°å­¦ã€‹çš„å…¬å¼å†…å®¹...

æŸ¥è¯¢è¿‡ç¨‹:
a) ç”¨ä½ çš„ Query å’Œæ¯ä¸ª Key è¿›è¡ŒåŒ¹é… (è®¡ç®—ç›¸ä¼¼åº¦)
   - "ç«¥è¯æ•…äº‹" vs "æ ¼æ—ç«¥è¯" â†’ åŒ¹é…åº¦ 0.9 (å¾ˆé«˜!)
   - "ç«¥è¯æ•…äº‹" vs "é«˜ç­‰æ•°å­¦" â†’ åŒ¹é…åº¦ 0.0 (å®Œå…¨ä¸ç›¸å…³)
   
b) æ ¹æ®åŒ¹é…åº¦ï¼ŒåŠ æƒè·å– Value (ä¹¦çš„å†…å®¹)
   - 90% æ³¨æ„åŠ›åœ¨ã€Šæ ¼æ—ç«¥è¯ã€‹çš„å†…å®¹
   - 0% æ³¨æ„åŠ›åœ¨ã€Šé«˜ç­‰æ•°å­¦ã€‹çš„å†…å®¹

c) è¾“å‡º: ç»¼åˆäº†ç›¸å…³ä¹¦ç±å†…å®¹çš„"ç­”æ¡ˆ"
```

#### ä»£ç ä¸­çš„ QKV æŠ•å½±

```python
# ç¬¬ 420-430 è¡Œ
self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
```

**éšå–»è§£é‡Š**ï¼š

| æŠ•å½±å±‚ | ä½œç”¨ | ç±»æ¯” |
|-------|------|------|
| `q_proj` | æŠŠè¾“å…¥å˜æˆ"é—®é¢˜" | æŠŠä½ çš„éœ€æ±‚ç¿»è¯‘æˆå›¾ä¹¦é¦†èƒ½ç†è§£çš„æŸ¥è¯¢è¯­è¨€ |
| `k_proj` | æŠŠè¾“å…¥å˜æˆ"ç´¢å¼•" | ç»™æ¯ä¸ªè¯è´´ä¸Šæ ‡ç­¾ï¼Œæ–¹ä¾¿è¢«æŸ¥æ‰¾ |
| `v_proj` | æŠŠè¾“å…¥å˜æˆ"å†…å®¹" | æå–æ¯ä¸ªè¯çš„æ ¸å¿ƒä¿¡æ¯ï¼Œå‡†å¤‡è¢«è¯»å– |

**ç»´åº¦è¯´æ˜**ï¼š

```python
# å‡è®¾ hidden_size=512, num_attention_heads=8, num_key_value_heads=2
# head_dim = 512 / 8 = 64

q_proj: [512] â†’ [8 * 64] = [512]  # 8 ä¸ª Query å¤´ï¼Œæ¯ä¸ª 64 ç»´
k_proj: [512] â†’ [2 * 64] = [128]  # 2 ä¸ª Key å¤´ï¼Œæ¯ä¸ª 64 ç»´ (GQA)
v_proj: [512] â†’ [2 * 64] = [128]  # 2 ä¸ª Value å¤´ï¼Œæ¯ä¸ª 64 ç»´ (GQA)
```

---

### ğŸ§  å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)

#### ä¸ºä»€ä¹ˆéœ€è¦"å¤šä¸ªå¤´"ï¼Ÿ

**é—®é¢˜åœºæ™¯**ï¼š

```
å¥å­: "å°çº¢å¸½çš„å¤–å©†ä½åœ¨æ£®æ—æ·±å¤„çš„å°æœ¨å±‹é‡Œ"

å•å¤´æ³¨æ„åŠ›çš„å±€é™:
- åªèƒ½å…³æ³¨ä¸€ç§å…³ç³»æ¨¡å¼
- å¯èƒ½åªå…³æ³¨åˆ°"å°çº¢å¸½"å’Œ"å¤–å©†"çš„äº²å±å…³ç³»
- å¿½ç•¥äº†"å¤–å©†"å’Œ"å°æœ¨å±‹"çš„å±…ä½å…³ç³»
- å¿½ç•¥äº†"æ£®æ—"å’Œ"å°æœ¨å±‹"çš„åœ°ç†å…³ç³»
```

**è§£å†³æ–¹æ¡ˆï¼šå¤šå¤´æ³¨æ„åŠ›**

```
8 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå„å¸å…¶èŒ:

å¤´1 (è¯­æ³•å¤´): å…³æ³¨ä¸»è°“å®¾ç»“æ„
   "å¤–å©†" â†’ "ä½åœ¨" â†’ "å°æœ¨å±‹"

å¤´2 (è¯­ä¹‰å¤´): å…³æ³¨è¯­ä¹‰ç›¸ä¼¼è¯
   "å°çº¢å¸½" â†” "å¤–å©†" (éƒ½æ˜¯äºº)
   "æ£®æ—" â†” "å°æœ¨å±‹" (éƒ½æ˜¯åœ°ç‚¹)

å¤´3 (è·ç¦»å¤´): å…³æ³¨ç›¸é‚»è¯
   "æ£®æ—æ·±å¤„" â†’ "çš„" â†’ "å°æœ¨å±‹"

å¤´4 (å®ä½“å¤´): å…³æ³¨ä¸“æœ‰åè¯
   "å°çº¢å¸½" â†” "å¤–å©†"

å¤´5-8: å…¶ä»–æ¨¡å¼ (æƒ…æ„Ÿã€å› æœã€æ—¶é—´ç­‰)

æœ€ç»ˆ: ç»¼åˆ 8 ä¸ªå¤´çš„å‘ç°ï¼Œå½¢æˆå…¨é¢ç†è§£
```

#### ä»£ç ä¸­çš„å¤šå¤´å®ç°

```python
# ç¬¬ 467-472 è¡Œ
# é‡å¡‘å½¢çŠ¶ï¼Œå°† hidden_size åˆ‡åˆ†ä¸º n_heads * head_dim
xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
```

**å½¢çŠ¶å˜åŒ–**ï¼š

```
è¾“å…¥ x: [batch=32, seq_len=128, hidden=512]
    â†“ q_proj
xq: [32, 128, 512]
    â†“ view (åˆ‡åˆ†ä¸ºå¤šä¸ªå¤´)
xq: [32, 128, 8, 64]
    â†‘     â†‘   â†‘   â†‘
  batch  seq heads head_dim
```

**éšå–»**ï¼š

```
åŸæœ¬: 512 ä¸ªç¥ç»å…ƒä¸€èµ·å¤„ç†
åˆ‡åˆ†å: åˆ†æˆ 8 ç»„ï¼Œæ¯ç»„ 64 ä¸ªç¥ç»å…ƒ

å°±åƒ:
åŸæœ¬: 1 ä¸ªå¤§ä¾¦æ¢è´Ÿè´£å…¨éƒ¨çº¿ç´¢
åˆ‡åˆ†å: 8 ä¸ªå°ä¾¦æ¢ï¼Œæ¯äººè´Ÿè´£ä¸€éƒ¨åˆ†çº¿ç´¢
        ç„¶åæ±‡æ€»å„è‡ªçš„å‘ç°
```

---

### ğŸ”„ æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)

#### ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ä¿¡æ¯ï¼Ÿ

**é—®é¢˜**ï¼š

```
å¥å­1: "ç‹—å’¬äºº" (ç‹—æ˜¯ä¸»è¯­ï¼Œäººæ˜¯å®¾è¯­)
å¥å­2: "äººå’¬ç‹—" (äººæ˜¯ä¸»è¯­ï¼Œç‹—æ˜¯å®¾è¯­)

å¦‚æœæ²¡æœ‰ä½ç½®ä¿¡æ¯:
- ä¸¤ä¸ªå¥å­çš„è¯å‘é‡å®Œå…¨ç›¸åŒ: {ç‹—, å’¬, äºº}
- æ¨¡å‹æ— æ³•åŒºåˆ†è°æ˜¯ä¸»è¯­ã€è°æ˜¯å®¾è¯­!
```

**è§£å†³æ–¹æ¡ˆï¼šRoPE æ—‹è½¬ä½ç½®ç¼–ç **

```python
# ç¬¬ 474-477 è¡Œ
cos, sin = position_embeddings
xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)
```

#### RoPE çš„éšå–»ï¼šç»™åº§ä½ç¼–å·

```
æƒ³è±¡ç”µå½±é™¢çš„åº§ä½:

æ²¡æœ‰ä½ç½®ç¼–ç :
- è§‚ä¼— A å’Œ B éƒ½åœ¨çœ‹ç”µå½±
- ä½†ä¸çŸ¥é“è°ååœ¨å‰æ’ã€è°åœ¨åæ’
- æ— æ³•åˆ¤æ–­"å‰æ’çš„è§‚ä¼—æŒ¡ä½äº†åæ’"è¿™ç§å…³ç³»

æœ‰äº† RoPE:
- è§‚ä¼— A åœ¨ 3 æ’ (ä½ç½® 3)
- è§‚ä¼— B åœ¨ 7 æ’ (ä½ç½® 7)
- ä»–ä»¬ä¹‹é—´çš„"ç›¸å¯¹è·ç¦»"æ˜¯ 4 æ’

RoPE çš„ç‰¹æ®Šä¹‹å¤„:
- æŠŠä½ç½®ä¿¡æ¯"èå…¥"åˆ°å‘é‡ä¸­
- é€šè¿‡"æ—‹è½¬"æ“ä½œï¼Œè®©ç›¸å¯¹ä½ç½®è‡ªç„¶ä½“ç°åœ¨ç‚¹ç§¯ä¸­
- ä½ç½® 3 å’Œä½ç½® 7 çš„ Query/Key æ—‹è½¬åï¼Œç‚¹ç§¯ç»“æœè‡ªåŠ¨åæ˜ äº†"è·ç¦» 4"
```

#### RoPE çš„æ•°å­¦ç›´è§‰

```python
# ç¬¬ 301-352 è¡Œ apply_rotary_pos_emb å‡½æ•°

def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

q_embed = (q * cos) + (rotate_half(q) * sin)
k_embed = (k * cos) + (rotate_half(k) * sin)
```

**æ—‹è½¬çš„å‡ ä½•æ„ä¹‰**ï¼š

```
æŠŠå‘é‡æƒ³è±¡æˆå¤æ•°å¹³é¢ä¸Šçš„ç‚¹:

åŸå§‹å‘é‡ q = [q1, q2] 
         â†’ å¤æ•°è¡¨ç¤º: q1 + i*q2

æ—‹è½¬ Î¸ è§’åº¦:
(q1 + i*q2) Ã— e^(iÎ¸) = (q1 + i*q2) Ã— (cos Î¸ + i*sin Î¸)

å±•å¼€:
å®éƒ¨: q1*cos - q2*sin
è™šéƒ¨: q1*sin + q2*cos

ä»£ç å®ç°:
q * cos = [q1*cos, q2*cos]
rotate_half(q) * sin = [-q2*sin, q1*sin]
ç›¸åŠ : [q1*cos - q2*sin, q2*cos + q1*sin]

è¿™å°±æ˜¯äºŒç»´æ—‹è½¬çŸ©é˜µçš„ç»“æœ!
```

**ä¸ºä»€ä¹ˆæ—‹è½¬èƒ½è¡¨ç¤ºä½ç½®ï¼Ÿ**

```
ä½ç½® 0 çš„å‘é‡: ä¸æ—‹è½¬
ä½ç½® 1 çš„å‘é‡: æ—‹è½¬ Î¸ åº¦
ä½ç½® 2 çš„å‘é‡: æ—‹è½¬ 2Î¸ åº¦
...
ä½ç½® n çš„å‘é‡: æ—‹è½¬ nÎ¸ åº¦

å½“è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯æ—¶:
- Q åœ¨ä½ç½® mï¼Œæ—‹è½¬äº† mÎ¸
- K åœ¨ä½ç½® nï¼Œæ—‹è½¬äº† nÎ¸

ç‚¹ç§¯ç»“æœåªå–å†³äº (m-n)Î¸ï¼Œå³ç›¸å¯¹ä½ç½®!

è¿™å°±æ˜¯ RoPE çš„ç²¾å¦™ä¹‹å¤„:
ç»å¯¹ä½ç½®ç¼–ç  â†’ ç›¸å¯¹ä½ç½®ä¿¡æ¯
```

---

### ğŸ“¦ KV Cacheï¼šæ¨ç†åŠ é€Ÿçš„ç§˜å¯†æ­¦å™¨

#### é—®é¢˜åœºæ™¯

```
ç”Ÿæˆæ–‡æœ¬: "ä»Šå¤©å¤©æ°”..."

Step 1: è¾“å…¥ "ä»Šå¤©å¤©æ°”"ï¼Œç”Ÿæˆ "å¾ˆ"
        è®¡ç®— Q, K, V for ["ä»Šå¤©", "å¤©æ°”"]
        è¾“å‡º: "å¾ˆ"

Step 2: è¾“å…¥ "ä»Šå¤©å¤©æ°”å¾ˆ"ï¼Œç”Ÿæˆ "å¥½"
        é‡æ–°è®¡ç®— Q, K, V for ["ä»Šå¤©", "å¤©æ°”", "å¾ˆ"]  â† æµªè´¹ï¼
        å…¶å® "ä»Šå¤©", "å¤©æ°”" çš„ K, V å·²ç»åœ¨ Step 1 ç®—è¿‡äº†

é—®é¢˜: æ¯ä¸€æ­¥éƒ½é‡å¤è®¡ç®—å†å² token çš„ K, V
     åºåˆ—è¶Šé•¿ï¼Œæµªè´¹è¶Šä¸¥é‡
```

**è§£å†³æ–¹æ¡ˆï¼šKV Cache**

```python
# ç¬¬ 479-486 è¡Œ
# å¦‚æœæä¾›äº†è¿‡å»çš„ KVï¼Œå°†å…¶ä¸å½“å‰çš„ KV æ‹¼æ¥
if past_key_value is not None:
    xk = torch.cat([past_key_value[0], xk], dim=1)
    xv = torch.cat([past_key_value[1], xv], dim=1)

# å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œä¿å­˜å½“å‰çš„å®Œæ•´ KV çŠ¶æ€
past_kv = (xk, xv) if use_cache else None
```

**éšå–»è§£é‡Š**ï¼š

```
KV Cache = åšç¬”è®°

æ²¡æœ‰ Cache:
è€å¸ˆ: "è¯·å›å¿†ä»Šå¤©ä¸Šåˆå­¦çš„å†…å®¹"
å­¦ç”Ÿ: ä»å¤´å¼€å§‹å›å¿†å…¨éƒ¨è¯¾ç¨‹... (æ…¢!)

æœ‰äº† Cache:
è€å¸ˆ: "è¯·å›å¿†ä»Šå¤©ä¸Šåˆå­¦çš„å†…å®¹"
å­¦ç”Ÿ: ç¿»å¼€ç¬”è®°æœ¬ï¼Œç›´æ¥çœ‹è®°å½• (å¿«!)

ä»£ç æµç¨‹:
Step 1: è®¡ç®— K, V for ["ä»Šå¤©", "å¤©æ°”"]
        ä¿å­˜åˆ° Cache: past_kv = (K, V)

Step 2: è¾“å…¥æ–°è¯ "å¾ˆ"
        åªè®¡ç®—æ–°è¯çš„ k, v
        ä» Cache ä¸­å–å‡ºå†å²çš„ K, V
        æ‹¼æ¥: K_new = [K_å†å², k_å½“å‰]
        
èŠ‚çœ: ä» O(nÂ²) é™åˆ° O(n)
```

---

### ğŸ­ åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)

#### é—®é¢˜åœºæ™¯

```
æ ‡å‡† Multi-Head Attention (MHA):
- 8 ä¸ª Query å¤´
- 8 ä¸ª Key å¤´
- 8 ä¸ª Value å¤´

KV Cache å¤§å° = 8 Ã— seq_len Ã— head_dim Ã— 2 (Kå’ŒV)
             = 8 Ã— 2048 Ã— 64 Ã— 2 = 2 MB / layer

32 å±‚æ¨¡å‹: 64 MB æ˜¾å­˜ç”¨äº KV Cache
100K ä¸Šä¸‹æ–‡: 3.2 GB æ˜¾å­˜ç”¨äº KV Cache!
```

**è§£å†³æ–¹æ¡ˆï¼šGQA (Grouped Query Attention)**

```python
# ç¬¬ 404-415 è¡Œ
self.n_local_heads = args.num_attention_heads  # Query å¤´æ•°: 8
self.n_local_kv_heads = self.num_key_value_heads  # KV å¤´æ•°: 2
self.n_rep = self.n_local_heads // self.n_local_kv_heads  # é‡å¤å€æ•°: 4
```

**éšå–»è§£é‡Š**ï¼š

```
åŸæ¥ (MHA):
8 ä¸ªå­¦ç”Ÿ (Query) å„å¸¦ 1 æœ¬ä¸“å±è¯å…¸ (Key+Value)
æ€»å…±éœ€è¦ 8 æœ¬è¯å…¸

ç°åœ¨ (GQA):
8 ä¸ªå­¦ç”Ÿ (Query) å…±ç”¨ 2 æœ¬å…¬å…±è¯å…¸ (Key+Value)
æ¯ 4 ä¸ªå­¦ç”Ÿå…±ç”¨ 1 æœ¬è¯å…¸
æ€»å…±åªéœ€è¦ 2 æœ¬è¯å…¸

èŠ‚çœ: æ˜¾å­˜å‡å°‘ 75% (8â†’2)
ä»£ä»·: æ€§èƒ½å‡ ä¹ä¸é™ (Meta è®ºæ–‡éªŒè¯)
```

#### repeat_kv å‡½æ•°ï¼šå¦‚ä½•"å…±äº«"è¯å…¸

```python
# ç¬¬ 355-389 è¡Œ
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )
```

**å½¢çŠ¶å˜åŒ–å›¾è§£**ï¼š

```
è¾“å…¥ K: [batch, seq, 2, 64]  (2 ä¸ª KV å¤´)
         â†“
æ’å…¥ç»´åº¦: [batch, seq, 2, 1, 64]
         â†“
å¹¿æ’­å¤åˆ¶: [batch, seq, 2, 4, 64]  (æ¯ä¸ªå¤´å¤åˆ¶ 4 æ¬¡)
         â†“
åˆå¹¶ç»´åº¦: [batch, seq, 8, 64]  (å˜æˆ 8 ä¸ªå¤´)

ç»“æœ: 2 ä¸ª KV å¤´è¢«"å¹¿æ’­"æˆ 8 ä¸ªï¼Œä¾› 8 ä¸ª Query å¤´ä½¿ç”¨
```

---

### âš¡ Flash Attentionï¼šé—ªç”µèˆ¬çš„è®¡ç®—é€Ÿåº¦

#### ä¼ ç»Ÿæ³¨æ„åŠ›çš„é—®é¢˜

```python
# ä¼ ç»Ÿå®ç° (ç¬¬ 517-543 è¡Œçš„ else åˆ†æ”¯)
scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
# scores å½¢çŠ¶: [batch, heads, seq_len, seq_len]
# å¦‚æœ seq_len=32768ï¼Œé‚£ä¹ˆ scores å ç”¨:
# 32768 Ã— 32768 Ã— 4 bytes = 4 GB æ˜¾å­˜!
```

**é—®é¢˜**ï¼š

```
è¦å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ:
- å½¢çŠ¶: [seq_len Ã— seq_len]
- 32K åºåˆ—: 1 billion ä¸ªå…ƒç´ 
- æ˜¾å­˜çˆ†ç‚¸!
```

#### Flash Attention çš„è§£å†³æ–¹æ¡ˆ

```python
# ç¬¬ 499-513 è¡Œ
if self.flash and (seq_len > 1) and ...:
    output = F.scaled_dot_product_attention(
        xq, xk, xv,
        dropout_p=self.dropout if self.training else 0.0,
        is_causal=True,
    )
```

**æ ¸å¿ƒæ€æƒ³ï¼šåˆ†å—è®¡ç®— (Tiling)**

```
ä¼ ç»Ÿæ–¹æ³•:
1. è®¡ç®—å®Œæ•´çš„ [32K Ã— 32K] æ³¨æ„åŠ›çŸ©é˜µ
2. å­˜å‚¨åˆ°æ˜¾å­˜
3. åº”ç”¨ Softmax
4. ä¹˜ä»¥ V

Flash Attention:
1. æŠŠçŸ©é˜µåˆ†æˆå°å— (å¦‚ 64Ã—64)
2. æ¯æ¬¡åªè®¡ç®—ä¸€ä¸ªå°å—
3. è®¡ç®—å®Œç«‹å³ä¸¢å¼ƒï¼ŒèŠ‚çœæ˜¾å­˜
4. ç»“æœå®Œå…¨ä¸€è‡´ï¼Œä½†æ˜¾å­˜å ç”¨ä» O(nÂ²) é™åˆ° O(n)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | ä¼ ç»Ÿæ³¨æ„åŠ› | Flash Attention |
|-----|-----------|-----------------|
| **æ˜¾å­˜** | O(nÂ²) | O(n) |
| **é€Ÿåº¦** | 100% | 200-400% |
| **ç²¾åº¦** | å®Œå…¨ä¸€è‡´ | å®Œå…¨ä¸€è‡´ |

---

### ğŸ­ å› æœæ©ç  (Causal Mask)

#### ä¸ºä»€ä¹ˆéœ€è¦å› æœæ©ç ï¼Ÿ

```
ç”Ÿæˆæ–‡æœ¬æ—¶çš„è§„åˆ™:
- åªèƒ½çœ‹åˆ°"å·²ç»ç”Ÿæˆçš„è¯"
- ä¸èƒ½"å·çœ‹æœªæ¥"

ä¾‹å¦‚ç”Ÿæˆ "ä»Šå¤©å¤©æ°”å¾ˆå¥½":
ä½ç½®0 "ä»Šå¤©" åªèƒ½çœ‹åˆ°: [ä»Šå¤©]
ä½ç½®1 "å¤©æ°”" åªèƒ½çœ‹åˆ°: [ä»Šå¤©, å¤©æ°”]
ä½ç½®2 "å¾ˆ"   åªèƒ½çœ‹åˆ°: [ä»Šå¤©, å¤©æ°”, å¾ˆ]
ä½ç½®3 "å¥½"   åªèƒ½çœ‹åˆ°: [ä»Šå¤©, å¤©æ°”, å¾ˆ, å¥½]
```

#### ä»£ç å®ç°

```python
# ç¬¬ 521-527 è¡Œ
# åº”ç”¨å› æœæ©ç 
scores[:, :, :, -seq_len:] += torch.triu(
    torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
    diagonal=1,
)
```

**æ©ç çŸ©é˜µå¯è§†åŒ–**ï¼š

```
                Key ä½ç½®
              0    1    2    3
Query    0   [0   -âˆ   -âˆ   -âˆ]   â† ä½ç½®0åªèƒ½çœ‹ä½ç½®0
ä½ç½®     1   [0    0   -âˆ   -âˆ]   â† ä½ç½®1èƒ½çœ‹0,1
         2   [0    0    0   -âˆ]   â† ä½ç½®2èƒ½çœ‹0,1,2
         3   [0    0    0    0]   â† ä½ç½®3èƒ½çœ‹å…¨éƒ¨

Softmax å:
         0   [1.0  0    0    0 ]
         1   [0.5  0.5  0    0 ]
         2   [0.3  0.3  0.3  0 ]
         3   [0.25 0.25 0.25 0.25]

-âˆ ç»è¿‡ Softmax å˜æˆ 0ï¼Œè¾¾åˆ°"çœ‹ä¸è§"çš„æ•ˆæœ
```

---

### ğŸ“Š å®Œæ•´çš„ Attention æµç¨‹å›¾

```mermaid
graph TD
    A[è¾“å…¥ x: batchÃ—seqÃ—hidden] --> B[çº¿æ€§æŠ•å½±]
    
    B --> C1[Q = q_projÃ—x]
    B --> C2[K = k_projÃ—x]
    B --> C3[V = v_projÃ—x]
    
    C1 --> D1[reshape: batchÃ—seqÃ—headsÃ—dim]
    C2 --> D2[reshape: batchÃ—seqÃ—kv_headsÃ—dim]
    C3 --> D3[reshape: batchÃ—seqÃ—kv_headsÃ—dim]
    
    D1 --> E1[RoPEæ—‹è½¬]
    D2 --> E2[RoPEæ—‹è½¬]
    
    E2 --> F2{æœ‰KV Cache?}
    D3 --> F3{æœ‰KV Cache?}
    
    F2 -->|æ˜¯| G2[æ‹¼æ¥å†å²K]
    F2 -->|å¦| H2[ç›´æ¥ä½¿ç”¨]
    F3 -->|æ˜¯| G3[æ‹¼æ¥å†å²V]
    F3 -->|å¦| H3[ç›´æ¥ä½¿ç”¨]
    
    G2 --> I2[repeat_kvæ‰©å±•]
    H2 --> I2
    G3 --> I3[repeat_kvæ‰©å±•]
    H3 --> I3
    
    E1 --> J[transpose: batchÃ—headsÃ—seqÃ—dim]
    I2 --> J
    I3 --> J
    
    J --> K{Flash Attention?}
    
    K -->|æ˜¯| L1[F.scaled_dot_product_attention]
    K -->|å¦| L2[æ‰‹åŠ¨è®¡ç®— Q@K/âˆšd + mask + softmax + @V]
    
    L1 --> M[è¾“å‡º: batchÃ—headsÃ—seqÃ—dim]
    L2 --> M
    
    M --> N[transpose+reshape: batchÃ—seqÃ—hidden]
    N --> O[o_proj: è¾“å‡ºæŠ•å½±]
    O --> P[Dropout]
    P --> Q[è¾“å‡º + KV Cache]
```

---

### âš¡ å‚æ•°é‡è®¡ç®—

```python
# å‡è®¾ hidden_size=512, num_heads=8, num_kv_heads=2

# q_proj: 512 Ã— 512 = 262,144 å‚æ•°
# k_proj: 512 Ã— 128 = 65,536 å‚æ•°  (GQA èŠ‚çœ!)
# v_proj: 512 Ã— 128 = 65,536 å‚æ•°  (GQA èŠ‚çœ!)
# o_proj: 512 Ã— 512 = 262,144 å‚æ•°

# æ€»è®¡: 655,360 å‚æ•° (0.66M)

# å¯¹æ¯”æ ‡å‡† MHA (num_kv_heads=8):
# k_proj: 512 Ã— 512 = 262,144
# v_proj: 512 Ã— 512 = 262,144
# æ€»è®¡: 1,048,576 å‚æ•° (1.05M)

# GQA èŠ‚çœ: 37.5% çš„ Attention å±‚å‚æ•°!
```

---

### ğŸ’¼ å®é™…åº”ç”¨åœºæ™¯

#### åœºæ™¯ 1: çŸ­æ–‡æœ¬ç”Ÿæˆ (èŠå¤©æœºå™¨äºº)

```python
# seq_len < 512ï¼Œä¸éœ€è¦ç‰¹åˆ«ä¼˜åŒ–
config = MiniMindConfig(
    hidden_size=512,
    num_attention_heads=8,
    num_key_value_heads=8,  # ä½¿ç”¨æ ‡å‡† MHA
    flash_attn=True,
)
```

#### åœºæ™¯ 2: é•¿æ–‡æœ¬å¤„ç† (æ–‡æ¡£åˆ†æ)

```python
# seq_len > 4096ï¼Œæ˜¾å­˜æ˜¯ç“¶é¢ˆ
config = MiniMindConfig(
    hidden_size=1024,
    num_attention_heads=16,
    num_key_value_heads=2,  # ä½¿ç”¨ GQA èŠ‚çœ KV Cache
    flash_attn=True,        # å¿…é¡»å¼€å¯!
    max_position_embeddings=32768,
)
```

#### åœºæ™¯ 3: æµå¼æ¨ç† (å®æ—¶ç”Ÿæˆ)

```python
# ä½¿ç”¨ KV Cache åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆ
past_key_values = None
for _ in range(max_new_tokens):
    outputs = model(
        input_ids=new_token,
        past_key_values=past_key_values,
        use_cache=True,  # å¯ç”¨ KV Cache
    )
    past_key_values = outputs.past_key_values
```

---

### ğŸ“ ç±»æ¯”æ€»ç»“

```
æ³¨æ„åŠ›æœºåˆ¶å°±åƒå°æ˜å‚åŠ "åœ†æ¡Œè®¨è®ºä¼š":

1. åœºæ™¯è®¾å®š:
   - å¥å­ä¸­çš„æ¯ä¸ªè¯æ˜¯ä¸€ä¸ª"ä¸ä¼šè€…"
   - æ¯ä¸ªä¸ä¼šè€…éƒ½æœ‰é—®é¢˜ (Query) å’ŒçŸ¥è¯† (Key+Value)

2. è®¨è®ºè¿‡ç¨‹:
   - å°æ˜ (å½“å‰è¯) æå‡ºé—®é¢˜ (Query)
   - å‘æ‰€æœ‰äººè¯¢é—® (å’Œæ¯ä¸ªäººçš„ Key è®¡ç®—ç›¸ä¼¼åº¦)
   - æ ¹æ®ç›¸å…³åº¦ï¼Œè·å–å¤§å®¶çš„çŸ¥è¯† (åŠ æƒæ±‚å’Œ Value)

3. å¤šå¤´æ³¨æ„åŠ›:
   - 8 ä¸ªå°æ˜åˆ†èº«ï¼Œå„è‡ªå…³æ³¨ä¸åŒè§’åº¦
   - è¯­æ³•ã€è¯­ä¹‰ã€æƒ…æ„Ÿã€è·ç¦»...
   - æœ€åæ±‡æ€»æ‰€æœ‰å‘ç°

4. ä½ç½®ç¼–ç  (RoPE):
   - ç»™æ¯ä¸ªåº§ä½ç¼–å·
   - è®©å°æ˜çŸ¥é“"è°ååœ¨å‰é¢ã€è°ååœ¨åé¢"

5. KV Cache:
   - æŠŠè®¨è®ºç»“æœè®°åœ¨ç¬”è®°æœ¬ä¸Š
   - ä¸‹æ¬¡è®¨è®ºç›´æ¥ç¿»ç¬”è®°ï¼Œä¸ç”¨é‡æ–°é—®

6. GQA:
   - 4 ä¸ªå­¦ç”Ÿå…±ç”¨ 1 æœ¬è¯å…¸
   - èŠ‚çœèµ„æºï¼Œæ•ˆæœç›¸å½“

7. Flash Attention:
   - åˆ†æ‰¹è®¨è®ºï¼Œè¾¹è®¨è®ºè¾¹æ€»ç»“
   - ä¸ç”¨åŒæ—¶è®°ä½æ‰€æœ‰äººè¯´çš„è¯
   - çœå†…å­˜ã€é€Ÿåº¦å¿«
```

---

### ğŸ”— ç›¸å…³ä»£ç ä½ç½®æ±‡æ€»

| ç»„ä»¶ | ä»£ç ä½ç½® | åŠŸèƒ½ |
|-----|---------|------|
| `apply_rotary_pos_emb` | 301-352 è¡Œ | åº”ç”¨ RoPE æ—‹è½¬ä½ç½®ç¼–ç  |
| `repeat_kv` | 355-389 è¡Œ | GQA çš„ KV å¤´å¤åˆ¶æ‰©å±• |
| `Attention.__init__` | 399-443 è¡Œ | åˆå§‹åŒ–æŠ•å½±å±‚å’Œé…ç½® |
| `Attention.forward` | 446-549 è¡Œ | å®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—æµç¨‹ |
| Flash Attention è·¯å¾„ | 499-513 è¡Œ | PyTorch å†…ç½®é«˜æ•ˆå®ç° |
| æ‰‹åŠ¨ Attention è·¯å¾„ | 515-543 è¡Œ | å…¼å®¹æ—§ç‰ˆæœ¬çš„å®ç° |

---



