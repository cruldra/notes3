---
title: Tokenizer Embedding Demo
marimo-version: 0.19.2
width: medium
---

```python {.marimo}
import marimo as mo
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from pathlib import Path
```

# ğŸ¯ åˆ†è¯å™¨ä¸è¯åµŒå…¥å®Œæ•´æµç¨‹æ¼”ç¤º

æœ¬ç¬”è®°æœ¬æ¼”ç¤ºä»**åŸå§‹æ–‡æœ¬**åˆ°**è¯åµŒå…¥å‘é‡**çš„å®Œæ•´è½¬æ¢è¿‡ç¨‹ã€‚

## ğŸ“Š æµç¨‹æ¦‚è§ˆ

```python {.marimo hide_code="true"}
mo.mermaid(r"""
    graph TD
        A[åŸå§‹æ–‡æœ¬<br/>'å°çº¢å¸½å»æ£®æ—'] -->|åˆ†è¯| B[Tokenizer]
        B -->|åˆ‡åˆ†è¯è¯­| C[Tokens<br/>å°çº¢å¸½,å»,æ£®æ—]
        C -->|æŸ¥è¯å…¸| D[Token IDs<br/>1,453,234,789,2]
        D -->|ç´¢å¼•æŸ¥æ‰¾| E[Embedding Table<br/>6400Ã—512çŸ©é˜µ]
        E -->|è·å–å‘é‡| F[Embedding Vectors<br/>5Ã—512çŸ©é˜µ]
""")
```

## ğŸ”§ æ­¥éª¤1: åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨

æˆ‘ä»¬ä½¿ç”¨ HuggingFace çš„ `AutoTokenizer` åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨ã€‚
è¿™é‡Œä½¿ç”¨ Qwen2.5-7B-Instruct çš„åˆ†è¯å™¨ä½œä¸ºç¤ºä¾‹ã€‚

```python {.marimo}
# åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨
# å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–æ¨¡å‹: "Qwen/Qwen2.5-1.5B-Instruct", "meta-llama/Llama-3.1-8B-Instruct" ç­‰
tokenizer_path = "Qwen/Qwen2.5-7B-Instruct"

print(f"ğŸ“¥ æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {tokenizer_path}")
print("â³ é¦–æ¬¡è¿è¡Œä¼šä» HuggingFace ä¸‹è½½,è¯·ç¨å€™...")

tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    trust_remote_code=True,  # Qwen æ¨¡å‹éœ€è¦æ­¤å‚æ•°
)

print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ!")
print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
print(f"ğŸ”¤ ç‰¹æ®Š Token:")
print(f"   BOS (å¼€å§‹): '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})")
print(f"   EOS (ç»“æŸ): '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")
print(f"   PAD (å¡«å……): '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
```

## ğŸ”¤ æ­¥éª¤2: æ–‡æœ¬åˆ†è¯ (Tokenization)
<!---->
### ğŸ’¡ åˆ†è¯å™¨å·²å°±ç»ª

`AutoTokenizer.from_pretrained()` è‡ªåŠ¨åŠ è½½äº†:
- **è¯æ±‡è¡¨ (vocab.json)**: æ‰€æœ‰æ”¯æŒçš„ Token åŠå…¶ ID
- **åˆå¹¶è§„åˆ™ (merges.txt/tokenizer.json)**: BPE/WordPiece åˆ†è¯è§„åˆ™
- **ç‰¹æ®Š Token é…ç½®**: BOS/EOS/PAD ç­‰æ ‡è®°
- **åˆ†è¯é€»è¾‘**: å®Œæ•´çš„ç¼–ç /è§£ç åŠŸèƒ½
<!---->
### ğŸ“ å®é™…æ¼”ç¤º: å¯¹ "å°çº¢å¸½å»æ£®æ—" è¿›è¡Œåˆ†è¯

```python {.marimo}
# åŸå§‹æ–‡æœ¬
text = "å°çº¢å¸½å»æ£®æ—"

# æ­¥éª¤1: åˆ†è¯
tokens = tokenizer.tokenize(text)
print(f"ğŸ”¤ åŸå§‹æ–‡æœ¬: {text}")
print(f"ğŸ“‹ Tokens: {tokens}")

# æ­¥éª¤2: è½¬æ¢ä¸º IDs
token_ids = tokenizer.encode(text, add_special_tokens=False)
print(f"ğŸ”¢ Token IDs: {token_ids}")

# æ˜¾ç¤ºå¯¹åº”å…³ç³»
print("\nğŸ“Š Token <-> ID æ˜ å°„:")
for token, tid in zip(tokens, token_ids):
    print(f"  '{token}' -> {tid}")

# éªŒè¯è§£ç 
decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)
print(f"\nğŸ”„ è§£ç ç»“æœ: {decoded_text}")
print(f"âœ… è§£ç æ­£ç¡®: {decoded_text == text}")
```

## ğŸ¨ æ­¥éª¤3: è¯åµŒå…¥ (Embedding)
<!---->
### æ ¸å¿ƒæ¦‚å¿µ

**Embedding Layer** = ä¸€ä¸ªæŸ¥æ‰¾è¡¨ (Lookup Table)

- **è¾“å…¥**: Token ID (æ•´æ•°, ä¾‹å¦‚ 453)
- **è¾“å‡º**: å¯†é›†å‘é‡ (ä¾‹å¦‚ 512 ç»´çš„æµ®ç‚¹æ•°å‘é‡)
- **æœ¬è´¨**: ä» `vocab_size Ã— hidden_size` çš„å¤§çŸ©é˜µä¸­,æ ¹æ® ID æå–å¯¹åº”çš„è¡Œ

```python {.marimo}
# åˆ›å»º Embedding å±‚ - ä½¿ç”¨çœŸå®çš„è¯æ±‡è¡¨å¤§å°
vocab_size = tokenizer.vocab_size  # ä½¿ç”¨åˆ†è¯å™¨çš„å®é™…è¯æ±‡è¡¨å¤§å°
hidden_size = 512  # åµŒå…¥ç»´åº¦ (å¯ä»¥è°ƒæ•´,å®é™…æ¨¡å‹å¯èƒ½æ˜¯ 2048, 4096 ç­‰)

embedding_layer = nn.Embedding(vocab_size, hidden_size)

# åˆå§‹åŒ–ä¸ºå°éšæœºæ•° (å®é™…è®­ç»ƒä¸­ä¼šå­¦ä¹ åˆ°è¯­ä¹‰ä¿¡æ¯)
nn.init.normal_(embedding_layer.weight, mean=0.0, std=0.02)

print(f"ğŸ“ Embedding å±‚å½¢çŠ¶: {embedding_layer.weight.shape}")
print(f"   = {vocab_size:,} ä¸ªè¯ Ã— {hidden_size} ç»´å‘é‡")
print(f"   = æ€»å…± {vocab_size * hidden_size:,} ä¸ªå‚æ•°")
print(f"   â‰ˆ {vocab_size * hidden_size * 4 / 1024 / 1024:.2f} MB (float32)")
```

### ğŸ” æ¼”ç¤º: å°† Token IDs è½¬æ¢ä¸ºåµŒå…¥å‘é‡

```python {.marimo}
# è½¬æ¢ä¸º Tensor
input_ids = torch.tensor([token_ids])  # shape: [1, seq_len]

# é€šè¿‡ Embedding å±‚
embeddings = embedding_layer(input_ids)

print(f"ğŸ“¥ è¾“å…¥ Token IDs: {input_ids.shape} = [batch_size, seq_len]")
print(f"   å®é™…å€¼: {input_ids.tolist()}")
print(
    f"\nğŸ“¤ è¾“å‡º Embeddings: {embeddings.shape} = [batch_size, seq_len, hidden_size]"
)
print(f"\nğŸ” ç¬¬ä¸€ä¸ª Token ('{tokens[0]}', ID={token_ids[0]}) çš„åµŒå…¥å‘é‡ (å‰10ç»´):")
print(f"   {embeddings[0, 0, :10].detach().numpy()}")
```

## ğŸ“Š å¯è§†åŒ–: Embedding æŸ¥æ‰¾è¿‡ç¨‹

ä»¥ Token "å°çº¢å¸½" (ID=453) ä¸ºä¾‹:

```python {.marimo}
# å•ç‹¬æŸ¥è¯¢ä¸€ä¸ª Token çš„åµŒå…¥
test_text = "å°çº¢å¸½"
token_id_test = tokenizer.encode(test_text, add_special_tokens=False)[0]
embedding_test = embedding_layer(torch.tensor([token_id_test]))

print(f"ğŸ¯ Token: '{test_text}'")
print(f"ğŸ”¢ ID: {token_id_test}")
print(f"ğŸ“ åµŒå…¥å‘é‡å½¢çŠ¶: {embedding_test.shape}")
print(f"\nğŸ” å‘é‡å†…å®¹ (å‰20ç»´):")
print(embedding_test[0, :20].detach().numpy())
```

## ğŸ§® æ­¥éª¤4: å®Œæ•´æµç¨‹ä¸²è”

æ¨¡æ‹Ÿå®Œæ•´çš„ **æ–‡æœ¬ -> Transformer** è¾“å…¥å‡†å¤‡è¿‡ç¨‹

```python {.marimo}
def text_to_embeddings(text, tokenizer, embedding_layer):
    """å®Œæ•´æµç¨‹: æ–‡æœ¬ -> Embeddings"""
    # 1. åˆ†è¯
    tokens_1 = tokenizer.tokenize(text)

    # 2. è½¬ IDs (ä¸æ·»åŠ ç‰¹æ®Š token,ä»…ä¸ºæ¼”ç¤º)
    ids = tokenizer.encode(text, add_special_tokens=False)

    # 3. è½¬ Tensor
    input_tensor = torch.tensor([ids])

    # 4. è·å– Embeddings
    embeddings_1 = embedding_layer(input_tensor)

    return {
        "text": text,
        "tokens": tokens_1,
        "ids": ids,
        "input_shape": input_tensor.shape,
        "embedding_shape": embeddings_1.shape,
        "embeddings": embeddings_1,
    }

# æ¼”ç¤º
result = text_to_embeddings(text, tokenizer, embedding_layer)

print("=" * 60)
print("ğŸ¯ å®Œæ•´æµç¨‹æ¼”ç¤º")
print("=" * 60)
print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {result['text']}")
print(f"ğŸ”¤ Tokens: {result['tokens']}")
print(f"ğŸ”¢ Token IDs: {result['ids']}")
print(f"ğŸ“¥ è¾“å…¥å½¢çŠ¶: {result['input_shape']}")
print(f"ğŸ“¤ åµŒå…¥å½¢çŠ¶: {result['embedding_shape']}")
print(f"\nâœ… æ­¤æ—¶åµŒå…¥å‘é‡å·²å‡†å¤‡å¥½é€å…¥ Transformer è¿›è¡Œå¤„ç†!")
```

## ğŸ“ å…³é”®è¦ç‚¹æ€»ç»“

### 1ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦ Embedding?

| è¡¨ç¤ºæ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|---------|------|------|
| **One-Hot** (ç‹¬çƒ­ç¼–ç ) | ç®€å•ç›´è§‚ | ç»´åº¦çˆ†ç‚¸ (6400ç»´), æ— è¯­ä¹‰ä¿¡æ¯ |
| **Embedding** (å¯†é›†å‘é‡) | ä½ç»´ (512ç»´), åŒ…å«è¯­ä¹‰ | éœ€è¦è®­ç»ƒå­¦ä¹  |

### 2ï¸âƒ£ Embedding å¦‚ä½•å­¦ä¹ è¯­ä¹‰?

- **åˆå§‹åŒ–**: éšæœºå°æ•°å€¼
- **è®­ç»ƒè¿‡ç¨‹**: é€šè¿‡åå‘ä¼ æ’­,æ ¹æ®ä¸Šä¸‹æ–‡è‡ªåŠ¨è°ƒæ•´
- **ç»“æœ**: è¯­ä¹‰ç›¸è¿‘çš„è¯å‘é‡è·ç¦»æ›´è¿‘

ä¾‹å¦‚è®­ç»ƒå:
```
similarity("å›½ç‹", "ç‹å") > similarity("å›½ç‹", "è‹¹æœ")
```

### 3ï¸âƒ£ å®é™…ä»£ç å¯¹åº”

```python
# æ¥è‡ª ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.py

# å®šä¹‰ Embedding å±‚ (py:1160)
self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)

# å‰å‘ä¼ æ’­ (py:1393)
h = self.embedding(idx)  # [batch, seq_len] -> [batch, seq_len, hidden_size]
```

### 4ï¸âƒ£ ä¸‹ä¸€æ­¥

åµŒå…¥å‘é‡ä¼šé€å…¥ **Transformer Blocks** è¿›è¡Œå¤šå±‚å¤„ç†:
- ä½ç½®ç¼–ç  (RoPE)
- è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Attention)
- å‰é¦ˆç½‘ç»œ (FeedForward)
- å±‚å½’ä¸€åŒ– (RMSNorm)

æœ€ç»ˆè¾“å‡ºé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚
<!---->
## ğŸ”¬ æ‰©å±•å®éªŒ: å¤šä¸ªå¥å­çš„æ‰¹å¤„ç†

å®é™…è®­ç»ƒæ—¶,æˆ‘ä»¬ä¼šåŒæ—¶å¤„ç†å¤šä¸ªå¥å­ (batch)

```python {.marimo}
# å¤šä¸ªå¥å­
sentences = [
    "å°çº¢å¸½å»æ£®æ—",
    "å°çº¢å¸½çˆ±å­¦ä¹ ",
]

# ä½¿ç”¨ tokenizer è‡ªå¸¦çš„æ‰¹å¤„ç†åŠŸèƒ½
# padding=True ä¼šè‡ªåŠ¨è¡¥é½åˆ°æœ€é•¿åºåˆ—
# return_tensors="pt" è¿”å› PyTorch tensor
batch_encoding = tokenizer(
    sentences, padding=True, return_tensors="pt", add_special_tokens=False
)

batch_tensor = batch_encoding["input_ids"]  # shape: [batch_size, max_len]
attention_mask = batch_encoding[
    "attention_mask"
]  # 0 è¡¨ç¤º padding, 1 è¡¨ç¤ºçœŸå® token

# è·å– Embeddings
batch_embeddings = embedding_layer(batch_tensor)

print(f"ğŸ“š æ‰¹å¤„ç† {len(sentences)} ä¸ªå¥å­")
print(f"ğŸ“¥ è¾“å…¥å½¢çŠ¶: {batch_tensor.shape} = [batch_size, max_seq_len]")
print(
    f"ğŸ“¤ è¾“å‡ºå½¢çŠ¶: {batch_embeddings.shape} = [batch_size, max_seq_len, hidden_size]"
)
print(f"\nğŸ” æ¯ä¸ªå¥å­çš„ IDs (å·²è‡ªåŠ¨ Padding):")
for i, sent in enumerate(sentences):
    print(f"  [{i}] '{sent}'")
    print(f"      IDs: {batch_tensor[i].tolist()}")
    print(f"      Mask: {attention_mask[i].tolist()}")
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- å®Œæ•´ä»£ç : `ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.py`
- è¯¦ç»†è§£é‡Š: `ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„å¤§æ¨¡å‹.md`
- Transformer è®ºæ–‡: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)