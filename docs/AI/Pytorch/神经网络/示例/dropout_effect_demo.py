import marimo

__generated_with = "0.19.2"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    # ç›´è§‚ç†è§£ Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆçš„åˆ©å™¨

    Dropout æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ç§å¼ºå¤§çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚

    **é€šä¿—çš„ä¾‹å­**ï¼š
    æƒ³è±¡ä½ åœ¨å­¦ä¹ ä¸€ä¸ªå¤æ‚çš„çŸ¥è¯†ï¼ˆæ¯”å¦‚åšé«˜æ•°é¢˜ï¼‰ã€‚
    - **æ²¡æœ‰ Dropoutï¼ˆè¿‡æ‹Ÿåˆï¼‰**ï¼šä½ æ­»è®°ç¡¬èƒŒäº†æ¯ä¸€é“ç»ƒä¹ é¢˜çš„ç­”æ¡ˆã€‚é‡åˆ°åšè¿‡çš„é¢˜ä½ ä¼šåšï¼Œä½†é‡åˆ°ç¨å¾®å˜ä¸€ç‚¹çš„é¢˜ï¼ˆæµ‹è¯•é›†ï¼‰ï¼Œä½ å°±æ‡µäº†ã€‚
    - **æœ‰ Dropout**ï¼šå­¦ä¹ æ—¶ï¼Œä½ çš„è„‘å­å¶å°”ä¼šâ€œæ–­ç‰‡â€ï¼Œå¿˜æ‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¿æ¥ã€‚è¿™è¿«ä½¿ä½ ä¸èƒ½ä¾èµ–æŸä¸€æ¡ç‰¹å®šçš„è·¯å¾„æ¥è§£é¢˜ï¼Œå¿…é¡»å­¦ä¼šé€šè¿‡å¤šç§çº¿ç´¢æ¨å¯¼ç­”æ¡ˆã€‚è¿™æ ·å­¦å‡ºæ¥çš„çŸ¥è¯†æ›´é²æ£’ï¼Œé‡åˆ°æ–°é¢˜ä¹Ÿèƒ½ä¸¾ä¸€åä¸‰ã€‚

    ä¸‹é¢æˆ‘ä»¬ç”¨ä¸€ä¸ªæå…¶ç®€å•çš„**å›å½’ä»»åŠ¡**æ¥æ¼”ç¤ºè¿™ä¸€ç°è±¡ã€‚
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ## 1. åˆ¶é€ ä¸€ä¸ªå®¹æ˜“"æ­»è®°ç¡¬èƒŒ"çš„æ•°æ®é›†

    æˆ‘ä»¬ç”Ÿæˆä»…ä»… 20 ä¸ªæ•°æ®ç‚¹ï¼Œä½†æ˜¯ç”¨ä¸€ä¸ªå¾ˆå¤æ‚çš„ç¥ç»ç½‘ç»œï¼ˆ300ä¸ªéšè—ç¥ç»å…ƒï¼‰å»æ‹Ÿåˆå®ƒã€‚
    """)
    return


@app.cell
def _():
    import torch
    import matplotlib.pyplot as plt

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
    torch.manual_seed(42)

    # ç”Ÿæˆæ•°æ®: y = x^2 + å™ªéŸ³
    # åªæœ‰20ä¸ªç‚¹ï¼Œå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆ
    N_SAMPLES = 20
    x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), dim=1)
    y = x.pow(2) + 0.3 * torch.rand(x.size()) # å¢åŠ ä¸€ç‚¹å™ªéŸ³å¹…åº¦

    # æµ‹è¯•é›†æ•°æ®ï¼ˆç”¨æ¥ç”»å‡ºçœŸå®çš„å¹³æ»‘æ›²çº¿ï¼Œä½œä¸ºå‚è€ƒï¼‰
    test_x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)
    return plt, test_x, torch, x, y


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ## 2. å®šä¹‰æ¨¡å‹ï¼šè¿‡å¤§ vs é€‚ä¸­ï¼ˆåŠ Dropoutï¼‰
    """)
    return


@app.cell
def _(torch):
    N_HIDDEN = 300 # éšè—å±‚ç¥ç»å…ƒéå¸¸å¤šï¼Œè¶³ä»¥æ­»è®°ç¡¬èƒŒæ‰€æœ‰ç‚¹

    class Net(torch.nn.Module):
        def __init__(self, dropout_prob=0.0):
            super(Net, self).__init__()
            self.hidden = torch.nn.Linear(1, N_HIDDEN)
            self.dropout = torch.nn.Dropout(p=dropout_prob) # æ ¸å¿ƒï¼šDropoutå±‚
            self.predict = torch.nn.Linear(N_HIDDEN, 1)

        def forward(self, x):
            x = torch.relu(self.hidden(x))
            x = self.dropout(x) # åœ¨æ¿€æ´»å‡½æ•°ååº”ç”¨ dropout
            x = self.predict(x)
            return x
    return (Net,)


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ## 3. è®­ç»ƒå¯¹æ¯”

    æˆ‘ä»¬è¦è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼š
    1. **Overfit Net**: ä¸ä½¿ç”¨ Dropout (`p=0`)
    2. **Dropout Net**: ä½¿ç”¨ 50% çš„ Dropout (`p=0.5`)

    è§‚å¯Ÿå®ƒä»¬åœ¨è®­ç»ƒé›†ï¼ˆçº¢ç‚¹ï¼‰ä¸Šçš„æ‹Ÿåˆæƒ…å†µã€‚
    """)
    return


@app.cell
def _(Net, torch, x, y):
    # å®ä¾‹åŒ–ä¸¤ä¸ªæ¨¡å‹
    net_overfit = Net(dropout_prob=0.0)
    net_dropout = Net(dropout_prob=0.5)

    optimizer_ofit = torch.optim.Adam(net_overfit.parameters(), lr=0.01)
    optimizer_drop = torch.optim.Adam(net_dropout.parameters(), lr=0.01)
    loss_func = torch.nn.MSELoss()

    loss_ofit = None
    loss_drop = None
    pred_ofit = None
    pred_drop = None
    t = 0

    # è®­ç»ƒ 500 æ­¥
    for t in range(500):
        # 1. Overfit Net è®­ç»ƒ
        pred_ofit = net_overfit(x)
        loss_ofit = loss_func(pred_ofit, y)
        optimizer_ofit.zero_grad()
        loss_ofit.backward()
        optimizer_ofit.step()

        # 2. Dropout Net è®­ç»ƒ
        pred_drop = net_dropout(x)
        loss_drop = loss_func(pred_drop, y)
        optimizer_drop.zero_grad()
        loss_drop.backward()
        optimizer_drop.step()
    return net_dropout, net_overfit


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ## 4. ç»“æœå¯è§†åŒ–
    """)
    return


@app.cell
def _(mo, net_dropout, net_overfit, plt, test_x, x, y):
    # é¢„æµ‹ï¼ˆé¢„æµ‹æ—¶éœ€è¦æŠŠæ¨¡å¼è°ƒä¸º evalï¼Œè¿™ä¼šè‡ªåŠ¨å…³é—­ dropoutï¼‰
    net_overfit.eval()
    net_dropout.eval()

    test_pred_ofit = net_overfit(test_x).data.numpy()
    test_pred_drop = net_dropout(test_x).data.numpy()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Overfitting Plot
    ax1.scatter(x.data.numpy(), y.data.numpy(), c='red', s=50, alpha=0.5, label='train samples')
    ax1.plot(test_x.data.numpy(), test_pred_ofit, 'r-', lw=3, label='overfitting')
    ax1.set_title('Without Dropout')
    ax1.set_ylim(0, 2.0)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Dropout Plot
    ax2.scatter(x.data.numpy(), y.data.numpy(), c='red', s=50, alpha=0.5, label='train samples')
    ax2.plot(test_x.data.numpy(), test_pred_drop, 'b--', lw=3, label='dropout (p=0.5)')
    ax2.set_title('With Dropout')
    ax2.set_ylim(0, 2.0)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    # ä½¿ç”¨ mo.vstack å‚ç›´å †å è¯´æ˜æ–‡æœ¬å’Œå›¾è¡¨
    mo.vstack([
        mo.md("å·¦å›¾å±•ç¤ºäº†æ²¡æœ‰ Dropout æ—¶ï¼Œæ¨¡å‹ä¸ºäº†è¿åˆæ¯ä¸€ä¸ªè®­ç»ƒç‚¹ï¼Œæ›²çº¿å¯èƒ½ä¼šå‡ºç°ä¸è‡ªç„¶çš„**æŠ–åŠ¨**ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚<br>å³å›¾å±•ç¤ºäº†åŠ ä¸Š Dropout åï¼Œæ¨¡å‹å­¦ä¼šäº†æ›´**å¹³æ»‘**çš„è§„å¾‹ã€‚"),
        fig
    ])
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ## ğŸ§  æµç¨‹ä¸ API è¯¦è§£

    ### 1. ä»£ç æ‰§è¡Œæµç¨‹
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.mermaid("""
    graph TB
        A["Start: ç”Ÿæˆæ•°æ®"] --> B{"å®šä¹‰æ¨¡å‹ç»“æ„"}
        B --> C["å®ä¾‹åŒ–æ¨¡å‹ 1: Overfit Net (p=0)"]
        B --> D["å®ä¾‹åŒ–æ¨¡å‹ 2: Dropout Net (p=0.5)"]

        C --> E["è®­ç»ƒå¾ªç¯ (500 epochs)"]
        D --> E

        subgraph Training ["è®­ç»ƒè¿‡ç¨‹"]
            E1["Forward å‰å‘ä¼ æ’­"] --> E2["Compute Loss è®¡ç®—æŸå¤±"]
            E2 --> E3["Zero Grad æ¸…ç©ºæ¢¯åº¦"]
            E3 --> E4["Backward åå‘ä¼ æ’­"]
            E4 --> E5["Optimizer Step æ›´æ–°å‚æ•°"]
        end

        E --> E1
        E5 --> F["Eval æ¨¡å¼: net.eval()"]

        F --> G["åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹"]
        G --> H["å¯è§†åŒ–å¯¹æ¯”ç»“æœ"]
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md("""
    ### 2. ğŸ“š æ›´å¤šå…³é”® API è¯´æ˜

    é™¤äº† Dropoutï¼Œæœ¬ç¤ºä¾‹è¿˜æ¶‰åŠäº†å®Œæ•´çš„ PyTorch è®­ç»ƒæµç¨‹ APIï¼š

    *   **æ•°æ®å‡†å¤‡**:
        *   `torch.linspace(start, end, steps)`: ç”Ÿæˆç­‰å·®æ•°åˆ—ï¼Œç”¨äºåˆ›å»º x è½´åæ ‡ã€‚
        *   `torch.rand(size)`: ç”Ÿæˆ [0, 1) åŒºé—´çš„å‡åŒ€åˆ†å¸ƒéšæœºæ•°ï¼Œç”¨äºæ·»åŠ å™ªå£°ã€‚
        *   `torch.unsqueeze(input, dim)`: å¢åŠ ç»´åº¦ï¼ˆä¾‹å¦‚å°† `[20]` å˜ä¸º `[20, 1]`ï¼‰ï¼Œä»¥é€‚é…å…¨è¿æ¥å±‚çš„è¾“å…¥è¦æ±‚ã€‚

    *   **æ¨¡å‹æ„å»º (`torch.nn`)**:
        *   `torch.nn.Linear(in_features, out_features)`: å…¨è¿æ¥å±‚ï¼ˆçº¿æ€§å±‚ï¼‰ï¼Œå®ç° $y = xA^T + b$ã€‚
        *   `torch.nn.Dropout(p=0.5)`: Dropout å±‚ã€‚**ä½œç”¨**: è®­ç»ƒæ—¶éšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºç½®é›¶ï¼Œ**ç›®çš„**: é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        *   `torch.relu(input)`: æ¿€æ´»å‡½æ•° $max(0, x)$ï¼Œä¸ºç½‘ç»œå¼•å…¥éçº¿æ€§èƒ½åŠ›ã€‚

    *   **è®­ç»ƒæ ¸å¿ƒ**:
        *   `torch.optim.Adam(params, lr)`: Adam ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£æ›´æ–°ç½‘ç»œå‚æ•°ã€‚
        *   `torch.nn.MSELoss()`: å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œç”¨äºå›å½’é—®é¢˜ ($loss = (y_{pred} - y_{true})^2$)ã€‚
        *   `loss.backward()`: åå‘ä¼ æ’­ï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚
        *   `optimizer.step()`: æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
        *   `optimizer.zero_grad()`: æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦ï¼ˆPyTorch æ¢¯åº¦ä¼šç´¯ç§¯ï¼Œæ‰€ä»¥æ¯æ¬¡æ›´æ–°å‰è¦æ¸…é›¶ï¼‰ã€‚

    *   **æ§åˆ¶ä¸å·¥å…·**:
        *   `model.eval()` / `model.train()`: åˆ‡æ¢æ¨¡å¼ã€‚**æ³¨æ„**: é¢„æµ‹æ—¶å¿…é¡»è°ƒç”¨ `.eval()`ï¼Œè¿™ä¼šå…³é—­ Dropoutï¼ˆè®©ç¥ç»å…ƒå…¨è¿æ¥ï¼‰ï¼Œå¦åˆ™é¢„æµ‹ç»“æœä¸ç¨³å®šã€‚
        *   `torch.manual_seed(seed)`: å›ºå®šéšæœºç§å­ï¼Œä¿è¯å®éªŒç»“æœå¯å¤ç°ã€‚
        *   `tensor.data.numpy()`: å°† PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œä»¥ä¾¿ç”¨ Matplotlib ç»˜å›¾ã€‚
    """)
    return


if __name__ == "__main__":
    app.run()
