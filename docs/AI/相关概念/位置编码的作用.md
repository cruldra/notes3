# 位置编码的作用 (Positional Encoding)

## 1. 为什么需要它？ (The Problem: Permutation Invariance)

Transformer 架构的核心是 **自注意力机制 (Self-Attention)**。与循环神经网络 (RNN) 或卷积神经网络 (CNN) 不同，标准自注意力机制具有 **排列不变性 (Permutation Invariance)**。

### 1.1 排列不变性证明

给定输入序列 $X = [x_1, x_2, ..., x_n] \in \mathbb{R}^{n \times d}$，自注意力层的输出为：

$$
\text{Attention}(X) = \text{softmax}\left(\frac{(XW_Q)(XW_K)^T}{\sqrt{d_k}}\right)(XW_V)
$$

假设我们要交换输入序列中的两个位置 $i$ 和 $j$，即输入变为 $X'$。
由于点积运算 $x_i W_Q (x_j W_K)^T$ 仅依赖于 token 对的内容 $(x_i, x_j)$，而不依赖于它们的绝对位置索引，因此：

$$
\text{Attention}(X') = P \cdot \text{Attention}(X)
$$

其中 $P$ 是对应的排列矩阵。这意味着，对于 Transformer 来说，**"我 爱 你"** 和 **"你 爱 我"** 在没有位置编码的情况下，除了输出顺序不同外，其内部特征提取逻辑是完全一样的。模型无法区分主语和宾语，也无法捕捉词序对语义的决定性影响。

位置编码的作用就是 **打破这种对称性**，将位置信息注入到输入嵌入中，使模型能够利用序列的顺序信息。

## 2. 解决方案演进 (Evolution of Solutions)

### 2.1 绝对位置编码 (Absolute Positional Embedding)

最直接的方法是为每个位置 $pos \in \{0, 1, ..., L-1\}$ 分配一个固定的向量 $p_{pos}$，并将其加到词嵌入上：

$$
x_{input} = x_{word} + p_{pos}
$$

**经典实现 (Sinusoidal)**:
Attention Is All You Need (Vaswani et al., 2017) 提出使用不同频率的正弦和余弦函数：

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{model}})
\end{aligned}
$$

*   **优点**: 可以推理出训练长度之外的相对位置关系（理论上）。
*   **缺点**: 直接相加的方式使得位置信息与语义信息混合，且随着层数加深，位置信息可能会被逐渐冲淡。

### 2.2 相对位置编码 (Relative Positional Encoding)

Shaw et al. (2018) 提出，注意力机制应该只关注 token 之间的相对距离 $i-j$，而不是绝对位置。

$$
e_{ij} = \frac{x_i W_Q (x_j W_K + a_{i-j})^T}{\sqrt{d_k}}
$$

*   **优点**: 对平移不变，更符合自然语言的局部性原理。
*   **缺点**: 引入了额外的训练参数，且计算复杂度略有增加。

### 2.3 旋转位置编码 (RoPE)

目前主流大模型 (LLaMA, PaLM 等) 的选择。它通过复数域的旋转操作，巧妙地结合了绝对位置编码和相对位置编码的优点。

$$
f(x, m) = x e^{im\theta}
$$

它保证了 $\langle f(x, m), f(y, n) \rangle = g(x, y, m-n)$，即内积仅取决于相对距离 $m-n$。

## 3. 大模型训练中解决的核心问题

位置编码在 LLM 训练中主要解决了以下挑战：

1.  **语义歧义消除**: 区分 "A 打 B" 与 "B 打 A"。
2.  **长距离依赖保持**: 帮助模型在长上下文中定位信息（例如，第 1000 个 token 需要引用第 5 个 token 的定义）。
3.  **外推性 (Extrapolation)**: (特别是 RoPE 和 ALiBi) 允许模型在推理时处理比训练序列更长的文本，这是现代 LLM 处理长文档的关键能力。

## 参考资料

1.  [Positional Embeddings in Transformer Models - ICLR Blogposts](https://iclr-blogposts.github.io/2025/blog/positional-embedding/)
2.  [Making Sense of Positional Encoding - Medium](https://medium.com/@a.arun283/a-deeper-look-into-the-positional-encoding-method-in-transformer-architectures-7e98f32a925f)
3.  [A Gentle Introduction to Positional Encoding - MachineLearningMastery](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
