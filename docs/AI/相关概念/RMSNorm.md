# å‡æ–¹æ ¹å±‚å½’ä¸€åŒ– (RMSNorm)

> **ä¸€å¥è¯è§£é‡Š**ï¼šRMSNorm æ˜¯å¤§æ¨¡å‹ï¼ˆå¦‚ Llamaï¼‰è®­ç»ƒä¸­æ›´é«˜æ•ˆçš„â€œæ•°æ®æ•´ç†å·¥â€ã€‚å®ƒé€šè¿‡ä¸€ç§æ›´ç®€å•ã€æ›´å¿«é€Ÿçš„æ–¹å¼ï¼ŒæŠŠæ•°æ®â€œæ‹‰â€å›æ ‡å‡†èŒƒå›´ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒä¸­å› ä¸ºæ•°æ®æ•°å€¼å¤ªå¤§æˆ–å¤ªå°è€Œâ€œæ™•å¤´è½¬å‘â€ã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿï¼ˆç—›ç‚¹ï¼‰

åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯åƒ Transformer è¿™æ ·å¾ˆæ·±çš„æ¨¡å‹ï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®åœ¨ç»è¿‡ä¸€å±‚å±‚è®¡ç®—åï¼Œæ•°å€¼çš„åˆ†å¸ƒä¼šå‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚

*   **æ•°å€¼çˆ†ç‚¸**ï¼šæœ‰çš„æ•°å­—å˜å¾—ç‰¹åˆ«å¤§ï¼ˆæ¯”å¦‚ 10000ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹æ¢¯åº¦çˆ†ç‚¸ï¼Œè®­ç»ƒå¤±è´¥ã€‚
*   **æ•°å€¼æ¶ˆå¤±**ï¼šæœ‰çš„æ•°å­—å˜å¾—ç‰¹åˆ«å°ï¼ˆæ¯”å¦‚ 0.00001ï¼‰ï¼Œå¯¼è‡´æ¨¡å‹å­¦ä¸åˆ°ä¸œè¥¿ã€‚

è¿™å°±å¥½æ¯”ä½ åœ¨ç‚’èœï¼Œå¦‚æœä¸€å¼€å§‹ç›æ”¾å¤šäº†ï¼Œåé¢æ€ä¹ˆè°ƒå‘³éƒ½å¾ˆéš¾æ•‘å›æ¥ã€‚**å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰** å°±æ˜¯é‚£ä¸ªåœ¨æ¯ä¸€æ­¥éƒ½å¸®ä½ æŠŠå‘³é“ï¼ˆæ•°æ®æ•°å€¼ï¼‰è°ƒå›æ ‡å‡†æ°´å¹³çš„â€œå¤§å¨åŠ©æ‰‹â€ã€‚

## 2. RMSNorm æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆé€šä¿—æ¯”å–»ï¼‰

ä¸ºäº†ç†è§£ RMSNormï¼Œæˆ‘ä»¬å…ˆçœ‹çœ‹å®ƒçš„å‰è¾ˆ **LayerNorm**ã€‚

### 2.1 å‰è¾ˆ LayerNormï¼šä¸¥è°¨çš„ç»Ÿè®¡å­¦å®¶
LayerNorm å¤„ç†æ•°æ®éå¸¸ä¸¥è°¨ï¼Œå®ƒåšä¸¤ä»¶äº‹ï¼š
1.  **å»ä¸­å¿ƒåŒ–ï¼ˆRe-centeringï¼‰**ï¼šç®—å‡ºæ‰€æœ‰æ•°æ®çš„å¹³å‡å€¼ï¼Œç„¶åæŠŠæ¯ä¸ªæ•°éƒ½å‡å»è¿™ä¸ªå¹³å‡å€¼ã€‚å°±åƒæŠŠå…¨ç­åŒå­¦çš„åˆ†æ•°éƒ½å‡å»å¹³å‡åˆ†ï¼Œè®©å¤§å®¶å›´ç»• 0 åˆ†æ³¢åŠ¨ã€‚
2.  **ç¼©æ”¾ï¼ˆRe-scalingï¼‰**ï¼šç®—å‡ºæ•°æ®çš„æ ‡å‡†å·®ï¼ˆæ³¢åŠ¨å¹…åº¦ï¼‰ï¼Œç„¶åé™¤ä»¥å®ƒã€‚å°±åƒæŠŠåˆ†æ•°ç¼©æ”¾åˆ° -1 åˆ° 1 ä¹‹é—´ã€‚

### 2.2 RMSNormï¼šé«˜æ•ˆçš„å®å¹²å®¶
ç§‘å­¦å®¶ä»¬åæ¥å‘ç°ï¼Œåœ¨å¤§æ¨¡å‹é‡Œï¼Œ**â€œå»ä¸­å¿ƒåŒ–â€è¿™ä¸€æ­¥å…¶å®æ²¡é‚£ä¹ˆé‡è¦ï¼Œåè€Œå¾ˆè´¹æ—¶é—´**ã€‚é‡è¦çš„æ˜¯æŠŠæ•°å€¼çš„å¹…åº¦ï¼ˆScaleï¼‰æ§åˆ¶ä½ã€‚

äºæ˜¯ï¼Œ**RMSNorm (Root Mean Square Normalization)** å‡ºç°äº†ã€‚å®ƒçš„åšæ³•æ›´ç®€å•ç²—æš´ï¼š
*   **åªç¼©æ”¾ï¼Œä¸ç§»ä½**ï¼šå®ƒä¸å‡å¹³å‡å€¼ï¼Œç›´æ¥ç®—å‡ºæ•°æ®çš„**å‡æ–¹æ ¹ (RMS)**ï¼ˆç›¸å½“äºä¸€ç§è¡¡é‡æ•°å€¼å¤§å°çš„â€œå°ºå­â€ï¼‰ï¼Œç„¶åé™¤ä»¥å®ƒã€‚

> **æ¯”å–»**ï¼š
> *   **LayerNorm** åƒæ˜¯ä¸€ä¸ªè¿™å°±è¦æ±‚ä½ å¿…é¡»å…ˆç«™åœ¨è®²å°æ­£ä¸­å¤®ï¼ˆå‡å‡å€¼ï¼‰ï¼Œç„¶åå†è°ƒæ•´èº«é«˜ï¼ˆé™¤æ ‡å‡†å·®ï¼‰ã€‚
> *   **RMSNorm** è¯´ï¼šâ€œä¸ç”¨éº»çƒ¦èµ°å»ä¸­é—´äº†ï¼Œä½ ç›´æ¥ç«™åœ¨åŸåœ°ï¼Œæˆ‘æŠŠä½ ç¼©å°ä¸€ç‚¹å°±è¡Œã€‚â€

---

## 3. ä¸ºä»€ä¹ˆ Llama ç­‰å¤§æ¨¡å‹éƒ½çˆ±ç”¨å®ƒï¼Ÿ

1.  **é€Ÿåº¦å¿«**ï¼šå°‘äº†ä¸€æ­¥å‡å‡å€¼çš„è®¡ç®—ï¼Œè¿™åœ¨å‡ ç™¾äº¿å‚æ•°çš„æ¨¡å‹é‡Œï¼Œèƒ½çœä¸‹å·¨å¤§çš„è®¡ç®—é‡ã€‚
2.  **æ•ˆæœå¥½**ï¼šå®éªŒè¯æ˜ï¼Œå®ƒåœ¨è®­ç»ƒç¨³å®šæ€§ä¸Šå’Œ LayerNorm å·®ä¸å¤šï¼Œç”šè‡³åœ¨æŸäº›ä»»åŠ¡ä¸Šæ›´å¥½ã€‚
3.  **æ›´ç®€å•**ï¼šä»£ç å®ç°æ›´ç®€æ´ï¼Œå¯¹ç¡¬ä»¶æ›´å‹å¥½ã€‚

## 4. æ€»ç»“

| ç‰¹æ€§ | LayerNorm (ä¼ ç»Ÿ) | **RMSNorm (ç°ä»£)** |
| :--- | :--- | :--- |
| **æ ¸å¿ƒæ“ä½œ** | å‡å‡å€¼ + é™¤æ ‡å‡†å·® | **åªé™¤å‡æ–¹æ ¹** |
| **è®¡ç®—é‡** | ğŸ”´ è¾ƒå¤§ | **ğŸŸ© è¾ƒå° (æ›´å¿«)** |
| **å‚æ•°** | ç¼©æ”¾å› å­ $\gamma$ + åç½® $\beta$ | **åªæœ‰ç¼©æ”¾å› å­ $\gamma$** |
| **ä»£è¡¨æ¨¡å‹** | BERT, GPT-2 | **Llama, PaLM, Gopher** |

RMSNorm å°±æ˜¯å¤§æ¨¡å‹æ—¶ä»£çš„â€œæç®€ä¸»ä¹‰â€èƒœåˆ©ï¼Œç”¨æ›´å°‘çš„è®¡ç®—è¾¾åˆ°äº†åŒæ ·ç”šè‡³æ›´å¥½çš„æ•ˆæœã€‚

---

## å‚è€ƒèµ„æ–™

1.  [MachineLearningMastery: LayerNorm and RMS Norm in Transformer Models](https://machinelearningmastery.com/layernorm-and-rms-norm-in-transformer-models/)
2.  [Medium: Why Modern Transformers Use RMSNorm Instead of LayerNorm](https://medium.com/@ashutoshs81127/why-modern-transformers-use-rmsnorm-instead-of-layernorm-5f386be7156c)
